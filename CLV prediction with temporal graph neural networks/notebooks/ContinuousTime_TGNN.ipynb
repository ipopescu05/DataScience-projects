{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ee9921b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InvoiceNo</th>\n",
       "      <th>StockCode</th>\n",
       "      <th>Description</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>489434</td>\n",
       "      <td>85048</td>\n",
       "      <td>15CM CHRISTMAS GLASS BALL 20 LIGHTS</td>\n",
       "      <td>12</td>\n",
       "      <td>2009-12-01 07:45:00</td>\n",
       "      <td>6.95</td>\n",
       "      <td>13085.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>489434</td>\n",
       "      <td>79323P</td>\n",
       "      <td>PINK CHERRY LIGHTS</td>\n",
       "      <td>12</td>\n",
       "      <td>2009-12-01 07:45:00</td>\n",
       "      <td>6.75</td>\n",
       "      <td>13085.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>489434</td>\n",
       "      <td>79323W</td>\n",
       "      <td>WHITE CHERRY LIGHTS</td>\n",
       "      <td>12</td>\n",
       "      <td>2009-12-01 07:45:00</td>\n",
       "      <td>6.75</td>\n",
       "      <td>13085.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>489434</td>\n",
       "      <td>22041</td>\n",
       "      <td>RECORD FRAME 7\" SINGLE SIZE</td>\n",
       "      <td>48</td>\n",
       "      <td>2009-12-01 07:45:00</td>\n",
       "      <td>2.10</td>\n",
       "      <td>13085.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>489434</td>\n",
       "      <td>21232</td>\n",
       "      <td>STRAWBERRY CERAMIC TRINKET BOX</td>\n",
       "      <td>24</td>\n",
       "      <td>2009-12-01 07:45:00</td>\n",
       "      <td>1.25</td>\n",
       "      <td>13085.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  InvoiceNo StockCode                          Description  Quantity  \\\n",
       "0    489434     85048  15CM CHRISTMAS GLASS BALL 20 LIGHTS        12   \n",
       "1    489434    79323P                   PINK CHERRY LIGHTS        12   \n",
       "2    489434    79323W                  WHITE CHERRY LIGHTS        12   \n",
       "3    489434     22041         RECORD FRAME 7\" SINGLE SIZE         48   \n",
       "4    489434     21232       STRAWBERRY CERAMIC TRINKET BOX        24   \n",
       "\n",
       "           InvoiceDate  UnitPrice  CustomerID         Country  \n",
       "0  2009-12-01 07:45:00       6.95     13085.0  United Kingdom  \n",
       "1  2009-12-01 07:45:00       6.75     13085.0  United Kingdom  \n",
       "2  2009-12-01 07:45:00       6.75     13085.0  United Kingdom  \n",
       "3  2009-12-01 07:45:00       2.10     13085.0  United Kingdom  \n",
       "4  2009-12-01 07:45:00       1.25     13085.0  United Kingdom  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy import stats\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import TGNMemory, TransformerConv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric_temporal import recurrent\n",
    "from torch_geometric_temporal.signal import DynamicGraphTemporalSignal, DynamicHeteroGraphTemporalSignal\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric_temporal.nn import HeteroGCLSTM\n",
    "from torch_geometric.nn.norm import GraphNorm\n",
    "from torch_geometric.nn import GATConv, HeteroConv\n",
    "from probabilistic_util_fuctions import generate_rfm_data2, generate_rfm_data\n",
    "\n",
    "\n",
    "#df = pd.read_excel('Online Retail.xlsx')\n",
    "df = pd.read_csv('online_retail_II.csv')\n",
    "df.rename(columns = {'Customer ID':'CustomerID', \n",
    "                     'Price': 'UnitPrice', \n",
    "                     'Invoice': \"InvoiceNo\"}, inplace = True)\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1516c2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggretate_df(df):\n",
    "    '''Generte aggregated weekly dataframe with CustomerID, StockCode, InvoiceNo, UnitPrice, Quantity'''\n",
    "    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "    first_date = df['InvoiceDate'].min()\n",
    "    last_date = df['InvoiceDate'].max()\n",
    "    df['week'] = df['InvoiceDate'].dt.to_period('M')\n",
    "\n",
    "    aggregated_daframe = pd.DataFrame(columns=['CustomerID', 'StockCode', 'InvoiceNo', 'UnitPrice', 'Quantity', 'Country', 'Description', 'InvoiceDate'])\n",
    "\n",
    "    for week in df.week.unique():\n",
    "        _ = week.start_time\n",
    "        end_date = week.end_time\n",
    "        week_df = df[df.week == week].copy()\n",
    "        week_df_aggregated = week_df.groupby(['CustomerID', 'StockCode']).agg({\n",
    "            'InvoiceNo': 'count',\n",
    "            'UnitPrice': 'mean', \n",
    "            'Quantity': 'sum', \n",
    "            'Country': 'first',\n",
    "            'Description': 'first'}).reset_index()\n",
    "        week_df_aggregated['InvoiceDate'] = end_date.strftime('%Y-%m-%d')\n",
    "        # drop rows with quantity <= 0\n",
    "        week_df_aggregated = week_df_aggregated[week_df_aggregated['Quantity'] > 0]\n",
    "\n",
    "        #append the aggregated week dataframe to the main dataframe\n",
    "        aggregated_daframe = pd.concat([aggregated_daframe, week_df_aggregated], ignore_index=True)\n",
    "    aggregated_daframe['Quantity'] = aggregated_daframe['Quantity'].astype(int)\n",
    "    return  aggregated_daframe \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07626573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(718088, 8)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = aggretate_df(df)\n",
    "test_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ddd0d2ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>StockCode</th>\n",
       "      <th>InvoiceNo</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Country</th>\n",
       "      <th>Description</th>\n",
       "      <th>InvoiceDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12346.0</td>\n",
       "      <td>TEST001</td>\n",
       "      <td>4</td>\n",
       "      <td>4.50</td>\n",
       "      <td>25</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>This is a test product.</td>\n",
       "      <td>2009-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12346.0</td>\n",
       "      <td>TEST002</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>This is a test product.</td>\n",
       "      <td>2009-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12358.0</td>\n",
       "      <td>15056BL</td>\n",
       "      <td>1</td>\n",
       "      <td>4.95</td>\n",
       "      <td>60</td>\n",
       "      <td>Austria</td>\n",
       "      <td>EDWARDIAN PARASOL BLACK</td>\n",
       "      <td>2009-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12358.0</td>\n",
       "      <td>15056N</td>\n",
       "      <td>1</td>\n",
       "      <td>4.95</td>\n",
       "      <td>60</td>\n",
       "      <td>Austria</td>\n",
       "      <td>EDWARDIAN PARASOL NATURAL</td>\n",
       "      <td>2009-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12358.0</td>\n",
       "      <td>15056P</td>\n",
       "      <td>1</td>\n",
       "      <td>5.95</td>\n",
       "      <td>30</td>\n",
       "      <td>Austria</td>\n",
       "      <td>EDWARDIAN PARASOL PINK</td>\n",
       "      <td>2009-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718083</th>\n",
       "      <td>18283.0</td>\n",
       "      <td>23582</td>\n",
       "      <td>1</td>\n",
       "      <td>2.08</td>\n",
       "      <td>3</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>VINTAGE DOILY JUMBO BAG RED</td>\n",
       "      <td>2011-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718084</th>\n",
       "      <td>18283.0</td>\n",
       "      <td>23583</td>\n",
       "      <td>1</td>\n",
       "      <td>1.65</td>\n",
       "      <td>3</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>LUNCH BAG PAISLEY PARK</td>\n",
       "      <td>2011-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718085</th>\n",
       "      <td>18283.0</td>\n",
       "      <td>23681</td>\n",
       "      <td>1</td>\n",
       "      <td>1.65</td>\n",
       "      <td>3</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>LUNCH BAG RED VINTAGE DOILY</td>\n",
       "      <td>2011-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718086</th>\n",
       "      <td>18283.0</td>\n",
       "      <td>85099F</td>\n",
       "      <td>1</td>\n",
       "      <td>2.08</td>\n",
       "      <td>3</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>JUMBO BAG STRAWBERRY</td>\n",
       "      <td>2011-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718087</th>\n",
       "      <td>18283.0</td>\n",
       "      <td>90099</td>\n",
       "      <td>1</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>NECKLACE+BRACELET SET BLUE HIBISCUS</td>\n",
       "      <td>2011-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>718088 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CustomerID StockCode InvoiceNo  UnitPrice  Quantity         Country  \\\n",
       "0          12346.0   TEST001         4       4.50        25  United Kingdom   \n",
       "1          12346.0   TEST002         1       1.00         1  United Kingdom   \n",
       "2          12358.0   15056BL         1       4.95        60         Austria   \n",
       "3          12358.0    15056N         1       4.95        60         Austria   \n",
       "4          12358.0    15056P         1       5.95        30         Austria   \n",
       "...            ...       ...       ...        ...       ...             ...   \n",
       "718083     18283.0     23582         1       2.08         3  United Kingdom   \n",
       "718084     18283.0     23583         1       1.65         3  United Kingdom   \n",
       "718085     18283.0     23681         1       1.65         3  United Kingdom   \n",
       "718086     18283.0    85099F         1       2.08         3  United Kingdom   \n",
       "718087     18283.0     90099         1       2.55         1  United Kingdom   \n",
       "\n",
       "                                Description InvoiceDate  \n",
       "0                   This is a test product.  2009-12-31  \n",
       "1                   This is a test product.  2009-12-31  \n",
       "2                   EDWARDIAN PARASOL BLACK  2009-12-31  \n",
       "3                 EDWARDIAN PARASOL NATURAL  2009-12-31  \n",
       "4                    EDWARDIAN PARASOL PINK  2009-12-31  \n",
       "...                                     ...         ...  \n",
       "718083         VINTAGE DOILY JUMBO BAG RED   2011-12-31  \n",
       "718084             LUNCH BAG PAISLEY PARK    2011-12-31  \n",
       "718085          LUNCH BAG RED VINTAGE DOILY  2011-12-31  \n",
       "718086                 JUMBO BAG STRAWBERRY  2011-12-31  \n",
       "718087  NECKLACE+BRACELET SET BLUE HIBISCUS  2011-12-31  \n",
       "\n",
       "[718088 rows x 8 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1080cf69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 718088 entries, 0 to 718087\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   CustomerID   718088 non-null  float64\n",
      " 1   StockCode    718088 non-null  object \n",
      " 2   InvoiceNo    718088 non-null  object \n",
      " 3   UnitPrice    718088 non-null  float64\n",
      " 4   Quantity     718088 non-null  int64  \n",
      " 5   Country      718088 non-null  object \n",
      " 6   Description  718088 non-null  object \n",
      " 7   InvoiceDate  718088 non-null  object \n",
      "dtypes: float64(2), int64(1), object(5)\n",
      "memory usage: 43.8+ MB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a28e3ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(df):\n",
    "\n",
    "    df = df[df['CustomerID'].notna()].reset_index(drop=True)\n",
    "    df = df[df.Quantity > 0]\n",
    "    df = df[df.UnitPrice > 0]\n",
    "    non_product_codes = ['POST', 'D', 'M', 'C2', 'BANK CHARGES', 'AMAZONFEE', 'DCGSSBOY', \n",
    "                         'DCGSSGIRL', 'DOT', 'PADS', 'TEST001','TEST002', 'ADJUST', 'ADJUST2','SP1002']\n",
    "\n",
    "    df = df[~df['StockCode'].isin(non_product_codes)]\n",
    "\n",
    "    #time features engeneering\n",
    "    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "    df['Date'] = df['InvoiceDate'].dt.date\n",
    "    df['Month'] = df['InvoiceDate'].dt.month\n",
    "    df['month'] = df['InvoiceDate'].dt.month\n",
    "\n",
    "    df['Year_month'] = df['InvoiceDate'].dt.to_period('M')\n",
    "    df['day_of_week'] = df['InvoiceDate'].dt.dayofweek\n",
    "    df['hour_minute'] = df['InvoiceDate'].dt.time\n",
    "\n",
    "    df['day_sin'] = np.sin(2*np.pi*df['day_of_week']/7)\n",
    "    df['day_cos'] = np.cos(2*np.pi*df['day_of_week']/7)\n",
    "\n",
    "    df = pd.get_dummies(df, columns = ['month'], prefix = 'month', drop_first=True)\n",
    "    #transfomr the boolean columns to int\n",
    "    df[['month_2', 'month_3',\n",
    "       'month_4', 'month_5', 'month_6', 'month_7', 'month_8', 'month_9',\n",
    "       'month_10', 'month_11', 'month_12']] = df[[ 'month_2', 'month_3',\n",
    "       'month_4', 'month_5', 'month_6', 'month_7', 'month_8', 'month_9',\n",
    "       'month_10', 'month_11', 'month_12']].astype(int)\n",
    "    df['TotalPrice'] = df['Quantity'] * df['UnitPrice']\n",
    "    df['Amount'] = df['Quantity'] * df['UnitPrice']\n",
    "    df = df[(np.abs(stats.zscore(df['TotalPrice'])) < 3)]\n",
    "    df = df[(np.abs(stats.zscore(df['Quantity'])) < 3)]\n",
    "    \n",
    "    # create first order of the customer column\n",
    "    df['FirstOrder'] = df.groupby('CustomerID')['InvoiceDate'].transform('min')\n",
    "\n",
    "\n",
    "    #ebedding for the country and description columns\n",
    "    country_encoder = LabelEncoder()\n",
    "    df['CountryID'] = country_encoder.fit_transform(df['Country'])\n",
    "    desc_encoder = LabelEncoder()\n",
    "    df['DescriptionID'] = desc_encoder.fit_transform(df['Description'])\n",
    "    #define embedding layers to be used in the model\n",
    "    num_countries = df.CountryID.nunique()\n",
    "    num_description = df.DescriptionID.nunique()\n",
    "    country_embedding = nn.Embedding( num_embeddings=num_countries, embedding_dim = 4)\n",
    "    description_embedding = nn.Embedding(num_embeddings=num_description, embedding_dim = 16)\n",
    "    df = df[df.Year_month != '2011-12']\n",
    "\n",
    "    return df, country_embedding, description_embedding\n",
    "def data_preprocessing_alternative(df):\n",
    "\n",
    "    df = df[df['CustomerID'].notna()].reset_index(drop=True)\n",
    "    df = df[df.Quantity > 0]\n",
    "    df = df[df.UnitPrice > 0]\n",
    "    non_product_codes = ['POST', 'D', 'M', 'C2', 'BANK CHARGES', 'AMAZONFEE', 'DCGSSBOY', \n",
    "                         'DCGSSGIRL', 'DOT', 'PADS', 'TEST001','TEST002', 'ADJUST', 'ADJUST2','SP1002']\n",
    "\n",
    "    df = df[~df['StockCode'].isin(non_product_codes)]\n",
    "\n",
    "    #time features engeneering\n",
    "    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "    df['Date'] = df['InvoiceDate'].dt.date\n",
    "    df['Month'] = df['InvoiceDate'].dt.month\n",
    "    df['month'] = df['InvoiceDate'].dt.month\n",
    "\n",
    "    df['Year_month'] = df['InvoiceDate'].dt.to_period('M')\n",
    "    df['day_of_week'] = df['InvoiceDate'].dt.dayofweek\n",
    "    df['hour_minute'] = df['InvoiceDate'].dt.time\n",
    "\n",
    "    df['day_sin'] = np.sin(2*np.pi*df['day_of_week']/7)\n",
    "    df['day_cos'] = np.cos(2*np.pi*df['day_of_week']/7)\n",
    "\n",
    "    morning_time = pd.to_datetime('12:00:00').time()\n",
    "    afternoon_time = pd.to_datetime('18:00:00').time()\n",
    "    df['time_of_day'] = df['hour_minute'].apply(lambda x: 'morning' if x < morning_time\n",
    "                                                                        else 'afternoon' if (x < afternoon_time and x >= morning_time)\n",
    "                                                                        else 'evening')\n",
    "    df = pd.get_dummies(df, columns=['time_of_day'], prefix='time_of_day', drop_first=True)\n",
    "    df = pd.get_dummies(df, columns = ['month'], prefix = 'month', drop_first=True)\n",
    "    #transfomr the boolean columns to int\n",
    "    df[['time_of_day_morning', 'time_of_day_evening', 'month_2', 'month_3',\n",
    "       'month_4', 'month_5', 'month_6', 'month_7', 'month_8', 'month_9',\n",
    "       'month_10', 'month_11', 'month_12']] = df[['time_of_day_morning', 'time_of_day_evening', 'month_2', 'month_3',\n",
    "       'month_4', 'month_5', 'month_6', 'month_7', 'month_8', 'month_9',\n",
    "       'month_10', 'month_11', 'month_12']].astype(int)\n",
    "    df['TotalPrice'] = df['Quantity'] * df['UnitPrice']\n",
    "    df['Amount'] = df['Quantity'] * df['UnitPrice']\n",
    "    df = df[(np.abs(stats.zscore(df['TotalPrice'])) < 3)]\n",
    "    df = df[(np.abs(stats.zscore(df['Quantity'])) < 3)]\n",
    "    \n",
    "    # create first order of the customer column\n",
    "    df['FirstOrder'] = df.groupby('CustomerID')['InvoiceDate'].transform('min')\n",
    "\n",
    "\n",
    "    #ebedding for the country and description columns\n",
    "    country_encoder = LabelEncoder()\n",
    "    df['CountryID'] = country_encoder.fit_transform(df['Country'])\n",
    "    desc_encoder = LabelEncoder()\n",
    "    df['DescriptionID'] = desc_encoder.fit_transform(df['Description'])\n",
    "    #define embedding layers to be used in the model\n",
    "    num_countries = df.CountryID.nunique()\n",
    "    num_description = df.DescriptionID.nunique()\n",
    "    country_embedding = nn.Embedding( num_embeddings=num_countries, embedding_dim = 4)\n",
    "    description_embedding = nn.Embedding(num_embeddings=num_description, embedding_dim = 16)\n",
    "    df = df[df.Year_month != '2011-12']\n",
    "\n",
    "    return df, country_embedding, description_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0193d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed, country_embedding, description_embedding = data_preprocessing(test_df)\n",
    "train_data = df_processed[df_processed['Year_month'] < '2011-09']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eca30dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(554594, 31)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1c9cc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def prepare_data_TGN(df, datetime_col: str = 'InvoiceDate',\n",
    "                     product_id_col: str = 'StockCode',\n",
    "                     customer_id_col: str = 'CustomerID'): \n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[datetime_col]):\n",
    "        df[datetime_col] = pd.to_datetime(df[datetime_col])\n",
    "    df = df.sort_values(by=datetime_col).reset_index(drop=True)\n",
    "    min_timestamp_val = df[datetime_col].min()\n",
    "    df['t_seconds'] = (df[datetime_col] - min_timestamp_val).dt.total_seconds()\n",
    "    #get dummy variables for time of day in int format\n",
    "    \n",
    "    \n",
    "\n",
    "    print(df.columns)\n",
    "\n",
    "    unique_products = df[product_id_col].unique()\n",
    "    unique_costomers = df[customer_id_col].unique()\n",
    "    customer_internal_map = {orig_id:i for i, orig_id in enumerate(unique_costomers)}\n",
    "    product_internal_map = {orig_id: i for i, orig_id in enumerate(unique_products)}\n",
    "\n",
    "    df_reindexed = df.copy()\n",
    "    df_reindexed['i_mapped'] = df_reindexed[product_id_col].map(product_internal_map)\n",
    "    df_reindexed['c_mapped'] = df_reindexed[customer_id_col].map(customer_internal_map)\n",
    "    \n",
    "    df_reindexed = df_reindexed.rename(columns = {'c_mapped': 'source_id',\n",
    "                         'i_mapped': 'destination_id',\n",
    "                       't_seconds': 'timestamp',})\n",
    "    #make sure source_id and destination_id are integers\n",
    "    df_reindexed['source_id'] = df_reindexed['source_id'].astype(int)\n",
    "    df_reindexed['destination_id'] = df_reindexed['destination_id'].astype(int)\n",
    "    #normalize TotalPrice and Quantity\n",
    "    scaler_price = StandardScaler()\n",
    "    df_reindexed['TotalPrice'] = scaler_price.fit_transform(df_reindexed[['TotalPrice']])\n",
    "    scaler_quantity = StandardScaler()\n",
    "    df_reindexed['Quantity'] = scaler_quantity.fit_transform(df_reindexed[['Quantity']])\n",
    "    \n",
    "    #add a columns of ones for the label\n",
    "    df_reindexed['label'] = 1.0\n",
    "    '''\n",
    "    return df_reindexed[['source_id', 'destination_id', 'timestamp', 'label', 'TotalPrice', 'Quantity', \n",
    "                         'day_sin', 'day_cos', 'time_of_day_morning', 'time_of_day_evening', 'month_2', 'month_3',\n",
    "       'month_4', 'month_5', 'month_6', 'month_7', 'month_8', 'month_9',\n",
    "       'month_10', 'month_11', 'month_12']], scaler_price, scaler_quantity, product_internal_map, customer_internal_map'''\n",
    "    return df_reindexed[['source_id', 'destination_id', 'timestamp', 'label', 'TotalPrice', 'Quantity', \n",
    "                         'day_sin', 'day_cos', 'month_2', 'month_3',\n",
    "       'month_4', 'month_5', 'month_6', 'month_7', 'month_8', 'month_9',\n",
    "       'month_10', 'month_11', 'month_12']], scaler_price, scaler_quantity, product_internal_map, customer_internal_map\n",
    "\n",
    "def create_event_data_for_tgn_script(\n",
    "    df: pd.DataFrame,\n",
    "    price_scaler: StandardScaler,\n",
    "    quantity_scaler: StandardScaler,\n",
    "    customer_id_col: str = 'CustomerID',\n",
    "    product_id_col: str = 'StockCode',\n",
    "    datetime_col: str = 'InvoiceDate',\n",
    "    event_feature_cols: list = ['Quantity', 'TotalPrice', 'day_sin', 'day_cos', \n",
    "                                'month_2', 'month_3', 'month_4', 'month_5', \n",
    "                                'month_6', 'month_7', 'month_8', 'month_9',\n",
    "                                'month_10', 'month_11', 'month_12'],\n",
    "    static_customer_feature_cols: list = ['CountryID'],\n",
    "    static_product_feature_cols: list = ['DescriptionID'],\n",
    "    is_bipartite: bool = True\n",
    ") -> dict:\n",
    "    print(\"Preparing data for TGN script compatibility...\")\n",
    "\n",
    "    # 0. Initial Data Cleaning for ID columns & Empty DataFrame Check\n",
    "    print(f\"Original DataFrame length: {len(df)}\")\n",
    "    initial_len = len(df)\n",
    "    df = df.dropna(subset=[customer_id_col, product_id_col, datetime_col]) # Also drop if datetime is NaN\n",
    "    if len(df) < initial_len:\n",
    "        print(f\"Dropped {initial_len - len(df)} rows with NaN IDs or datetime.\")\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(\"Error: DataFrame is empty after dropping critical NaNs. Cannot proceed.\")\n",
    "        # Return a defined empty structure\n",
    "        default_edge_feat_dim = len(event_feature_cols) if event_feature_cols else 1\n",
    "        default_node_feat_dim = 1 # Default if no static features specified\n",
    "        return {\n",
    "            'sources': np.array([], dtype=np.int64), 'destinations': np.array([], dtype=np.int64),\n",
    "            'timestamps': np.array([], dtype=np.float32), \n",
    "            'event_datetimes': np.array([], dtype='datetime64[ns]'),\n",
    "            'edge_features_global_store': np.zeros((1, default_edge_feat_dim), dtype=np.float32),\n",
    "            'edge_idxs': np.array([], dtype=np.int64), 'labels': np.array([], dtype=np.float32),\n",
    "            'node_features_global_store': np.zeros((1, default_node_feat_dim), dtype=np.float32),\n",
    "            'max_node_id': 0, 'num_total_nodes': 0, # Corrected, num_total_nodes from maps below\n",
    "            'num_customers': 0, 'num_products': 0,\n",
    "            'customer_original_to_internal_map': {}, 'product_original_to_internal_map': {}\n",
    "        }\n",
    "    #use the scallers to transform the features\n",
    "    df['TotalPrice'] = price_scaler.transform(df[['TotalPrice']])\n",
    "    df['Quantity'] = quantity_scaler.transform(df[['Quantity']])\n",
    "    # 1. Timestamp handling and sorting\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[datetime_col]):\n",
    "        df[datetime_col] = pd.to_datetime(df[datetime_col])\n",
    "    df = df.sort_values(by=datetime_col).reset_index(drop=True)\n",
    "    min_timestamp_val = df[datetime_col].min()\n",
    "    df['t_seconds'] = (df[datetime_col] - min_timestamp_val).dt.total_seconds()\n",
    "    print(f\"Timestamps converted to seconds since first event: {min_timestamp_val}\")\n",
    "\n",
    "    # 2. Global Node Mapping (Now on cleaned df)\n",
    "    unique_customers = df[customer_id_col].unique()\n",
    "    unique_products = df[product_id_col].unique()\n",
    "\n",
    "    num_customers = len(unique_customers)\n",
    "    num_products = len(unique_products)\n",
    "    \n",
    "    customer_internal_map = {orig_id: i+1 for i, orig_id in enumerate(unique_customers)}\n",
    "    product_internal_map = {orig_id: i+1+num_customers for i, orig_id in enumerate(unique_products)}\n",
    "    print(f\"Created node mappings: {num_customers} customers, {num_products} products.\")\n",
    "\n",
    "    df_reindexed = df.copy()\n",
    "    df_reindexed['u_mapped'] = df_reindexed[customer_id_col].map(customer_internal_map)\n",
    "    df_reindexed['i_mapped'] = df_reindexed[product_id_col].map(product_internal_map)\n",
    "\n",
    "    # This check should ideally not be needed if dropna was effective\n",
    "    if df_reindexed['u_mapped'].isnull().any() or df_reindexed['i_mapped'].isnull().any():\n",
    "        raise ValueError(\"Internal Error: NaNs found after mapping customer/product IDs despite dropna. Check ID consistency or df copying.\")\n",
    "\n",
    "    max_u_mapped = -1 # Default if no customers\n",
    "    if num_customers > 0:\n",
    "        max_u_mapped = df_reindexed['u_mapped'].max()\n",
    "        if pd.isna(max_u_mapped): # Should only happen if u_mapped is all NaNs (now prevented) or empty\n",
    "            raise ValueError(\"max_u_mapped is NaN. This should be prevented by earlier checks.\")\n",
    "\n",
    "\n",
    "    # --- Global Edge Features (Prepended with zeros for 1-based indexing) ---\n",
    "    if event_feature_cols and all(col in df.columns for col in event_feature_cols):\n",
    "        raw_event_edge_features_np = df_reindexed[event_feature_cols].values.astype(np.float32)\n",
    "    else:\n",
    "        raw_event_edge_features_np = np.ones((len(df_reindexed), 1), dtype=np.float32)\n",
    "    global_edge_features_np = np.vstack([\n",
    "        np.zeros((1, raw_event_edge_features_np.shape[1]), dtype=np.float32),\n",
    "        raw_event_edge_features_np\n",
    "    ])\n",
    "    print(f\"Global edge features created. Shape: {global_edge_features_np.shape}\")\n",
    "\n",
    "    if 'label' in df_reindexed.columns:\n",
    "        labels_np = df_reindexed['label'].values.astype(np.float32)\n",
    "    else:\n",
    "        labels_np = np.zeros(len(df_reindexed), dtype=np.float32)\n",
    "\n",
    "    # --- Re-indexing to 1-based global IDs ---\n",
    "    if is_bipartite:\n",
    "        df_reindexed['i_global_1_based'] = df_reindexed['i_mapped'].astype(int) \n",
    "        df_reindexed['u_global_1_based'] = df_reindexed['u_mapped'].astype(int) \n",
    "    else:\n",
    "        # For homogeneous, need a single combined map from original IDs to 0-indexed continuous\n",
    "        all_original_nodes = pd.unique(np.concatenate((df[customer_id_col].values, df[product_id_col].values)))\n",
    "        node_overall_internal_map = {orig_id: i for i, orig_id in enumerate(all_original_nodes)}\n",
    "        df_reindexed['u_global_1_based'] = df_reindexed[customer_id_col].map(node_overall_internal_map).astype(int) + 1\n",
    "        df_reindexed['i_global_1_based'] = df_reindexed[product_id_col].map(node_overall_internal_map).astype(int) + 1\n",
    "    \n",
    "    df_reindexed['edge_idxs_1_based'] = np.arange(1, len(df_reindexed) + 1)\n",
    "\n",
    "    # Now cast to int64, NaNs should be gone.\n",
    "    sources_1_based_np = df_reindexed['u_global_1_based'].values.astype(np.int64)\n",
    "    destinations_1_based_np = df_reindexed['i_global_1_based'].values.astype(np.int64)\n",
    "    edge_idxs_1_based_np = df_reindexed['edge_idxs_1_based'].values.astype(np.int64)\n",
    "    \n",
    "    timestamps_numeric_np = df_reindexed['t_seconds'].values.astype(np.float32)\n",
    "    event_datetimes_np = df_reindexed[datetime_col].values\n",
    "\n",
    "    # Determine max_node_id correctly\n",
    "    if len(sources_1_based_np) == 0 and len(destinations_1_based_np) == 0: # Should be caught by len(df)==0\n",
    "        max_node_id = 0\n",
    "    elif len(sources_1_based_np) == 0:\n",
    "        max_node_id = destinations_1_based_np.max() if len(destinations_1_based_np) > 0 else 0\n",
    "    elif len(destinations_1_based_np) == 0:\n",
    "        max_node_id = sources_1_based_np.max() if len(sources_1_based_np) > 0 else 0\n",
    "    else:\n",
    "        max_node_id = max(sources_1_based_np.max(), destinations_1_based_np.max())\n",
    "    print(f\"Re-indexing complete. Max node ID (1-based): {max_node_id}\")\n",
    "\n",
    "    # --- Initial/Static Node Features ---\n",
    "    node_feat_dim_to_use = 1 # Default if no static features\n",
    "    if static_customer_feature_cols or static_product_feature_cols:\n",
    "        actual_cust_feat_dim = len(static_customer_feature_cols) if static_customer_feature_cols and static_customer_feature_cols[0] else 0\n",
    "        actual_prod_feat_dim = len(static_product_feature_cols) if static_product_feature_cols and static_product_feature_cols[0] else 0\n",
    "        if actual_cust_feat_dim > 0 or actual_prod_feat_dim > 0:\n",
    "            node_feat_dim_to_use = max(actual_cust_feat_dim, actual_prod_feat_dim, 1) # Ensure at least 1\n",
    "    \n",
    "    global_node_features_np = np.zeros((max_node_id + 1, node_feat_dim_to_use), dtype=np.float32)\n",
    "\n",
    "    # Populate static features (this logic needs to be robust if is_bipartite=False)\n",
    "    if static_customer_feature_cols and static_customer_feature_cols[0] and all(col in df.columns for col in static_customer_feature_cols):\n",
    "        cust_static_vals = df.groupby(customer_id_col, as_index=False)[static_customer_feature_cols].first()\n",
    "        for _, row in cust_static_vals.iterrows():\n",
    "            original_id = row[customer_id_col]\n",
    "            internal_0_idx = customer_internal_map.get(original_id) # Uses 0-indexed map\n",
    "            if internal_0_idx is not None:\n",
    "                global_1_idx_u = internal_0_idx + 1 # u_global_1_based for customers\n",
    "                if 0 < global_1_idx_u <= max_node_id: # Node 0 is dummy\n",
    "                    features_to_assign = row[static_customer_feature_cols].values.astype(np.float32)\n",
    "                    global_node_features_np[global_1_idx_u, :len(features_to_assign)] = features_to_assign\n",
    "    \n",
    "    if static_product_feature_cols and static_product_feature_cols[0] and all(col in df.columns for col in static_product_feature_cols):\n",
    "        prod_static_vals = df.groupby(product_id_col, as_index=False)[static_product_feature_cols].first()\n",
    "        for _, row in prod_static_vals.iterrows():\n",
    "            original_id = row[product_id_col]\n",
    "            internal_0_idx = product_internal_map.get(original_id) # Uses 0-indexed map\n",
    "            if internal_0_idx is not None:\n",
    "                if is_bipartite:\n",
    "                    global_1_idx_i = internal_0_idx + (int(max_u_mapped) + 1) + 1 # i_global_1_based for products\n",
    "                else: # Homogeneous\n",
    "                    global_1_idx_i = node_overall_internal_map.get(original_id) + 1 # Must use the combined map\n",
    "                \n",
    "                if 0 < global_1_idx_i <= max_node_id:\n",
    "                    features_to_assign = row[static_product_feature_cols].values.astype(np.float32)\n",
    "                    global_node_features_np[global_1_idx_i, :len(features_to_assign)] = features_to_assign\n",
    "    print(f\"Global node features created. Shape: {global_node_features_np.shape}\")\n",
    "    \n",
    "    num_total_nodes_for_tgn = max_node_id + 1 # For array sizing if 0 is dummy, or just max_node_id if TGN handles it\n",
    "\n",
    "    data = {\n",
    "        'sources': sources_1_based_np,\n",
    "        'destinations': destinations_1_based_np,\n",
    "        'timestamps': timestamps_numeric_np,\n",
    "        'event_datetimes': event_datetimes_np,\n",
    "        'edge_features_global_store': global_edge_features_np,\n",
    "        'edge_idxs': edge_idxs_1_based_np,\n",
    "        'labels': labels_np,\n",
    "        'node_features_global_store': global_node_features_np,\n",
    "        'max_node_id': max_node_id, # Highest 1-based ID used\n",
    "        'num_total_nodes_for_array_sizing': num_total_nodes_for_tgn, # For TGN(n_nodes=...) if it implies array size\n",
    "        'num_customers': num_customers, # Count of unique original customers\n",
    "        'num_products': num_products,   # Count of unique original products\n",
    "        'customer_original_to_internal_map': customer_internal_map, # 0-indexed internal map\n",
    "        'product_original_to_internal_map': product_internal_map,   # 0-indexed internal map\n",
    "    }\n",
    "    return data, customer_internal_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2acda706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['CustomerID', 'StockCode', 'InvoiceNo', 'UnitPrice', 'Quantity',\n",
      "       'Country', 'Description', 'InvoiceDate', 'Date', 'Month', 'Year_month',\n",
      "       'day_of_week', 'hour_minute', 'day_sin', 'day_cos', 'month_2',\n",
      "       'month_3', 'month_4', 'month_5', 'month_6', 'month_7', 'month_8',\n",
      "       'month_9', 'month_10', 'month_11', 'month_12', 'TotalPrice', 'Amount',\n",
      "       'FirstOrder', 'CountryID', 'DescriptionID', 't_seconds'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_for_tgn, scaler_price, scaler_quantity, prod_map, cust_map = prepare_data_TGN(train_data)\n",
    "\n",
    "#create a new folder named 'data' in the current directory\n",
    "import os\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "#save the df_for_tgn dataframe to a csv file in the data folder\n",
    "df_for_tgn.to_csv('data/online_retail.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c786fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for TGN script compatibility...\n",
      "Original DataFrame length: 554594\n",
      "Timestamps converted to seconds since first event: 2009-12-31 00:00:00\n",
      "Created node mappings: 5166 customers, 4381 products.\n",
      "Global edge features created. Shape: (554595, 15)\n",
      "Re-indexing complete. Max node ID (1-based): 9547\n",
      "Global node features created. Shape: (9548, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_data, customer_internal_map = create_event_data_for_tgn_script(train_data,scaler_price, scaler_quantity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25482208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sources': array([   1,    2,    2, ...,  834, 4737, 5091]),\n",
       " 'destinations': array([5167, 5168, 5169, ..., 9371, 6159, 9488]),\n",
       " 'timestamps': array([       0.,        0.,        0., ..., 52531200., 52531200.,\n",
       "        52531200.], dtype=float32),\n",
       " 'event_datetimes': array(['2009-12-31T00:00:00.000000000', '2009-12-31T00:00:00.000000000',\n",
       "        '2009-12-31T00:00:00.000000000', ...,\n",
       "        '2011-08-31T00:00:00.000000000', '2011-08-31T00:00:00.000000000',\n",
       "        '2011-08-31T00:00:00.000000000'], dtype='datetime64[ns]'),\n",
       " 'edge_features_global_store': array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 3.6239634 , 10.862664  ,  0.43388373, ...,  0.        ,\n",
       "          0.        ,  1.        ],\n",
       "        [-0.29652938, -0.23069206,  0.43388373, ...,  0.        ,\n",
       "          0.        ,  1.        ],\n",
       "        ...,\n",
       "        [-0.29652938, -0.04580279,  0.9749279 , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.7198947 , -0.394518  ,  0.9749279 , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [-0.15132596, -0.46316886,  0.9749279 , ...,  0.        ,\n",
       "          0.        ,  0.        ]], dtype=float32),\n",
       " 'edge_idxs': array([     1,      2,      3, ..., 554592, 554593, 554594]),\n",
       " 'labels': array([0., 0., 0., ..., 0., 0., 0.], dtype=float32),\n",
       " 'node_features_global_store': array([[0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]], dtype=float32),\n",
       " 'max_node_id': 9547,\n",
       " 'num_total_nodes_for_array_sizing': 9548,\n",
       " 'num_customers': 5166,\n",
       " 'num_products': 4381,\n",
       " 'customer_original_to_internal_map': {12358.0: 1,\n",
       "  16409.0: 2,\n",
       "  16400.0: 3,\n",
       "  16410.0: 4,\n",
       "  16451.0: 5,\n",
       "  16442.0: 6,\n",
       "  16470.0: 7,\n",
       "  16467.0: 8,\n",
       "  16459.0: 9,\n",
       "  16439.0: 10,\n",
       "  16436.0: 11,\n",
       "  16393.0: 12,\n",
       "  16366.0: 13,\n",
       "  16365.0: 14,\n",
       "  16353.0: 15,\n",
       "  16395.0: 16,\n",
       "  16560.0: 17,\n",
       "  16558.0: 18,\n",
       "  16556.0: 19,\n",
       "  16554.0: 20,\n",
       "  16551.0: 21,\n",
       "  16600.0: 22,\n",
       "  16596.0: 23,\n",
       "  16593.0: 24,\n",
       "  16612.0: 25,\n",
       "  16592.0: 26,\n",
       "  16563.0: 27,\n",
       "  16569.0: 28,\n",
       "  16525.0: 29,\n",
       "  16497.0: 30,\n",
       "  16541.0: 31,\n",
       "  16485.0: 32,\n",
       "  16550.0: 33,\n",
       "  16351.0: 34,\n",
       "  16200.0: 35,\n",
       "  16197.0: 36,\n",
       "  16195.0: 37,\n",
       "  16202.0: 38,\n",
       "  16210.0: 39,\n",
       "  16204.0: 40,\n",
       "  16158.0: 41,\n",
       "  16156.0: 42,\n",
       "  16150.0: 43,\n",
       "  16159.0: 44,\n",
       "  16163.0: 45,\n",
       "  16139.0: 46,\n",
       "  16143.0: 47,\n",
       "  16190.0: 48,\n",
       "  16186.0: 49,\n",
       "  16192.0: 50,\n",
       "  16191.0: 51,\n",
       "  16167.0: 52,\n",
       "  16168.0: 53,\n",
       "  16329.0: 54,\n",
       "  16327.0: 55,\n",
       "  16335.0: 56,\n",
       "  16277.0: 57,\n",
       "  16253.0: 58,\n",
       "  16280.0: 59,\n",
       "  16233.0: 60,\n",
       "  16222.0: 61,\n",
       "  16325.0: 62,\n",
       "  16301.0: 63,\n",
       "  16293.0: 64,\n",
       "  16282.0: 65,\n",
       "  16296.0: 66,\n",
       "  16769.0: 67,\n",
       "  16771.0: 68,\n",
       "  16773.0: 69,\n",
       "  16772.0: 70,\n",
       "  16763.0: 71,\n",
       "  16755.0: 72,\n",
       "  16768.0: 73,\n",
       "  16779.0: 74,\n",
       "  16782.0: 75,\n",
       "  16736.0: 76,\n",
       "  16732.0: 77,\n",
       "  16735.0: 78,\n",
       "  16749.0: 79,\n",
       "  16748.0: 80,\n",
       "  16742.0: 81,\n",
       "  16746.0: 82,\n",
       "  16830.0: 83,\n",
       "  16823.0: 84,\n",
       "  16858.0: 85,\n",
       "  16843.0: 86,\n",
       "  16805.0: 87,\n",
       "  16891.0: 88,\n",
       "  16887.0: 89,\n",
       "  16883.0: 90,\n",
       "  16791.0: 91,\n",
       "  16797.0: 92,\n",
       "  16802.0: 93,\n",
       "  16798.0: 94,\n",
       "  16131.0: 95,\n",
       "  16705.0: 96,\n",
       "  16709.0: 97,\n",
       "  16702.0: 98,\n",
       "  16701.0: 99,\n",
       "  16710.0: 100,\n",
       "  16656.0: 101,\n",
       "  16645.0: 102,\n",
       "  16661.0: 103,\n",
       "  16663.0: 104,\n",
       "  16670.0: 105,\n",
       "  16620.0: 106,\n",
       "  16700.0: 107,\n",
       "  16699.0: 108,\n",
       "  16694.0: 109,\n",
       "  16686.0: 110,\n",
       "  16684.0: 111,\n",
       "  16722.0: 112,\n",
       "  16720.0: 113,\n",
       "  16723.0: 114,\n",
       "  16719.0: 115,\n",
       "  16725.0: 116,\n",
       "  16713.0: 117,\n",
       "  16711.0: 118,\n",
       "  16718.0: 119,\n",
       "  16717.0: 120,\n",
       "  15750.0: 121,\n",
       "  15751.0: 122,\n",
       "  15738.0: 123,\n",
       "  15768.0: 124,\n",
       "  15755.0: 125,\n",
       "  15759.0: 126,\n",
       "  15719.0: 127,\n",
       "  15694.0: 128,\n",
       "  15691.0: 129,\n",
       "  15684.0: 130,\n",
       "  15680.0: 131,\n",
       "  15713.0: 132,\n",
       "  15712.0: 133,\n",
       "  15674.0: 134,\n",
       "  15672.0: 135,\n",
       "  15662.0: 136,\n",
       "  15658.0: 137,\n",
       "  15716.0: 138,\n",
       "  15823.0: 139,\n",
       "  15822.0: 140,\n",
       "  15812.0: 141,\n",
       "  15808.0: 142,\n",
       "  15805.0: 143,\n",
       "  15785.0: 144,\n",
       "  15769.0: 145,\n",
       "  15793.0: 146,\n",
       "  15794.0: 147,\n",
       "  15550.0: 148,\n",
       "  15547.0: 149,\n",
       "  15555.0: 150,\n",
       "  15542.0: 151,\n",
       "  15571.0: 152,\n",
       "  15566.0: 153,\n",
       "  15562.0: 154,\n",
       "  15573.0: 155,\n",
       "  15523.0: 156,\n",
       "  15518.0: 157,\n",
       "  15534.0: 158,\n",
       "  15527.0: 159,\n",
       "  15514.0: 160,\n",
       "  15615.0: 161,\n",
       "  15628.0: 162,\n",
       "  15623.0: 163,\n",
       "  15601.0: 164,\n",
       "  15602.0: 165,\n",
       "  15641.0: 166,\n",
       "  15642.0: 167,\n",
       "  15640.0: 168,\n",
       "  15637.0: 169,\n",
       "  15633.0: 170,\n",
       "  15581.0: 171,\n",
       "  15577.0: 172,\n",
       "  15998.0: 173,\n",
       "  15989.0: 174,\n",
       "  15984.0: 175,\n",
       "  16003.0: 176,\n",
       "  15999.0: 177,\n",
       "  15965.0: 178,\n",
       "  15953.0: 179,\n",
       "  15945.0: 180,\n",
       "  15944.0: 181,\n",
       "  15967.0: 182,\n",
       "  15920.0: 183,\n",
       "  15917.0: 184,\n",
       "  15924.0: 185,\n",
       "  15971.0: 186,\n",
       "  15983.0: 187,\n",
       "  16125.0: 188,\n",
       "  16124.0: 189,\n",
       "  16119.0: 190,\n",
       "  16104.0: 191,\n",
       "  16127.0: 192,\n",
       "  16126.0: 193,\n",
       "  16018.0: 194,\n",
       "  16013.0: 195,\n",
       "  16019.0: 196,\n",
       "  16064.0: 197,\n",
       "  16029.0: 198,\n",
       "  16011.0: 199,\n",
       "  16086.0: 200,\n",
       "  16098.0: 201,\n",
       "  15863.0: 202,\n",
       "  15862.0: 203,\n",
       "  15867.0: 204,\n",
       "  15878.0: 205,\n",
       "  15879.0: 206,\n",
       "  15856.0: 207,\n",
       "  15854.0: 208,\n",
       "  15858.0: 209,\n",
       "  15850.0: 210,\n",
       "  15841.0: 211,\n",
       "  15833.0: 212,\n",
       "  15860.0: 213,\n",
       "  15912.0: 214,\n",
       "  15916.0: 215,\n",
       "  15911.0: 216,\n",
       "  15889.0: 217,\n",
       "  15884.0: 218,\n",
       "  15882.0: 219,\n",
       "  15898.0: 220,\n",
       "  17841.0: 221,\n",
       "  17818.0: 222,\n",
       "  17819.0: 223,\n",
       "  17825.0: 224,\n",
       "  17833.0: 225,\n",
       "  17809.0: 226,\n",
       "  17804.0: 227,\n",
       "  17816.0: 228,\n",
       "  17860.0: 229,\n",
       "  17861.0: 230,\n",
       "  17858.0: 231,\n",
       "  17854.0: 232,\n",
       "  17865.0: 233,\n",
       "  17853.0: 234,\n",
       "  17851.0: 235,\n",
       "  17848.0: 236,\n",
       "  17852.0: 237,\n",
       "  17625.0: 238,\n",
       "  17642.0: 239,\n",
       "  17631.0: 240,\n",
       "  17624.0: 241,\n",
       "  17623.0: 242,\n",
       "  17662.0: 243,\n",
       "  17660.0: 244,\n",
       "  17677.0: 245,\n",
       "  17675.0: 246,\n",
       "  17646.0: 247,\n",
       "  17610.0: 248,\n",
       "  17611.0: 249,\n",
       "  17606.0: 250,\n",
       "  17603.0: 251,\n",
       "  17596.0: 252,\n",
       "  17612.0: 253,\n",
       "  17797.0: 254,\n",
       "  17776.0: 255,\n",
       "  17799.0: 256,\n",
       "  17768.0: 257,\n",
       "  17802.0: 258,\n",
       "  17700.0: 259,\n",
       "  17696.0: 260,\n",
       "  17707.0: 261,\n",
       "  17679.0: 262,\n",
       "  17716.0: 263,\n",
       "  17764.0: 264,\n",
       "  17754.0: 265,\n",
       "  17739.0: 266,\n",
       "  17742.0: 267,\n",
       "  18071.0: 268,\n",
       "  18065.0: 269,\n",
       "  18077.0: 270,\n",
       "  18051.0: 271,\n",
       "  18037.0: 272,\n",
       "  18036.0: 273,\n",
       "  18012.0: 274,\n",
       "  18061.0: 275,\n",
       "  18087.0: 276,\n",
       "  18108.0: 277,\n",
       "  18102.0: 278,\n",
       "  18144.0: 279,\n",
       "  18092.0: 280,\n",
       "  18097.0: 281,\n",
       "  17988.0: 282,\n",
       "  17984.0: 283,\n",
       "  17978.0: 284,\n",
       "  18001.0: 285,\n",
       "  17998.0: 286,\n",
       "  18006.0: 287,\n",
       "  18002.0: 288,\n",
       "  17990.0: 289,\n",
       "  18145.0: 290,\n",
       "  18223.0: 291,\n",
       "  18220.0: 292,\n",
       "  18219.0: 293,\n",
       "  18201.0: 294,\n",
       "  18190.0: 295,\n",
       "  18203.0: 296,\n",
       "  18225.0: 297,\n",
       "  18276.0: 298,\n",
       "  18275.0: 299,\n",
       "  18271.0: 300,\n",
       "  18286.0: 301,\n",
       "  18268.0: 302,\n",
       "  18252.0: 303,\n",
       "  18251.0: 304,\n",
       "  18229.0: 305,\n",
       "  18260.0: 306,\n",
       "  18259.0: 307,\n",
       "  18168.0: 308,\n",
       "  18162.0: 309,\n",
       "  18181.0: 310,\n",
       "  18186.0: 311,\n",
       "  18177.0: 312,\n",
       "  18172.0: 313,\n",
       "  17913.0: 314,\n",
       "  17910.0: 315,\n",
       "  17909.0: 316,\n",
       "  17908.0: 317,\n",
       "  17920.0: 318,\n",
       "  17916.0: 319,\n",
       "  17880.0: 320,\n",
       "  17873.0: 321,\n",
       "  17867.0: 322,\n",
       "  17884.0: 323,\n",
       "  17866.0: 324,\n",
       "  17888.0: 325,\n",
       "  17961.0: 326,\n",
       "  17969.0: 327,\n",
       "  17965.0: 328,\n",
       "  17954.0: 329,\n",
       "  17935.0: 330,\n",
       "  17929.0: 331,\n",
       "  17927.0: 332,\n",
       "  17958.0: 333,\n",
       "  17178.0: 334,\n",
       "  17172.0: 335,\n",
       "  17188.0: 336,\n",
       "  17184.0: 337,\n",
       "  17182.0: 338,\n",
       "  17160.0: 339,\n",
       "  17165.0: 340,\n",
       "  17209.0: 341,\n",
       "  17193.0: 342,\n",
       "  17191.0: 343,\n",
       "  17211.0: 344,\n",
       "  17189.0: 345,\n",
       "  17103.0: 346,\n",
       "  17095.0: 347,\n",
       "  17091.0: 348,\n",
       "  17115.0: 349,\n",
       "  17085.0: 350,\n",
       "  17087.0: 351,\n",
       "  17157.0: 352,\n",
       "  17155.0: 353,\n",
       "  17150.0: 354,\n",
       "  17127.0: 355,\n",
       "  17151.0: 356,\n",
       "  17243.0: 357,\n",
       "  17238.0: 358,\n",
       "  17270.0: 359,\n",
       "  17268.0: 360,\n",
       "  17262.0: 361,\n",
       "  17251.0: 362,\n",
       "  17271.0: 363,\n",
       "  17244.0: 364,\n",
       "  17246.0: 365,\n",
       "  17230.0: 366,\n",
       "  17231.0: 367,\n",
       "  17213.0: 368,\n",
       "  17233.0: 369,\n",
       "  16940.0: 370,\n",
       "  16945.0: 371,\n",
       "  16933.0: 372,\n",
       "  16985.0: 373,\n",
       "  16987.0: 374,\n",
       "  16964.0: 375,\n",
       "  16952.0: 376,\n",
       "  16901.0: 377,\n",
       "  16892.0: 378,\n",
       "  16905.0: 379,\n",
       "  16918.0: 380,\n",
       "  16912.0: 381,\n",
       "  16908.0: 382,\n",
       "  17063.0: 383,\n",
       "  17056.0: 384,\n",
       "  17067.0: 385,\n",
       "  17066.0: 386,\n",
       "  17044.0: 387,\n",
       "  17043.0: 388,\n",
       "  17047.0: 389,\n",
       "  17082.0: 390,\n",
       "  17076.0: 391,\n",
       "  17069.0: 392,\n",
       "  17075.0: 393,\n",
       "  17019.0: 394,\n",
       "  17018.0: 395,\n",
       "  17017.0: 396,\n",
       "  17027.0: 397,\n",
       "  17012.0: 398,\n",
       "  17028.0: 399,\n",
       "  17035.0: 400,\n",
       "  17032.0: 401,\n",
       "  17508.0: 402,\n",
       "  17487.0: 403,\n",
       "  17476.0: 404,\n",
       "  17469.0: 405,\n",
       "  17465.0: 406,\n",
       "  17511.0: 407,\n",
       "  17512.0: 408,\n",
       "  17428.0: 409,\n",
       "  17421.0: 410,\n",
       "  17416.0: 411,\n",
       "  17412.0: 412,\n",
       "  17414.0: 413,\n",
       "  17432.0: 414,\n",
       "  17449.0: 415,\n",
       "  17441.0: 416,\n",
       "  17457.0: 417,\n",
       "  17436.0: 418,\n",
       "  17593.0: 419,\n",
       "  17581.0: 420,\n",
       "  17576.0: 421,\n",
       "  17572.0: 422,\n",
       "  17561.0: 423,\n",
       "  17530.0: 424,\n",
       "  17526.0: 425,\n",
       "  17539.0: 426,\n",
       "  17519.0: 427,\n",
       "  17557.0: 428,\n",
       "  17344.0: 429,\n",
       "  17343.0: 430,\n",
       "  17345.0: 431,\n",
       "  17329.0: 432,\n",
       "  17364.0: 433,\n",
       "  17351.0: 434,\n",
       "  17287.0: 435,\n",
       "  17293.0: 436,\n",
       "  17290.0: 437,\n",
       "  17296.0: 438,\n",
       "  17322.0: 439,\n",
       "  17318.0: 440,\n",
       "  17324.0: 441,\n",
       "  17314.0: 442,\n",
       "  17400.0: 443,\n",
       "  17377.0: 444,\n",
       "  17389.0: 445,\n",
       "  17411.0: 446,\n",
       "  17409.0: 447,\n",
       "  17406.0: 448,\n",
       "  17402.0: 449,\n",
       "  17371.0: 450,\n",
       "  17368.0: 451,\n",
       "  17372.0: 452,\n",
       "  13590.0: 453,\n",
       "  13589.0: 454,\n",
       "  13599.0: 455,\n",
       "  13591.0: 456,\n",
       "  13576.0: 457,\n",
       "  13571.0: 458,\n",
       "  13564.0: 459,\n",
       "  13585.0: 460,\n",
       "  13577.0: 461,\n",
       "  13623.0: 462,\n",
       "  13611.0: 463,\n",
       "  13520.0: 464,\n",
       "  13510.0: 465,\n",
       "  13526.0: 466,\n",
       "  13498.0: 467,\n",
       "  13495.0: 468,\n",
       "  13491.0: 469,\n",
       "  13557.0: 470,\n",
       "  13542.0: 471,\n",
       "  13531.0: 472,\n",
       "  13635.0: 473,\n",
       "  13693.0: 474,\n",
       "  13694.0: 475,\n",
       "  13680.0: 476,\n",
       "  13735.0: 477,\n",
       "  13733.0: 478,\n",
       "  13736.0: 479,\n",
       "  13728.0: 480,\n",
       "  13727.0: 481,\n",
       "  13711.0: 482,\n",
       "  13643.0: 483,\n",
       "  13641.0: 484,\n",
       "  13654.0: 485,\n",
       "  13648.0: 486,\n",
       "  13666.0: 487,\n",
       "  13668.0: 488,\n",
       "  13679.0: 489,\n",
       "  13672.0: 490,\n",
       "  13319.0: 491,\n",
       "  13327.0: 492,\n",
       "  13317.0: 493,\n",
       "  13313.0: 494,\n",
       "  13289.0: 495,\n",
       "  13373.0: 496,\n",
       "  13381.0: 497,\n",
       "  13382.0: 498,\n",
       "  13393.0: 499,\n",
       "  13329.0: 500,\n",
       "  13225.0: 501,\n",
       "  13211.0: 502,\n",
       "  13208.0: 503,\n",
       "  13234.0: 504,\n",
       "  13206.0: 505,\n",
       "  13269.0: 506,\n",
       "  13267.0: 507,\n",
       "  13266.0: 508,\n",
       "  13455.0: 509,\n",
       "  13448.0: 510,\n",
       "  13457.0: 511,\n",
       "  13442.0: 512,\n",
       "  13462.0: 513,\n",
       "  13481.0: 514,\n",
       "  13473.0: 515,\n",
       "  13485.0: 516,\n",
       "  13487.0: 517,\n",
       "  13468.0: 518,\n",
       "  13415.0: 519,\n",
       "  13410.0: 520,\n",
       "  13408.0: 521,\n",
       "  13403.0: 522,\n",
       "  13394.0: 523,\n",
       "  13405.0: 524,\n",
       "  13426.0: 525,\n",
       "  13427.0: 526,\n",
       "  13418.0: 527,\n",
       "  13416.0: 528,\n",
       "  14031.0: 529,\n",
       "  14032.0: 530,\n",
       "  14030.0: 531,\n",
       "  14037.0: 532,\n",
       "  14040.0: 533,\n",
       "  13984.0: 534,\n",
       "  13983.0: 535,\n",
       "  13979.0: 536,\n",
       "  13993.0: 537,\n",
       "  13952.0: 538,\n",
       "  13958.0: 539,\n",
       "  14003.0: 540,\n",
       "  13999.0: 541,\n",
       "  13995.0: 542,\n",
       "  14000.0: 543,\n",
       "  14081.0: 544,\n",
       "  14085.0: 545,\n",
       "  14083.0: 546,\n",
       "  14055.0: 547,\n",
       "  14051.0: 548,\n",
       "  14057.0: 549,\n",
       "  14061.0: 550,\n",
       "  14043.0: 551,\n",
       "  14042.0: 552,\n",
       "  14080.0: 553,\n",
       "  14062.0: 554,\n",
       "  14063.0: 555,\n",
       "  13777.0: 556,\n",
       "  13770.0: 557,\n",
       "  13769.0: 558,\n",
       "  13782.0: 559,\n",
       "  13781.0: 560,\n",
       "  13768.0: 561,\n",
       "  13767.0: 562,\n",
       "  13813.0: 563,\n",
       "  13807.0: 564,\n",
       "  13800.0: 565,\n",
       "  13819.0: 566,\n",
       "  13786.0: 567,\n",
       "  13798.0: 568,\n",
       "  13758.0: 569,\n",
       "  13756.0: 570,\n",
       "  13740.0: 571,\n",
       "  13748.0: 572,\n",
       "  13753.0: 573,\n",
       "  13931.0: 574,\n",
       "  13921.0: 575,\n",
       "  13905.0: 576,\n",
       "  13902.0: 577,\n",
       "  13895.0: 578,\n",
       "  13890.0: 579,\n",
       "  13946.0: 580,\n",
       "  13941.0: 581,\n",
       "  13948.0: 582,\n",
       "  13831.0: 583,\n",
       "  13825.0: 584,\n",
       "  13855.0: 585,\n",
       "  13847.0: 586,\n",
       "  13823.0: 587,\n",
       "  13872.0: 588,\n",
       "  13871.0: 589,\n",
       "  13875.0: 590,\n",
       "  12758.0: 591,\n",
       "  12759.0: 592,\n",
       "  12748.0: 593,\n",
       "  12835.0: 594,\n",
       "  12826.0: 595,\n",
       "  12820.0: 596,\n",
       "  12785.0: 597,\n",
       "  12779.0: 598,\n",
       "  12709.0: 599,\n",
       "  12708.0: 600,\n",
       "  12712.0: 601,\n",
       "  12683.0: 602,\n",
       "  12714.0: 603,\n",
       "  12747.0: 604,\n",
       "  12921.0: 605,\n",
       "  12920.0: 606,\n",
       "  12917.0: 607,\n",
       "  12872.0: 608,\n",
       "  12913.0: 609,\n",
       "  12926.0: 610,\n",
       "  12924.0: 611,\n",
       "  12867.0: 612,\n",
       "  12839.0: 613,\n",
       "  12836.0: 614,\n",
       "  12842.0: 615,\n",
       "  12843.0: 616,\n",
       "  12854.0: 617,\n",
       "  12455.0: 618,\n",
       "  12471.0: 619,\n",
       "  12472.0: 620,\n",
       "  12490.0: 621,\n",
       "  12487.0: 622,\n",
       "  12437.0: 623,\n",
       "  12422.0: 624,\n",
       "  12417.0: 625,\n",
       "  12439.0: 626,\n",
       "  12359.0: 627,\n",
       "  12440.0: 628,\n",
       "  12443.0: 629,\n",
       "  12613.0: 630,\n",
       "  12609.0: 631,\n",
       "  12586.0: 632,\n",
       "  12583.0: 633,\n",
       "  12600.0: 634,\n",
       "  12682.0: 635,\n",
       "  12681.0: 636,\n",
       "  12664.0: 637,\n",
       "  12533.0: 638,\n",
       "  12539.0: 639,\n",
       "  12510.0: 640,\n",
       "  12523.0: 641,\n",
       "  12577.0: 642,\n",
       "  12540.0: 643,\n",
       "  13097.0: 644,\n",
       "  13093.0: 645,\n",
       "  13089.0: 646,\n",
       "  13087.0: 647,\n",
       "  13111.0: 648,\n",
       "  13110.0: 649,\n",
       "  13136.0: 650,\n",
       "  13114.0: 651,\n",
       "  13078.0: 652,\n",
       "  13077.0: 653,\n",
       "  13081.0: 654,\n",
       "  13074.0: 655,\n",
       "  13070.0: 656,\n",
       "  13082.0: 657,\n",
       "  13085.0: 658,\n",
       "  13204.0: 659,\n",
       "  13203.0: 660,\n",
       "  13202.0: 661,\n",
       "  13190.0: 662,\n",
       "  13178.0: 663,\n",
       "  13162.0: 664,\n",
       "  13154.0: 665,\n",
       "  13173.0: 666,\n",
       "  13148.0: 667,\n",
       "  13141.0: 668,\n",
       "  13175.0: 669,\n",
       "  13174.0: 670,\n",
       "  12970.0: 671,\n",
       "  12975.0: 672,\n",
       "  12971.0: 673,\n",
       "  12957.0: 674,\n",
       "  12979.0: 675,\n",
       "  12978.0: 676,\n",
       "  12941.0: 677,\n",
       "  12937.0: 678,\n",
       "  12949.0: 679,\n",
       "  12948.0: 680,\n",
       "  12933.0: 681,\n",
       "  12931.0: 682,\n",
       "  12934.0: 683,\n",
       "  12956.0: 684,\n",
       "  13037.0: 685,\n",
       "  13026.0: 686,\n",
       "  13042.0: 687,\n",
       "  13039.0: 688,\n",
       "  13021.0: 689,\n",
       "  13019.0: 690,\n",
       "  13025.0: 691,\n",
       "  13047.0: 692,\n",
       "  13050.0: 693,\n",
       "  13001.0: 694,\n",
       "  13000.0: 695,\n",
       "  12989.0: 696,\n",
       "  12980.0: 697,\n",
       "  12992.0: 698,\n",
       "  13013.0: 699,\n",
       "  13008.0: 700,\n",
       "  13004.0: 701,\n",
       "  14961.0: 702,\n",
       "  14958.0: 703,\n",
       "  14979.0: 704,\n",
       "  14980.0: 705,\n",
       "  14977.0: 706,\n",
       "  14966.0: 707,\n",
       "  14967.0: 708,\n",
       "  14955.0: 709,\n",
       "  14911.0: 710,\n",
       "  14952.0: 711,\n",
       "  14949.0: 712,\n",
       "  14942.0: 713,\n",
       "  14940.0: 714,\n",
       "  14915.0: 715,\n",
       "  15044.0: 716,\n",
       "  15051.0: 717,\n",
       "  15039.0: 718,\n",
       "  15059.0: 719,\n",
       "  15057.0: 720,\n",
       "  15055.0: 721,\n",
       "  15005.0: 722,\n",
       "  14990.0: 723,\n",
       "  14987.0: 724,\n",
       "  15032.0: 725,\n",
       "  15031.0: 726,\n",
       "  15015.0: 727,\n",
       "  14755.0: 728,\n",
       "  14751.0: 729,\n",
       "  14750.0: 730,\n",
       "  14769.0: 731,\n",
       "  14739.0: 732,\n",
       "  14738.0: 733,\n",
       "  14790.0: 734,\n",
       "  14771.0: 735,\n",
       "  14796.0: 736,\n",
       "  14735.0: 737,\n",
       "  14702.0: 738,\n",
       "  14709.0: 739,\n",
       "  14695.0: 740,\n",
       "  14732.0: 741,\n",
       "  14723.0: 742,\n",
       "  14717.0: 743,\n",
       "  14713.0: 744,\n",
       "  14710.0: 745,\n",
       "  14897.0: 746,\n",
       "  14896.0: 747,\n",
       "  14898.0: 748,\n",
       "  14895.0: 749,\n",
       "  14905.0: 750,\n",
       "  14831.0: 751,\n",
       "  14838.0: 752,\n",
       "  14841.0: 753,\n",
       "  14819.0: 754,\n",
       "  14799.0: 755,\n",
       "  14828.0: 756,\n",
       "  14871.0: 757,\n",
       "  14869.0: 758,\n",
       "  14894.0: 759,\n",
       "  14861.0: 760,\n",
       "  14867.0: 761,\n",
       "  14865.0: 762,\n",
       "  15409.0: 763,\n",
       "  15403.0: 764,\n",
       "  15394.0: 765,\n",
       "  15393.0: 766,\n",
       "  15420.0: 767,\n",
       "  15413.0: 768,\n",
       "  15382.0: 769,\n",
       "  15380.0: 770,\n",
       "  15421.0: 771,\n",
       "  15456.0: 772,\n",
       "  15446.0: 773,\n",
       "  15428.0: 774,\n",
       "  15441.0: 775,\n",
       "  15353.0: 776,\n",
       "  15358.0: 777,\n",
       "  15329.0: 778,\n",
       "  15326.0: 779,\n",
       "  15367.0: 780,\n",
       "  15362.0: 781,\n",
       "  15369.0: 782,\n",
       "  15498.0: 783,\n",
       "  15494.0: 784,\n",
       "  15497.0: 785,\n",
       "  15503.0: 786,\n",
       "  15499.0: 787,\n",
       "  15466.0: 788,\n",
       "  15465.0: 789,\n",
       "  15479.0: 790,\n",
       "  15485.0: 791,\n",
       "  15484.0: 792,\n",
       "  15482.0: 793,\n",
       "  15179.0: 794,\n",
       "  15188.0: 795,\n",
       "  15184.0: 796,\n",
       "  15181.0: 797,\n",
       "  15169.0: 798,\n",
       "  15162.0: 799,\n",
       "  15196.0: 800,\n",
       "  15194.0: 801,\n",
       "  15203.0: 802,\n",
       "  15197.0: 803,\n",
       "  15077.0: 804,\n",
       "  15070.0: 805,\n",
       "  15078.0: 806,\n",
       "  15061.0: 807,\n",
       "  15064.0: 808,\n",
       "  15144.0: 809,\n",
       "  15115.0: 810,\n",
       "  15109.0: 811,\n",
       "  15157.0: 812,\n",
       "  15093.0: 813,\n",
       "  15079.0: 814,\n",
       "  15085.0: 815,\n",
       "  15311.0: 816,\n",
       "  15312.0: 817,\n",
       "  15240.0: 818,\n",
       "  15260.0: 819,\n",
       "  15255.0: 820,\n",
       "  15209.0: 821,\n",
       "  15222.0: 822,\n",
       "  15223.0: 823,\n",
       "  15309.0: 824,\n",
       "  15302.0: 825,\n",
       "  15270.0: 826,\n",
       "  15265.0: 827,\n",
       "  15287.0: 828,\n",
       "  15298.0: 829,\n",
       "  15291.0: 830,\n",
       "  14329.0: 831,\n",
       "  14307.0: 832,\n",
       "  14341.0: 833,\n",
       "  14299.0: 834,\n",
       "  14298.0: 835,\n",
       "  14302.0: 836,\n",
       "  14401.0: 837,\n",
       "  14396.0: 838,\n",
       "  14404.0: 839,\n",
       "  14408.0: 840,\n",
       "  14347.0: 841,\n",
       "  14286.0: 842,\n",
       "  14258.0: 843,\n",
       "  14239.0: 844,\n",
       "  14285.0: 845,\n",
       "  14277.0: 846,\n",
       "  14293.0: 847,\n",
       "  14291.0: 848,\n",
       "  14295.0: 849,\n",
       "  14290.0: 850,\n",
       "  14484.0: 851,\n",
       "  14483.0: 852,\n",
       "  14480.0: 853,\n",
       "  14505.0: 854,\n",
       "  14494.0: 855,\n",
       "  14479.0: 856,\n",
       "  14478.0: 857,\n",
       "  14471.0: 858,\n",
       "  14422.0: 859,\n",
       "  14425.0: 860,\n",
       "  14440.0: 861,\n",
       "  14415.0: 862,\n",
       "  14468.0: 863,\n",
       "  14467.0: 864,\n",
       "  14454.0: 865,\n",
       "  14460.0: 866,\n",
       "  14466.0: 867,\n",
       "  14238.0: 868,\n",
       "  14215.0: 869,\n",
       "  14130.0: 870,\n",
       "  14123.0: 871,\n",
       "  14113.0: 872,\n",
       "  14110.0: 873,\n",
       "  14122.0: 874,\n",
       "  14119.0: 875,\n",
       "  14136.0: 876,\n",
       "  14135.0: 877,\n",
       "  14151.0: 878,\n",
       "  14133.0: 879,\n",
       "  14134.0: 880,\n",
       "  14156.0: 881,\n",
       "  14108.0: 882,\n",
       "  14087.0: 883,\n",
       "  14106.0: 884,\n",
       "  14105.0: 885,\n",
       "  14107.0: 886,\n",
       "  14099.0: 887,\n",
       "  14095.0: 888,\n",
       "  14092.0: 889,\n",
       "  14161.0: 890,\n",
       "  14159.0: 891,\n",
       "  14211.0: 892,\n",
       "  14204.0: 893,\n",
       "  14173.0: 894,\n",
       "  14171.0: 895,\n",
       "  14646.0: 896,\n",
       "  14644.0: 897,\n",
       "  14643.0: 898,\n",
       "  14625.0: 899,\n",
       "  14626.0: 900,\n",
       "  14648.0: 901,\n",
       "  14653.0: 902,\n",
       "  14606.0: 903,\n",
       "  14608.0: 904,\n",
       "  14607.0: 905,\n",
       "  14680.0: 906,\n",
       "  14679.0: 907,\n",
       "  14670.0: 908,\n",
       "  14685.0: 909,\n",
       "  14667.0: 910,\n",
       "  14669.0: 911,\n",
       "  14654.0: 912,\n",
       "  14662.0: 913,\n",
       "  14527.0: 914,\n",
       "  14540.0: 915,\n",
       "  14546.0: 916,\n",
       "  14543.0: 917,\n",
       "  14514.0: 918,\n",
       "  14519.0: 919,\n",
       "  14507.0: 920,\n",
       "  14512.0: 921,\n",
       "  14525.0: 922,\n",
       "  14590.0: 923,\n",
       "  14577.0: 924,\n",
       "  14593.0: 925,\n",
       "  14564.0: 926,\n",
       "  14574.0: 927,\n",
       "  14605.0: 928,\n",
       "  14548.0: 929,\n",
       "  14549.0: 930,\n",
       "  14558.0: 931,\n",
       "  14555.0: 932,\n",
       "  14560.0: 933,\n",
       "  15959.0: 934,\n",
       "  15910.0: 935,\n",
       "  15903.0: 936,\n",
       "  15950.0: 937,\n",
       "  15943.0: 938,\n",
       "  16085.0: 939,\n",
       "  16017.0: 940,\n",
       "  16014.0: 941,\n",
       "  16057.0: 942,\n",
       "  16027.0: 943,\n",
       "  16051.0: 944,\n",
       "  16004.0: 945,\n",
       "  15845.0: 946,\n",
       "  15827.0: 947,\n",
       "  15771.0: 948,\n",
       "  15798.0: 949,\n",
       "  15797.0: 950,\n",
       "  15795.0: 951,\n",
       "  15874.0: 952,\n",
       "  15885.0: 953,\n",
       "  15865.0: 954,\n",
       "  15861.0: 955,\n",
       "  16524.0: 956,\n",
       "  16523.0: 957,\n",
       "  16510.0: 958,\n",
       "  16422.0: 959,\n",
       "  16414.0: 960,\n",
       "  16390.0: 961,\n",
       "  16430.0: 962,\n",
       "  16434.0: 963,\n",
       "  16481.0: 964,\n",
       "  16452.0: 965,\n",
       "  16585.0: 966,\n",
       "  16655.0: 967,\n",
       "  16641.0: 968,\n",
       "  16673.0: 969,\n",
       "  16660.0: 970,\n",
       "  16595.0: 971,\n",
       "  16137.0: 972,\n",
       "  16166.0: 973,\n",
       "  16206.0: 974,\n",
       "  16109.0: 975,\n",
       "  16103.0: 976,\n",
       "  16087.0: 977,\n",
       "  16326.0: 978,\n",
       "  16320.0: 979,\n",
       "  16255.0: 980,\n",
       "  16281.0: 981,\n",
       "  16304.0: 982,\n",
       "  16287.0: 983,\n",
       "  15266.0: 984,\n",
       "  15259.0: 985,\n",
       "  15296.0: 986,\n",
       "  15251.0: 987,\n",
       "  15224.0: 988,\n",
       "  15191.0: 989,\n",
       "  15192.0: 990,\n",
       "  15186.0: 991,\n",
       "  15218.0: 992,\n",
       "  15198.0: 993,\n",
       "  15317.0: 994,\n",
       "  15324.0: 995,\n",
       "  15338.0: 996,\n",
       "  15359.0: 997,\n",
       "  15303.0: 998,\n",
       "  15022.0: 999,\n",
       "  15013.0: 1000,\n",
       "  ...},\n",
       " 'product_original_to_internal_map': {'15056BL': 5167,\n",
       "  '72349B': 5168,\n",
       "  '72008': 5169,\n",
       "  '47566': 5170,\n",
       "  '37503': 5171,\n",
       "  '37370': 5172,\n",
       "  '35995': 5173,\n",
       "  '35991': 5174,\n",
       "  '35824C': 5175,\n",
       "  '35607A': 5176,\n",
       "  '35271S': 5177,\n",
       "  '22350': 5178,\n",
       "  '22293': 5179,\n",
       "  '22274': 5180,\n",
       "  '22273': 5181,\n",
       "  '22268': 5182,\n",
       "  '22156': 5183,\n",
       "  '22147': 5184,\n",
       "  '22145': 5185,\n",
       "  '22144': 5186,\n",
       "  '22142': 5187,\n",
       "  '22119': 5188,\n",
       "  '72351B': 5189,\n",
       "  '72756': 5190,\n",
       "  '72760B': 5191,\n",
       "  '75049L': 5192,\n",
       "  '85112': 5193,\n",
       "  '85065': 5194,\n",
       "  '85040B': 5195,\n",
       "  '85040A': 5196,\n",
       "  '84997D': 5197,\n",
       "  '84997B': 5198,\n",
       "  '84997A': 5199,\n",
       "  '84993B': 5200,\n",
       "  '84993A': 5201,\n",
       "  '84992': 5202,\n",
       "  '22117': 5203,\n",
       "  '84991': 5204,\n",
       "  '84879': 5205,\n",
       "  '84839': 5206,\n",
       "  '84596K': 5207,\n",
       "  '84596F': 5208,\n",
       "  '84596B': 5209,\n",
       "  '84578': 5210,\n",
       "  '84520D': 5211,\n",
       "  '84519A': 5212,\n",
       "  '82599': 5213,\n",
       "  '82494L': 5214,\n",
       "  '84989A': 5215,\n",
       "  '85123A': 5216,\n",
       "  '22112': 5217,\n",
       "  '22103': 5218,\n",
       "  '20749': 5219,\n",
       "  '20702': 5220,\n",
       "  '17091J': 5221,\n",
       "  '17091F': 5222,\n",
       "  '17091A': 5223,\n",
       "  '85178': 5224,\n",
       "  '85176': 5225,\n",
       "  '85049F': 5226,\n",
       "  '85049E': 5227,\n",
       "  '84837': 5228,\n",
       "  '84682': 5229,\n",
       "  '84615': 5230,\n",
       "  '84569D': 5231,\n",
       "  '84569C': 5232,\n",
       "  '84569B': 5233,\n",
       "  '84569A': 5234,\n",
       "  '84482': 5235,\n",
       "  '82486': 5236,\n",
       "  '82482': 5237,\n",
       "  '20762': 5238,\n",
       "  '20764': 5239,\n",
       "  '20943': 5240,\n",
       "  '21136': 5241,\n",
       "  '22090': 5242,\n",
       "  '21977': 5243,\n",
       "  '21976': 5244,\n",
       "  '21975': 5245,\n",
       "  '21877': 5246,\n",
       "  '21874': 5247,\n",
       "  '21872': 5248,\n",
       "  '21871': 5249,\n",
       "  '21805': 5250,\n",
       "  '21768': 5251,\n",
       "  '22111': 5252,\n",
       "  '21755': 5253,\n",
       "  '21740': 5254,\n",
       "  '21739': 5255,\n",
       "  '21738': 5256,\n",
       "  '21447': 5257,\n",
       "  '21348': 5258,\n",
       "  '21288': 5259,\n",
       "  '21215': 5260,\n",
       "  '21213': 5261,\n",
       "  '21212': 5262,\n",
       "  '21175': 5263,\n",
       "  '21741': 5264,\n",
       "  '47591D': 5265,\n",
       "  '85152': 5266,\n",
       "  '22149': 5267,\n",
       "  '21232': 5268,\n",
       "  '21231': 5269,\n",
       "  '21032': 5270,\n",
       "  '21031': 5271,\n",
       "  '21027': 5272,\n",
       "  '21026': 5273,\n",
       "  '20867': 5274,\n",
       "  '20723': 5275,\n",
       "  '20665': 5276,\n",
       "  '85166A': 5277,\n",
       "  '85114C': 5278,\n",
       "  '85047': 5279,\n",
       "  '85024C': 5280,\n",
       "  '84870B': 5281,\n",
       "  '82551': 5282,\n",
       "  '72802C': 5283,\n",
       "  '72802B': 5284,\n",
       "  '62074B': 5285,\n",
       "  '48187': 5286,\n",
       "  '47570B': 5287,\n",
       "  '21429': 5288,\n",
       "  '21479': 5289,\n",
       "  '21481': 5290,\n",
       "  '21482': 5291,\n",
       "  '21251': 5292,\n",
       "  '21034': 5293,\n",
       "  '20910': 5294,\n",
       "  '16238': 5295,\n",
       "  '10002': 5296,\n",
       "  '84327A': 5297,\n",
       "  '21156': 5298,\n",
       "  '21051': 5299,\n",
       "  '21042': 5300,\n",
       "  '17033': 5301,\n",
       "  '85217': 5302,\n",
       "  '84535A': 5303,\n",
       "  '84510A': 5304,\n",
       "  '47568': 5305,\n",
       "  '22356': 5306,\n",
       "  '22355': 5307,\n",
       "  '22125': 5308,\n",
       "  '21581': 5309,\n",
       "  '21039': 5310,\n",
       "  '16046': 5311,\n",
       "  '22349': 5312,\n",
       "  '22304': 5313,\n",
       "  '21784': 5314,\n",
       "  '21484': 5315,\n",
       "  '85038': 5316,\n",
       "  '84948': 5317,\n",
       "  '84947': 5318,\n",
       "  '84406B': 5319,\n",
       "  '22301': 5320,\n",
       "  '22300': 5321,\n",
       "  '22294': 5322,\n",
       "  '22170': 5323,\n",
       "  '21385': 5324,\n",
       "  '20940': 5325,\n",
       "  '20886': 5326,\n",
       "  '18097C': 5327,\n",
       "  '82616A': 5328,\n",
       "  '35969': 5329,\n",
       "  '22114': 5330,\n",
       "  '22122': 5331,\n",
       "  '22187': 5332,\n",
       "  '22175': 5333,\n",
       "  '21931': 5334,\n",
       "  '21928': 5335,\n",
       "  '21586': 5336,\n",
       "  '21446': 5337,\n",
       "  '21317': 5338,\n",
       "  '21245': 5339,\n",
       "  '22330': 5340,\n",
       "  '21244': 5341,\n",
       "  '21240': 5342,\n",
       "  '21153': 5343,\n",
       "  '21012': 5344,\n",
       "  '20974': 5345,\n",
       "  '20970': 5346,\n",
       "  '20717': 5347,\n",
       "  '20675': 5348,\n",
       "  '20674': 5349,\n",
       "  '79341': 5350,\n",
       "  '21241': 5351,\n",
       "  '47591C': 5352,\n",
       "  '47503G': 5353,\n",
       "  '21677': 5354,\n",
       "  '21650': 5355,\n",
       "  '21611': 5356,\n",
       "  '21579': 5357,\n",
       "  '21578': 5358,\n",
       "  '21577': 5359,\n",
       "  '21561': 5360,\n",
       "  '21493': 5361,\n",
       "  '21491': 5362,\n",
       "  '21480': 5363,\n",
       "  '21440': 5364,\n",
       "  '21379': 5365,\n",
       "  '21377': 5366,\n",
       "  '21375': 5367,\n",
       "  '21357': 5368,\n",
       "  '21356': 5369,\n",
       "  '21186': 5370,\n",
       "  '21166': 5371,\n",
       "  '21680': 5372,\n",
       "  '21730': 5373,\n",
       "  '21747': 5374,\n",
       "  '21786': 5375,\n",
       "  '22139': 5376,\n",
       "  '22113': 5377,\n",
       "  '22109': 5378,\n",
       "  '22107': 5379,\n",
       "  '22086': 5380,\n",
       "  '22075': 5381,\n",
       "  '22068': 5382,\n",
       "  '22064': 5383,\n",
       "  '21990': 5384,\n",
       "  '21948': 5385,\n",
       "  '21122': 5386,\n",
       "  '21934': 5387,\n",
       "  '21918': 5388,\n",
       "  '21892': 5389,\n",
       "  '21890': 5390,\n",
       "  '21865': 5391,\n",
       "  '21864': 5392,\n",
       "  '21832': 5393,\n",
       "  '21809': 5394,\n",
       "  '21791': 5395,\n",
       "  '21789': 5396,\n",
       "  '21788': 5397,\n",
       "  '21071': 5398,\n",
       "  '37450': 5399,\n",
       "  '37446': 5400,\n",
       "  '22065': 5401,\n",
       "  '22063': 5402,\n",
       "  '22055': 5403,\n",
       "  '22054': 5404,\n",
       "  '15060B': 5405,\n",
       "  '85049A': 5406,\n",
       "  '84763': 5407,\n",
       "  '47599A': 5408,\n",
       "  '35981C': 5409,\n",
       "  '21114': 5410,\n",
       "  '37495': 5411,\n",
       "  '47580': 5412,\n",
       "  '84509G': 5413,\n",
       "  '84529A': 5414,\n",
       "  '21018': 5415,\n",
       "  '20975': 5416,\n",
       "  '20966': 5417,\n",
       "  '20961': 5418,\n",
       "  '20960': 5419,\n",
       "  '20956': 5420,\n",
       "  '20814': 5421,\n",
       "  '20754': 5422,\n",
       "  '20752': 5423,\n",
       "  '20728': 5424,\n",
       "  '21068': 5425,\n",
       "  '20727': 5426,\n",
       "  '20712': 5427,\n",
       "  '20677': 5428,\n",
       "  '20676': 5429,\n",
       "  '20660': 5430,\n",
       "  '20655': 5431,\n",
       "  '17164D': 5432,\n",
       "  '16047': 5433,\n",
       "  '84529C': 5434,\n",
       "  '20713': 5435,\n",
       "  '22150': 5436,\n",
       "  '22174': 5437,\n",
       "  '84893': 5438,\n",
       "  '84559D': 5439,\n",
       "  '84559B': 5440,\n",
       "  '84559A': 5441,\n",
       "  '84536A': 5442,\n",
       "  '84032B': 5443,\n",
       "  '84032A': 5444,\n",
       "  '72805': 5445,\n",
       "  '22276': 5446,\n",
       "  '22138': 5447,\n",
       "  '22095': 5448,\n",
       "  '22091': 5449,\n",
       "  '21993': 5450,\n",
       "  '21991': 5451,\n",
       "  '21912': 5452,\n",
       "  '21888': 5453,\n",
       "  '21875': 5454,\n",
       "  '21826': 5455,\n",
       "  '85048': 5456,\n",
       "  '85099B': 5457,\n",
       "  '85099C': 5458,\n",
       "  '85147': 5459,\n",
       "  '37502': 5460,\n",
       "  '37494A': 5461,\n",
       "  '37447': 5462,\n",
       "  '22082': 5463,\n",
       "  '22079': 5464,\n",
       "  '22078': 5465,\n",
       "  '22056': 5466,\n",
       "  '21737': 5467,\n",
       "  '21155': 5468,\n",
       "  '21154': 5469,\n",
       "  '21124': 5470,\n",
       "  '21110': 5471,\n",
       "  '21109': 5472,\n",
       "  '90196B': 5473,\n",
       "  '85231B': 5474,\n",
       "  '21790': 5475,\n",
       "  '21593': 5476,\n",
       "  '21499': 5477,\n",
       "  '84671B': 5478,\n",
       "  '84388': 5479,\n",
       "  '84375': 5480,\n",
       "  '79067': 5481,\n",
       "  '70007': 5482,\n",
       "  '70006': 5483,\n",
       "  '51012C': 5484,\n",
       "  '48179': 5485,\n",
       "  '48129': 5486,\n",
       "  '47504K': 5487,\n",
       "  '84946': 5488,\n",
       "  '47504F': 5489,\n",
       "  '22353': 5490,\n",
       "  '22295': 5491,\n",
       "  '22197': 5492,\n",
       "  '22189': 5493,\n",
       "  '22186': 5494,\n",
       "  '22185': 5495,\n",
       "  '84952B': 5496,\n",
       "  '85071A': 5497,\n",
       "  '21063': 5498,\n",
       "  '21059': 5499,\n",
       "  '20996': 5500,\n",
       "  '20992': 5501,\n",
       "  '20865': 5502,\n",
       "  '20864': 5503,\n",
       "  '20862': 5504,\n",
       "  '85033S': 5505,\n",
       "  '20857': 5506,\n",
       "  '20855': 5507,\n",
       "  '20773': 5508,\n",
       "  '16235': 5509,\n",
       "  '16156S': 5510,\n",
       "  '85227': 5511,\n",
       "  '85170D': 5512,\n",
       "  '85170A': 5513,\n",
       "  '85071B': 5514,\n",
       "  '20856': 5515,\n",
       "  '21289': 5516,\n",
       "  '21217': 5517,\n",
       "  '21086': 5518,\n",
       "  '20750': 5519,\n",
       "  '85226A': 5520,\n",
       "  '85014B': 5521,\n",
       "  '22193': 5522,\n",
       "  '22190': 5523,\n",
       "  '22083': 5524,\n",
       "  '21490': 5525,\n",
       "  '85222': 5526,\n",
       "  '85049D': 5527,\n",
       "  '85049B': 5528,\n",
       "  '21291': 5529,\n",
       "  '21292': 5530,\n",
       "  '21477': 5531,\n",
       "  '22221': 5532,\n",
       "  '22220': 5533,\n",
       "  '22073': 5534,\n",
       "  '22072': 5535,\n",
       "  '85035A': 5536,\n",
       "  '22059': 5537,\n",
       "  '22028': 5538,\n",
       "  '22024': 5539,\n",
       "  '21536': 5540,\n",
       "  '21535': 5541,\n",
       "  '21533': 5542,\n",
       "  '21528': 5543,\n",
       "  '21527': 5544,\n",
       "  '21485': 5545,\n",
       "  '22236': 5546,\n",
       "  '85014C': 5547,\n",
       "  '84968B': 5548,\n",
       "  '85066': 5549,\n",
       "  '85061W': 5550,\n",
       "  '85042': 5551,\n",
       "  '85035B': 5552,\n",
       "  '84971L': 5553,\n",
       "  '84970L': 5554,\n",
       "  '84836': 5555,\n",
       "  '84029E': 5556,\n",
       "  '22183': 5557,\n",
       "  '22173': 5558,\n",
       "  '21744': 5559,\n",
       "  '21410': 5560,\n",
       "  '21185': 5561,\n",
       "  '20658': 5562,\n",
       "  '21040': 5563,\n",
       "  '21067': 5564,\n",
       "  '84509B': 5565,\n",
       "  '84279B': 5566,\n",
       "  '71038': 5567,\n",
       "  '46000S': 5568,\n",
       "  '22296': 5569,\n",
       "  '84968F': 5570,\n",
       "  '22279': 5571,\n",
       "  '22178': 5572,\n",
       "  '22045': 5573,\n",
       "  '21955': 5574,\n",
       "  '21866': 5575,\n",
       "  '21733': 5576,\n",
       "  '21656': 5577,\n",
       "  '22191': 5578,\n",
       "  '22309': 5579,\n",
       "  '35241': 5580,\n",
       "  '37379A': 5581,\n",
       "  '21896': 5582,\n",
       "  '21895': 5583,\n",
       "  '21754': 5584,\n",
       "  '20993': 5585,\n",
       "  '20984': 5586,\n",
       "  '20892': 5587,\n",
       "  '20770': 5588,\n",
       "  '16225': 5589,\n",
       "  '21841': 5590,\n",
       "  '21108': 5591,\n",
       "  '37449': 5592,\n",
       "  '22061': 5593,\n",
       "  '22060': 5594,\n",
       "  '72753A': 5595,\n",
       "  '72753D': 5596,\n",
       "  '72754B': 5597,\n",
       "  '84520B': 5598,\n",
       "  '72741': 5599,\n",
       "  '22278': 5600,\n",
       "  '22275': 5601,\n",
       "  '21930': 5602,\n",
       "  '22033': 5603,\n",
       "  '21210': 5604,\n",
       "  '21080': 5605,\n",
       "  '20983': 5606,\n",
       "  '20982': 5607,\n",
       "  '84970S': 5608,\n",
       "  '82578': 5609,\n",
       "  '84029G': 5610,\n",
       "  '82580': 5611,\n",
       "  '82001S': 5612,\n",
       "  '22246': 5613,\n",
       "  '21137': 5614,\n",
       "  '21133': 5615,\n",
       "  '20765': 5616,\n",
       "  '85232D': 5617,\n",
       "  '85232B': 5618,\n",
       "  '84990': 5619,\n",
       "  '84509A': 5620,\n",
       "  '84380': 5621,\n",
       "  '84378': 5622,\n",
       "  '79030D': 5623,\n",
       "  '79030A': 5624,\n",
       "  '37448': 5625,\n",
       "  '84533B': 5626,\n",
       "  '20901': 5627,\n",
       "  '21911': 5628,\n",
       "  '21844': 5629,\n",
       "  '21843': 5630,\n",
       "  '21531': 5631,\n",
       "  '21529': 5632,\n",
       "  '21218': 5633,\n",
       "  '21174': 5634,\n",
       "  '20914': 5635,\n",
       "  '84050': 5636,\n",
       "  '71477': 5637,\n",
       "  '22067': 5638,\n",
       "  '21556': 5639,\n",
       "  '21341': 5640,\n",
       "  '21314': 5641,\n",
       "  '21041': 5642,\n",
       "  '84532B': 5643,\n",
       "  '79072': 5644,\n",
       "  '21256': 5645,\n",
       "  '21181': 5646,\n",
       "  '85063': 5647,\n",
       "  '85049H': 5648,\n",
       "  '85049C': 5649,\n",
       "  '21592': 5650,\n",
       "  '21537': 5651,\n",
       "  '21524': 5652,\n",
       "  '21116': 5653,\n",
       "  '21621': 5654,\n",
       "  '21624': 5655,\n",
       "  '21626': 5656,\n",
       "  '21164': 5657,\n",
       "  '20778': 5658,\n",
       "  '20766': 5659,\n",
       "  '20758': 5660,\n",
       "  '85231L': 5661,\n",
       "  '85150': 5662,\n",
       "  '84884A': 5663,\n",
       "  '84691': 5664,\n",
       "  '72484': 5665,\n",
       "  '22219': 5666,\n",
       "  '21774': 5667,\n",
       "  '82483': 5668,\n",
       "  '21332': 5669,\n",
       "  '20685': 5670,\n",
       "  '21009': 5671,\n",
       "  '90179B': 5672,\n",
       "  '90029': 5673,\n",
       "  '90024E': 5674,\n",
       "  '90024B': 5675,\n",
       "  '90001C': 5676,\n",
       "  '90000B': 5677,\n",
       "  '85099F': 5678,\n",
       "  '85031C': 5679,\n",
       "  '22093': 5680,\n",
       "  '21929': 5681,\n",
       "  '21584': 5682,\n",
       "  '21587': 5683,\n",
       "  '21589': 5684,\n",
       "  '84978': 5685,\n",
       "  '84754': 5686,\n",
       "  '84029C': 5687,\n",
       "  '77101A': 5688,\n",
       "  '71459': 5689,\n",
       "  '35071': 5690,\n",
       "  '22121': 5691,\n",
       "  '21916': 5692,\n",
       "  '21742': 5693,\n",
       "  '48194': 5694,\n",
       "  '21558': 5695,\n",
       "  '21559': 5696,\n",
       "  '22347': 5697,\n",
       "  '22241': 5698,\n",
       "  '22207': 5699,\n",
       "  '22198': 5700,\n",
       "  '22148': 5701,\n",
       "  '22146': 5702,\n",
       "  '21889': 5703,\n",
       "  '21822': 5704,\n",
       "  '21821': 5705,\n",
       "  '46000M': 5706,\n",
       "  '79320': 5707,\n",
       "  '22215': 5708,\n",
       "  '22196': 5709,\n",
       "  '22195': 5710,\n",
       "  '21126': 5711,\n",
       "  '21125': 5712,\n",
       "  '20714': 5713,\n",
       "  '21745': 5714,\n",
       "  '84562B': 5715,\n",
       "  '79323W': 5716,\n",
       "  '84997C': 5717,\n",
       "  '21658': 5718,\n",
       "  '84924D': 5719,\n",
       "  '84924A': 5720,\n",
       "  '84596L': 5721,\n",
       "  '84405A': 5722,\n",
       "  '84270': 5723,\n",
       "  '82600': 5724,\n",
       "  '82581': 5725,\n",
       "  '72814': 5726,\n",
       "  '21917': 5727,\n",
       "  '21851': 5728,\n",
       "  '21743': 5729,\n",
       "  '21732': 5730,\n",
       "  '72598': 5731,\n",
       "  '85104': 5732,\n",
       "  '21355': 5733,\n",
       "  '85114B': 5734,\n",
       "  '21192': 5735,\n",
       "  '20753': 5736,\n",
       "  '20751': 5737,\n",
       "  '20686': 5738,\n",
       "  '20682': 5739,\n",
       "  '20681': 5740,\n",
       "  '20913': 5741,\n",
       "  '22217': 5742,\n",
       "  '22216': 5743,\n",
       "  '22212': 5744,\n",
       "  '22088': 5745,\n",
       "  '22087': 5746,\n",
       "  '22057': 5747,\n",
       "  '21891': 5748,\n",
       "  '22224': 5749,\n",
       "  '22227': 5750,\n",
       "  '22248': 5751,\n",
       "  '35979': 5752,\n",
       "  '35962': 5753,\n",
       "  '35603C': 5754,\n",
       "  '35004P': 5755,\n",
       "  '21756': 5756,\n",
       "  '22340': 5757,\n",
       "  '22336': 5758,\n",
       "  '22335': 5759,\n",
       "  '22265': 5760,\n",
       "  '22253': 5761,\n",
       "  '22252': 5762,\n",
       "  '22251': 5763,\n",
       "  '22249': 5764,\n",
       "  '22337': 5765,\n",
       "  '47578A': 5766,\n",
       "  '20933': 5767,\n",
       "  '20932': 5768,\n",
       "  '20931': 5769,\n",
       "  '20711': 5770,\n",
       "  '20693': 5771,\n",
       "  '20692': 5772,\n",
       "  '84880': 5773,\n",
       "  '20804': 5774,\n",
       "  '22115': 5775,\n",
       "  '21913': 5776,\n",
       "  '21010': 5777,\n",
       "  '21665': 5778,\n",
       "  '21523': 5779,\n",
       "  '21506': 5780,\n",
       "  '21494': 5781,\n",
       "  '21444': 5782,\n",
       "  '21389': 5783,\n",
       "  '21384': 5784,\n",
       "  '21333': 5785,\n",
       "  '21313': 5786,\n",
       "  '21193': 5787,\n",
       "  '21171': 5788,\n",
       "  '21388': 5789,\n",
       "  '48184': 5790,\n",
       "  '48185': 5791,\n",
       "  '35001G': 5792,\n",
       "  '22415': 5793,\n",
       "  '22042': 5794,\n",
       "  '22034': 5795,\n",
       "  '21672': 5796,\n",
       "  '21076': 5797,\n",
       "  '20772': 5798,\n",
       "  '20769': 5799,\n",
       "  '85232A': 5800,\n",
       "  '84551': 5801,\n",
       "  '84549': 5802,\n",
       "  '79321': 5803,\n",
       "  '79190B': 5804,\n",
       "  '22066': 5805,\n",
       "  '21700': 5806,\n",
       "  '21264': 5807,\n",
       "  '85231G': 5808,\n",
       "  '20780': 5809,\n",
       "  '85031B': 5810,\n",
       "  '85014D': 5811,\n",
       "  '84975': 5812,\n",
       "  '84968A': 5813,\n",
       "  '21069': 5814,\n",
       "  '85231E': 5815,\n",
       "  '85199S': 5816,\n",
       "  '84832': 5817,\n",
       "  '84755': 5818,\n",
       "  '84705F': 5819,\n",
       "  '84705D': 5820,\n",
       "  '84705A': 5821,\n",
       "  '84347': 5822,\n",
       "  '84288B': 5823,\n",
       "  '84249D': 5824,\n",
       "  '84247J': 5825,\n",
       "  '82583': 5826,\n",
       "  '82552': 5827,\n",
       "  '79030G': 5828,\n",
       "  '48189': 5829,\n",
       "  '84971S': 5830,\n",
       "  '85199L': 5831,\n",
       "  '85175': 5832,\n",
       "  '85172': 5833,\n",
       "  '85167A': 5834,\n",
       "  '85165': 5835,\n",
       "  '85164B': 5836,\n",
       "  '85163A': 5837,\n",
       "  '85151': 5838,\n",
       "  '85129C': 5839,\n",
       "  '85080': 5840,\n",
       "  '85053': 5841,\n",
       "  '85129A': 5842,\n",
       "  '21770': 5843,\n",
       "  '84507B': 5844,\n",
       "  '46000P': 5845,\n",
       "  '37482P': 5846,\n",
       "  '37482B': 5847,\n",
       "  '35961': 5848,\n",
       "  '21430': 5849,\n",
       "  '21407': 5850,\n",
       "  '22314': 5851,\n",
       "  '22311': 5852,\n",
       "  '22084': 5853,\n",
       "  '84819': 5854,\n",
       "  '85055': 5855,\n",
       "  '21448': 5856,\n",
       "  '21304': 5857,\n",
       "  '21299': 5858,\n",
       "  '21259': 5859,\n",
       "  '21258': 5860,\n",
       "  '21107': 5861,\n",
       "  '48188': 5862,\n",
       "  '48111': 5863,\n",
       "  '21380': 5864,\n",
       "  '22271': 5865,\n",
       "  '22254': 5866,\n",
       "  '22120': 5867,\n",
       "  '22076': 5868,\n",
       "  '21509': 5869,\n",
       "  '21507': 5870,\n",
       "  '21439': 5871,\n",
       "  '21267': 5872,\n",
       "  '84670': 5873,\n",
       "  '20707': 5874,\n",
       "  '21486': 5875,\n",
       "  '21035': 5876,\n",
       "  '47566B': 5877,\n",
       "  '47556B': 5878,\n",
       "  '47480': 5879,\n",
       "  '21908': 5880,\n",
       "  '21734': 5881,\n",
       "  '21519': 5882,\n",
       "  '85045': 5883,\n",
       "  '85044': 5884,\n",
       "  '35954': 5885,\n",
       "  '22071': 5886,\n",
       "  '20972': 5887,\n",
       "  '20971': 5888,\n",
       "  '85087': 5889,\n",
       "  '79191B': 5890,\n",
       "  '22180': 5891,\n",
       "  '22049': 5892,\n",
       "  '21673': 5893,\n",
       "  '21668': 5894,\n",
       "  '21644': 5895,\n",
       "  '21285': 5896,\n",
       "  '20718': 5897,\n",
       "  '21671': 5898,\n",
       "  '21670': 5899,\n",
       "  '21669': 5900,\n",
       "  '84977': 5901,\n",
       "  '84795C': 5902,\n",
       "  '84371': 5903,\n",
       "  '22322': 5904,\n",
       "  '22321': 5905,\n",
       "  '22184': 5906,\n",
       "  '22169': 5907,\n",
       "  '22134': 5908,\n",
       "  '22030': 5909,\n",
       "  '22027': 5910,\n",
       "  '21752': 5911,\n",
       "  '22181': 5912,\n",
       "  '21462': 5913,\n",
       "  '21731': 5914,\n",
       "  '20719': 5915,\n",
       "  '20668': 5916,\n",
       "  '22163': 5917,\n",
       "  '21769': 5918,\n",
       "  '21735': 5919,\n",
       "  '79323S': 5920,\n",
       "  '79323P': 5921,\n",
       "  '22320': 5922,\n",
       "  '22232': 5923,\n",
       "  '22179': 5924,\n",
       "  '21437': 5925,\n",
       "  '16207A': 5926,\n",
       "  '85060': 5927,\n",
       "  '85014A': 5928,\n",
       "  '84949': 5929,\n",
       "  '21252': 5930,\n",
       "  '20725': 5931,\n",
       "  '84951A': 5932,\n",
       "  '84791': 5933,\n",
       "  '47591A': 5934,\n",
       "  '22130': 5935,\n",
       "  '21837': 5936,\n",
       "  '21705': 5937,\n",
       "  '22319': 5938,\n",
       "  '84510B': 5939,\n",
       "  '21902': 5940,\n",
       "  '21901': 5941,\n",
       "  '21900': 5942,\n",
       "  '21868': 5943,\n",
       "  '21777': 5944,\n",
       "  '21729': 5945,\n",
       "  '21580': 5946,\n",
       "  '21422': 5947,\n",
       "  '21421': 5948,\n",
       "  '21322': 5949,\n",
       "  '21260': 5950,\n",
       "  '72351A': 5951,\n",
       "  '71053': 5952,\n",
       "  '22352': 5953,\n",
       "  '22348': 5954,\n",
       "  '21935': 5955,\n",
       "  '90000C': 5956,\n",
       "  '90024D': 5957,\n",
       "  '90065A': 5958,\n",
       "  '90065B': 5959,\n",
       "  '84456': 5960,\n",
       "  '84452': 5961,\n",
       "  '22231': 5962,\n",
       "  '22229': 5963,\n",
       "  '22228': 5964,\n",
       "  '21458': 5965,\n",
       "  '21457': 5966,\n",
       "  '21456': 5967,\n",
       "  '20726': 5968,\n",
       "  '90124B': 5969,\n",
       "  '82613D': 5970,\n",
       "  '82613C': 5971,\n",
       "  '82613B': 5972,\n",
       "  '22077': 5973,\n",
       "  '21903': 5974,\n",
       "  '21870': 5975,\n",
       "  '21473': 5976,\n",
       "  '21390': 5977,\n",
       "  '21257': 5978,\n",
       "  '21243': 5979,\n",
       "  '21242': 5980,\n",
       "  '22131': 5981,\n",
       "  '72801C': 5982,\n",
       "  '72487': 5983,\n",
       "  '22155': 5984,\n",
       "  '37501': 5985,\n",
       "  '21274': 5986,\n",
       "  '21340': 5987,\n",
       "  '21303': 5988,\n",
       "  '20704': 5989,\n",
       "  '84950': 5990,\n",
       "  '22297': 5991,\n",
       "  '21698': 5992,\n",
       "  '21781': 5993,\n",
       "  '22171': 5994,\n",
       "  '21869': 5995,\n",
       "  '21845': 5996,\n",
       "  '21489': 5997,\n",
       "  '85177': 5998,\n",
       "  '21115': 5999,\n",
       "  '85197': 6000,\n",
       "  '85184C': 6001,\n",
       "  '79029': 6002,\n",
       "  '90126A': 6003,\n",
       "  '21588': 6004,\n",
       "  '21359': 6005,\n",
       "  '21350': 6006,\n",
       "  '84813': 6007,\n",
       "  '47570': 6008,\n",
       "  '21674': 6009,\n",
       "  '21625': 6010,\n",
       "  '21622': 6011,\n",
       "  '21555': 6012,\n",
       "  '21438': 6013,\n",
       "  '21836': 6014,\n",
       "  '22143': 6015,\n",
       "  '22085': 6016,\n",
       "  '79342B': 6017,\n",
       "  '79323G': 6018,\n",
       "  '37399': 6019,\n",
       "  '35095B': 6020,\n",
       "  '35095A': 6021,\n",
       "  '21893': 6022,\n",
       "  '21746': 6023,\n",
       "  '21426': 6024,\n",
       "  '21360': 6025,\n",
       "  '16239': 6026,\n",
       "  '15039': 6027,\n",
       "  '15044A': 6028,\n",
       "  '15044B': 6029,\n",
       "  '21226': 6030,\n",
       "  '21165': 6031,\n",
       "  '20989': 6032,\n",
       "  '72807C': 6033,\n",
       "  '20738': 6034,\n",
       "  '18096C': 6035,\n",
       "  '16048': 6036,\n",
       "  '15044D': 6037,\n",
       "  '15044C': 6038,\n",
       "  '20736': 6039,\n",
       "  '21684': 6040,\n",
       "  '21683': 6041,\n",
       "  '21576': 6042,\n",
       "  '84499': 6043,\n",
       "  '21255': 6044,\n",
       "  '20703': 6045,\n",
       "  '84926C': 6046,\n",
       "  '84659A': 6047,\n",
       "  '22081': 6048,\n",
       "  '21411': 6049,\n",
       "  '21343': 6050,\n",
       "  '21275': 6051,\n",
       "  '21198': 6052,\n",
       "  '20906': 6053,\n",
       "  '84988': 6054,\n",
       "  '85142': 6055,\n",
       "  '84767': 6056,\n",
       "  '84279P': 6057,\n",
       "  '82582': 6058,\n",
       "  '22283': 6059,\n",
       "  '21904': 6060,\n",
       "  '85173': 6061,\n",
       "  '20894': 6062,\n",
       "  '20893': 6063,\n",
       "  '48195': 6064,\n",
       "  '85180B': 6065,\n",
       "  '85180A': 6066,\n",
       "  '85179B': 6067,\n",
       "  '85179A': 6068,\n",
       "  '79303D': 6069,\n",
       "  '21541': 6070,\n",
       "  '20659': 6071,\n",
       "  '90184B': 6072,\n",
       "  '90057': 6073,\n",
       "  '90002A': 6074,\n",
       "  '84705C': 6075,\n",
       "  '51014C': 6076,\n",
       "  '72807B': 6077,\n",
       "  '21986': 6078,\n",
       "  '21985': 6079,\n",
       "  '21842': 6080,\n",
       "  '21829': 6081,\n",
       "  '72802A': 6082,\n",
       "  '72800F': 6083,\n",
       "  '72800E': 6084,\n",
       "  '72800A': 6085,\n",
       "  '72799B': 6086,\n",
       "  '72799A': 6087,\n",
       "  '72780': 6088,\n",
       "  '51012L': 6089,\n",
       "  '37508': 6090,\n",
       "  '84031A': 6091,\n",
       "  '84692': 6092,\n",
       "  '90214S': 6093,\n",
       "  '90214L': 6094,\n",
       "  '90214J': 6095,\n",
       "  '90214F': 6096,\n",
       "  '90214E': 6097,\n",
       "  '90214C': 6098,\n",
       "  '90214B': 6099,\n",
       "  '90214A': 6100,\n",
       "  '85161': 6101,\n",
       "  '84989B': 6102,\n",
       "  '84986B': 6103,\n",
       "  '84854': 6104,\n",
       "  '37424': 6105,\n",
       "  '37423': 6106,\n",
       "  '37345': 6107,\n",
       "  '20990': 6108,\n",
       "  '20987': 6109,\n",
       "  '20622': 6110,\n",
       "  '20621': 6111,\n",
       "  '18098C': 6112,\n",
       "  '17039': 6113,\n",
       "  '84789': 6114,\n",
       "  '35537': 6115,\n",
       "  '22282': 6116,\n",
       "  '37333': 6117,\n",
       "  '35443': 6118,\n",
       "  '21949': 6119,\n",
       "  '21947': 6120,\n",
       "  '21936': 6121,\n",
       "  '21867': 6122,\n",
       "  '21846': 6123,\n",
       "  '79192A': 6124,\n",
       "  '72128': 6125,\n",
       "  '21728': 6126,\n",
       "  '21711': 6127,\n",
       "  '21710': 6128,\n",
       "  '21708': 6129,\n",
       "  '21707': 6130,\n",
       "  '21706': 6131,\n",
       "  '21692': 6132,\n",
       "  '21691': 6133,\n",
       "  '21687': 6134,\n",
       "  '21667': 6135,\n",
       "  '21544': 6136,\n",
       "  '21543': 6137,\n",
       "  '21324': 6138,\n",
       "  '21323': 6139,\n",
       "  '51008': 6140,\n",
       "  '47590A': 6141,\n",
       "  '47562': 6142,\n",
       "  '47505': 6143,\n",
       "  '22203': 6144,\n",
       "  '22202': 6145,\n",
       "  '22200': 6146,\n",
       "  '22098': 6147,\n",
       "  '22199': 6148,\n",
       "  '79000': 6149,\n",
       "  '21055': 6150,\n",
       "  '48116': 6151,\n",
       "  '35832': 6152,\n",
       "  '22208': 6153,\n",
       "  '85071C': 6154,\n",
       "  '85071D': 6155,\n",
       "  '21054': 6156,\n",
       "  '20679': 6157,\n",
       "  '15056N': 6158,\n",
       "  '10133': 6159,\n",
       "  '21326': 6160,\n",
       "  '90167': 6161,\n",
       "  '84849B': 6162,\n",
       "  '20934': 6163,\n",
       "  '20895': 6164,\n",
       "  '20618': 6165,\n",
       "  '20988': 6166,\n",
       "  ...}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1300b220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer # Ensure this is installed\n",
    "from sklearn.decomposition import PCA\n",
    "def get_orders_df(df, aggregated = False): \n",
    "    rfm_data, orders_df= generate_rfm_data(df)\n",
    "    if not aggregated:\n",
    "        orders_df1 = df.groupby('InvoiceNo').agg({\n",
    "                                                'InvoiceDate': 'first',\n",
    "                                                'Month': 'first',\n",
    "                                                'hour_minute': 'first',\n",
    "                                                'day_of_week': 'first',\n",
    "                                                'day_sin': 'first',\n",
    "                                                'day_cos': 'first',\n",
    "                                                'StockCode': 'nunique',\n",
    "                                                }).reset_index()\n",
    "        orders_df = orders_df1.merge(orders_df, on='InvoiceNo', how='left')\n",
    "        orders_df.rename(columns = {'StockCode':'unique_per_order'}, inplace = True)\n",
    "        morning_time = pd.to_datetime('12:00:00').time()\n",
    "        afternoon_time = pd.to_datetime('18:00:00').time()\n",
    "        orders_df['time_of_day'] = orders_df['hour_minute'].apply(lambda x: 'morning' if x < morning_time\n",
    "                                                                        else 'afternoon' if (x < afternoon_time and x >= morning_time)\n",
    "                                                                        else 'evening')\n",
    "        comon_type_od_day = orders_df.groupby('CustomerID')['time_of_day'].agg(lambda x:x.value_counts().idxmax()).reset_index(name = 'most_common_time_of_day')\n",
    "        \n",
    "        \n",
    "\n",
    "        customer_df = orders_df.groupby('CustomerID')['unique_per_order'].mean().reset_index()\n",
    "        \n",
    "        customer_df = customer_df.merge(comon_type_od_day, on='CustomerID', how='left')\n",
    "        \n",
    "        customer_df = pd.get_dummies(customer_df, columns=['most_common_time_of_day'], prefix='in_').astype(int)\n",
    "    else:\n",
    "        df['FirstOrder'] = df.groupby('CustomerID')['InvoiceDate'].transform('min')\n",
    "        customer_df = df.groupby('CustomerID')['InvoiceNo'].sum().reset_index()\n",
    "    \n",
    "    n_unique_items = df.groupby('CustomerID')['StockCode'].nunique().reset_index(name = 'n_unique_items')\n",
    "    customer_df = customer_df.merge(n_unique_items, on='CustomerID', how='left')\n",
    "   \n",
    "    customer_df = customer_df.merge(rfm_data, on='CustomerID', how='left')\n",
    "    customer_df['avg_time_between_orders'] = customer_df['recency']/customer_df['frequency']\n",
    "    customer_df['days_since_last_order'] = customer_df['T'] - customer_df['recency']\n",
    "    customer_df['TotalPrice '] = customer_df['frequency'] * customer_df['monetary_value']\n",
    "    customer_df['first_order'] = customer_df['CustomerID'].map(df.groupby('CustomerID')['FirstOrder'].min())\n",
    "    customer_df['first_order'] = pd.to_datetime(customer_df['first_order'])\n",
    "    customer_df['first_order_month'] = customer_df['first_order'].dt.month\n",
    "    customer_df = pd.get_dummies(customer_df, columns = ['first_order_month'], prefix = 'month', drop_first=True)\n",
    "    customer_df[[ 'month_2', 'month_3','month_4', 'month_5', 'month_6', 'month_7', 'month_8', 'month_9',\n",
    "       'month_10', 'month_11', 'month_12']] = customer_df[[ 'month_2', 'month_3','month_4', 'month_5', 'month_6', 'month_7', 'month_8', 'month_9',\n",
    "       'month_10', 'month_11', 'month_12']].astype(int)\n",
    "    customer_df = customer_df.drop(columns=['first_order'])\n",
    "    # normalize all the features in the customer_df\n",
    "    scaler = StandardScaler()\n",
    "    if not aggregated:\n",
    "        customer_df[['unique_per_order', 'n_unique_items', 'frequency', 'recency', 'T',\n",
    "        'monetary_value', 'avg_time_between_orders', 'days_since_last_order',\n",
    "        'TotalPrice ']] = scaler.fit_transform(customer_df[['unique_per_order', 'n_unique_items', 'frequency', 'recency', 'T',\n",
    "        'monetary_value', 'avg_time_between_orders', 'days_since_last_order',\n",
    "        'TotalPrice ']])\n",
    "    else:\n",
    "        customer_df[[ 'n_unique_items', 'frequency', 'recency', 'T',\n",
    "        'monetary_value', 'avg_time_between_orders', 'days_since_last_order',\n",
    "        'TotalPrice ']] = scaler.fit_transform(customer_df[[ 'n_unique_items', 'frequency', 'recency', 'T',\n",
    "        'monetary_value', 'avg_time_between_orders', 'days_since_last_order',\n",
    "        'TotalPrice ']])\n",
    "\n",
    "    \n",
    "    \n",
    "    return customer_df\n",
    "\n",
    "def build_node_attributes(\n",
    "    df_all_transactions: pd.DataFrame,\n",
    "    customer_id_col: str,\n",
    "    product_id_col: str,\n",
    "    country_col: str,         # For customers - will be treated as text for SBERT\n",
    "    description_col: str,     # For products - will be treated as text for SBERT\n",
    "    \n",
    "    # Mappings and ID info from your main data prep script\n",
    "    customer_original_to_0_idx_map: dict,\n",
    "    product_original_to_0_idx_map: dict,\n",
    "    max_0_idx_customer: int,\n",
    "    max_1_based_global_node_id: int,\n",
    "    is_bipartite: bool = True,\n",
    "    is_aggregated: bool = False, # If True, use aggregated customer features\n",
    "    \n",
    "    target_feature_dim: int = 64,\n",
    "    sbert_model_name: str = 'all-MiniLM-L6-v2' # Model to use for both country and desc\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Builds static node features using SBERT for both Country (customers)\n",
    "    and Description (products). All features are adjusted to target_feature_dim.\n",
    "    Output array is 1-indexed (row 0 is dummy zeros).\n",
    "    \"\"\"\n",
    "    print(f\"Building static node features with SBERT, target dimension: {target_feature_dim}. \"\n",
    "          f\"Max 1-based global node ID: {max_1_based_global_node_id}\")\n",
    "\n",
    "    # --- 1. Initialize Sentence Transformer Model ---\n",
    "    print(f\"Loading sentence transformer model: {sbert_model_name}...\")\n",
    "    sbert_model = SentenceTransformer(sbert_model_name)\n",
    "    sbert_native_dim = sbert_model.get_sentence_embedding_dimension()\n",
    "    print(f\"Native SBERT embedding dimension: {sbert_native_dim}\")\n",
    "\n",
    "    # --- 2. Initialize Global Node Features Array ---\n",
    "    global_node_features_np = np.zeros((max_1_based_global_node_id + 1, sbert_native_dim), dtype=np.float32)\n",
    "    print(f\"Initialized global node features matrix with shape: {global_node_features_np.shape}\")\n",
    "\n",
    "    # --- 3. Populate Customer Features (SBERT on Country name) ---\n",
    "    print(\"Populating customer features (SBERT on Country name)...\")\n",
    "    unique_customer_df = df_all_transactions.drop_duplicates(subset=[customer_id_col]).copy()\n",
    "    unique_customer_df[country_col] = unique_customer_df[country_col].astype(str).fillna('Unknown') # Ensure string\n",
    "\n",
    "    for original_cust_id, internal_0_idx_cust in customer_original_to_0_idx_map.items():\n",
    "        global_1_idx_cust = internal_0_idx_cust+1\n",
    "\n",
    "        if 0 < global_1_idx_cust <= max_1_based_global_node_id:\n",
    "            customer_data_row = unique_customer_df[unique_customer_df[customer_id_col] == original_cust_id]\n",
    "            if not customer_data_row.empty:\n",
    "                country_text = customer_data_row[country_col].iloc[0]\n",
    "                if country_text:\n",
    "                    try:\n",
    "                        country_sbert_embedding = sbert_model.encode(country_text).astype(np.float32)\n",
    "                        global_node_features_np[global_1_idx_cust+1, :] = country_sbert_embedding\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Could not encode country for customer {original_cust_id}: '{country_text}'. Error: {e}\")\n",
    "    \n",
    "    # --- 4. Populate Product Features (SBERT on Description) ---\n",
    "    print(\"Populating product features (SBERT on Description)...\")\n",
    "    unique_product_df = df_all_transactions.drop_duplicates(subset=[product_id_col]).copy()\n",
    "    unique_product_df[description_col] = unique_product_df[description_col].astype(str).fillna('') # Ensure string\n",
    "\n",
    "    for original_prod_id, internal_0_idx_prod in product_original_to_0_idx_map.items():\n",
    "        if is_bipartite:\n",
    "            global_1_idx_prod = internal_0_idx_prod + (max_0_idx_customer + 1) + 1\n",
    "        else:\n",
    "            global_1_idx_prod = internal_0_idx_prod + 1 # Adjust if your non-bipartite mapping is different\n",
    "\n",
    "        if 0 < global_1_idx_prod <= max_1_based_global_node_id:\n",
    "            product_data_row = unique_product_df[unique_product_df[product_id_col] == original_prod_id]\n",
    "            if not product_data_row.empty:\n",
    "                description_text = product_data_row[description_col].iloc[0]\n",
    "                if description_text:\n",
    "                    try:\n",
    "                        desc_sbert_embedding = sbert_model.encode(description_text).astype(np.float32)        \n",
    "                        global_node_features_np[global_1_idx_prod+1, :] = desc_sbert_embedding\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Could not encode description for product {original_prod_id}: '{description_text[:50]}...'. Error: {e}\")\n",
    "    \n",
    "    print(\"Apply PCA dimensionality reduction to target feature dimension...\")\n",
    "    pca = PCA(n_components=target_feature_dim)\n",
    "    final_node_features_np = pca.fit_transform(global_node_features_np) \n",
    "    print(\"Adding additional customer features to node feature matrix...\")\n",
    "    #load the other customer features \n",
    "    cust_features_df = get_orders_df(df_all_transactions, aggregated=is_aggregated)\n",
    "    #map customer IDs to their internal 0-based indices\n",
    "    cust_features_df['internal_0_idx'] = cust_features_df['CustomerID'].map(customer_original_to_0_idx_map)\n",
    "    #add the customer features to the node feature matrix\n",
    "    for _, row in cust_features_df.iterrows():\n",
    "        internal_0_idx_cust = int(row['internal_0_idx']) if pd.notna(row['internal_0_idx']) else None\n",
    "        if internal_0_idx_cust is not None:\n",
    "            global_1_idx_cust = internal_0_idx_cust + 1\n",
    "            if 0 < global_1_idx_cust <= max_1_based_global_node_id:\n",
    "                # Ensure we don't overwrite SBERT features\n",
    "                existing_features = final_node_features_np[global_1_idx_cust, :]\n",
    "                #additional_features = row.drop(['CustomerID', 'internal_0_idx']).values.astype(np.float32)\n",
    "                additional_features = row.drop(['CustomerID', 'internal_0_idx']).values\n",
    "                final_node_features_np[global_1_idx_cust, :len(additional_features)] = additional_features\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "    print(\"Static node feature matrix (SBERT unified) built successfully.\")\n",
    "    return final_node_features_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "acade820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building static node features with SBERT, target dimension: 100. Max 1-based global node ID: 9547\n",
      "Loading sentence transformer model: all-MiniLM-L6-v2...\n",
      "Native SBERT embedding dimension: 384\n",
      "Initialized global node features matrix with shape: (9548, 384)\n",
      "Populating customer features (SBERT on Country name)...\n",
      "Populating product features (SBERT on Description)...\n",
      "Warning: Could not encode description for product 90176E: 'DIAMANTE NECKLACE GREEN...'. Error: index 9548 is out of bounds for axis 0 with size 9548\n",
      "Apply PCA dimensionality reduction to target feature dimension...\n",
      "Adding additional customer features to node feature matrix...\n",
      "Static node feature matrix (SBERT unified) built successfully.\n"
     ]
    }
   ],
   "source": [
    "node_np = build_node_attributes(\n",
    "    df_all_transactions=train_data,\n",
    "    customer_id_col='CustomerID',\n",
    "    product_id_col='StockCode',\n",
    "    country_col='Country',  # Use the encoded country ID\n",
    "    description_col='Description',  # Use the encoded description ID\n",
    "    customer_original_to_0_idx_map=cust_map,\n",
    "    product_original_to_0_idx_map=prod_map,\n",
    "    max_0_idx_customer=train_data['CustomerID'].nunique() - 1,  # Max index for customers\n",
    "    max_1_based_global_node_id =9547, # From your input data\n",
    "    is_bipartite=True,  # Set to False if not bipartite\n",
    "    is_aggregated=True,  # Set to True if using aggregated customer features\n",
    "    target_feature_dim=100,  # Adjust as needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "52e6b570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the node features to a numpy file\n",
    "np.save('data/ml_online_retail_node.npy', node_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "70f2bf45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9548, 128)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "07eb93c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing command: python ./train_self_supervised.py --data online_retail --n_epoch 20 --bs 300 --node_dim 100 --memory_dim 128 --time_dim 100 --message_dim 100 --use_memory --lr 0.0001 --n_degree 10 --embedding_module time --message_function identity --n_head 2 --patience 10 --drop_out 0.05 --aggregator last --memory_updater gru\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script STDOUT:\n",
      "The dataset has 554594 interactions, involving 9547 different nodes\n",
      "The training dataset has 309859 interactions, involving 7656 different nodes\n",
      "The validation dataset has 71408 interactions, involving 4717 different nodes\n",
      "The test dataset has 73585 interactions, involving 4718 different nodes\n",
      "The new node validation dataset has 32646 interactions, involving 4230 different nodes\n",
      "The new node test dataset has 40825 interactions, involving 4358 different nodes\n",
      "954 nodes were used for the inductive testing, i.e. are never seen during training\n",
      "\n",
      "Script STDERR:\n",
      "INFO:root:Namespace(data='online_retail', bs=300, prefix='', n_degree=10, n_head=2, n_epoch=20, n_layer=1, lr=0.0001, patience=10, n_runs=1, drop_out=0.05, gpu=0, node_dim=100, time_dim=100, backprop_every=1, use_memory=True, embedding_module='time', message_function='identity', memory_updater='gru', aggregator='last', memory_update_at_end=False, message_dim=100, memory_dim=128, different_new_nodes=False, uniform=False, randomize_features=False, use_destination_embedding_in_message=False, use_source_embedding_in_message=False, dyrep=False)\n",
      "INFO:root:num of training instances: 309859\n",
      "INFO:root:num of batches per epoch: 1033\n",
      "INFO:root:start 0 epoch\n",
      "INFO:root:epoch: 0 took 122.58s\n",
      "INFO:root:Epoch mean loss: 0.9823553593322591\n",
      "INFO:root:val auc: 0.8748170041899441, new node val auc: 0.8787274223108488\n",
      "INFO:root:val ap: 0.8486695857854684, new node val ap: 0.8568237216456072\n",
      "INFO:root:start 1 epoch\n",
      "INFO:root:epoch: 1 took 132.77s\n",
      "INFO:root:Epoch mean loss: 0.9623958930618419\n",
      "INFO:root:val auc: 0.8765561103351954, new node val auc: 0.8789295417608005\n",
      "INFO:root:val ap: 0.8508917591019003, new node val ap: 0.8570006375429874\n",
      "INFO:root:start 2 epoch\n",
      "INFO:root:epoch: 2 took 129.85s\n",
      "INFO:root:Epoch mean loss: 0.9529242023955472\n",
      "INFO:root:val auc: 0.8785508729050279, new node val auc: 0.8812461777859744\n",
      "INFO:root:val ap: 0.8533383034602919, new node val ap: 0.8605953672657479\n",
      "INFO:root:start 3 epoch\n",
      "INFO:root:epoch: 3 took 130.42s\n",
      "INFO:root:Epoch mean loss: 0.9501423400115413\n",
      "INFO:root:val auc: 0.879598533519553, new node val auc: 0.8850570679549541\n",
      "INFO:root:val ap: 0.8546573518738284, new node val ap: 0.865894157147074\n",
      "INFO:root:start 4 epoch\n",
      "INFO:root:epoch: 4 took 132.09s\n",
      "INFO:root:Epoch mean loss: 0.9447190846962943\n",
      "INFO:root:val auc: 0.8799963337988828, new node val auc: 0.8852475285571028\n",
      "INFO:root:val ap: 0.8552740435652214, new node val ap: 0.8657646913843206\n",
      "INFO:root:start 5 epoch\n",
      "INFO:root:epoch: 5 took 129.21s\n",
      "INFO:root:Epoch mean loss: 0.9419200809281193\n",
      "INFO:root:val auc: 0.8799958798882683, new node val auc: 0.8850350019018858\n",
      "INFO:root:val ap: 0.854950987817251, new node val ap: 0.8638098667701029\n",
      "INFO:root:start 6 epoch\n",
      "INFO:root:epoch: 6 took 129.85s\n",
      "INFO:root:Epoch mean loss: 0.9370641811080962\n",
      "INFO:root:val auc: 0.8799729399441341, new node val auc: 0.8852939838454978\n",
      "INFO:root:val ap: 0.8550241874050202, new node val ap: 0.8649471812533713\n",
      "INFO:root:start 7 epoch\n",
      "INFO:root:epoch: 7 took 130.35s\n",
      "INFO:root:Epoch mean loss: 0.9352707046968547\n",
      "INFO:root:val auc: 0.8804750698324023, new node val auc: 0.8861110717126193\n",
      "INFO:root:val ap: 0.8555857624892095, new node val ap: 0.8667088249072146\n",
      "INFO:root:start 8 epoch\n",
      "INFO:root:epoch: 8 took 129.45s\n",
      "INFO:root:Epoch mean loss: 0.9342451740672951\n",
      "INFO:root:val auc: 0.8803855097765364, new node val auc: 0.8863649295149616\n",
      "INFO:root:val ap: 0.8553196719051586, new node val ap: 0.8670471800165218\n",
      "INFO:root:start 9 epoch\n",
      "INFO:root:epoch: 9 took 134.75s\n",
      "INFO:root:Epoch mean loss: 0.9311596216329185\n",
      "INFO:root:val auc: 0.880631529329609, new node val auc: 0.8866669727454011\n",
      "INFO:root:val ap: 0.8554908966553852, new node val ap: 0.8668181954533307\n",
      "INFO:root:start 10 epoch\n",
      "INFO:root:epoch: 10 took 130.01s\n",
      "INFO:root:Epoch mean loss: 0.9311054822666948\n",
      "INFO:root:val auc: 0.88098156424581, new node val auc: 0.8877722373668681\n",
      "INFO:root:val ap: 0.8563135699578965, new node val ap: 0.869829728106114\n",
      "INFO:root:start 11 epoch\n",
      "INFO:root:epoch: 11 took 130.44s\n",
      "INFO:root:Epoch mean loss: 0.9283646810781806\n",
      "INFO:root:val auc: 0.8808054120111731, new node val auc: 0.8870078934021854\n",
      "INFO:root:val ap: 0.8556326963905728, new node val ap: 0.8679859560221481\n",
      "INFO:root:start 12 epoch\n",
      "INFO:root:epoch: 12 took 129.56s\n",
      "INFO:root:Epoch mean loss: 0.9249710476410008\n",
      "INFO:root:val auc: 0.8813686103351955, new node val auc: 0.8899617071153579\n",
      "INFO:root:val ap: 0.8566107350775297, new node val ap: 0.8725268092269354\n",
      "INFO:root:start 13 epoch\n",
      "INFO:root:epoch: 13 took 131.63s\n",
      "INFO:root:Epoch mean loss: 0.9273276391934102\n",
      "INFO:root:val auc: 0.8808203910614526, new node val auc: 0.8885989022373091\n",
      "INFO:root:val ap: 0.8557004333821906, new node val ap: 0.8699308599000732\n",
      "INFO:root:start 14 epoch\n",
      "INFO:root:epoch: 14 took 127.95s\n",
      "INFO:root:Epoch mean loss: 0.9256935642972689\n",
      "INFO:root:val auc: 0.8809915502793297, new node val auc: 0.8902052752835538\n",
      "INFO:root:val ap: 0.8559033742000295, new node val ap: 0.872897649802306\n",
      "INFO:root:start 15 epoch\n",
      "INFO:root:epoch: 15 took 133.42s\n",
      "INFO:root:Epoch mean loss: 0.9239844402869927\n",
      "INFO:root:val auc: 0.8809450069832402, new node val auc: 0.8885377567545761\n",
      "INFO:root:val ap: 0.8557365124505304, new node val ap: 0.8698127174584099\n",
      "INFO:root:start 16 epoch\n",
      "INFO:root:epoch: 16 took 129.29s\n",
      "INFO:root:Epoch mean loss: 0.9244108239886606\n",
      "INFO:root:val auc: 0.8808121508379889, new node val auc: 0.8890701233920422\n",
      "INFO:root:val ap: 0.8559069752411483, new node val ap: 0.8718510006284331\n",
      "INFO:root:start 17 epoch\n",
      "INFO:root:epoch: 17 took 126.98s\n",
      "INFO:root:Epoch mean loss: 0.9252710666107186\n",
      "INFO:root:val auc: 0.8806796438547486, new node val auc: 0.8879901369934068\n",
      "INFO:root:val ap: 0.8556774493048885, new node val ap: 0.8697704631010578\n",
      "INFO:root:start 18 epoch\n",
      "INFO:root:epoch: 18 took 128.28s\n",
      "INFO:root:Epoch mean loss: 0.9218440287226283\n",
      "INFO:root:val auc: 0.880789280726257, new node val auc: 0.8887874693104797\n",
      "INFO:root:val ap: 0.8556870740192312, new node val ap: 0.8710679872058981\n",
      "INFO:root:start 19 epoch\n",
      "INFO:root:epoch: 19 took 126.55s\n",
      "INFO:root:Epoch mean loss: 0.9207479393493748\n",
      "INFO:root:val auc: 0.8807086592178771, new node val auc: 0.8872263532205265\n",
      "INFO:root:val ap: 0.8555794721360385, new node val ap: 0.8696128453353004\n",
      "INFO:root:Test statistics: Old nodes -- auc: 0.8744041494803412, ap: 0.8491579391089716\n",
      "INFO:root:Test statistics: New nodes -- auc: 0.8108169512195121, ap: 0.8015350054908631\n",
      "INFO:root:Saving TGN model\n",
      "INFO:root:TGN model saved\n",
      "\n",
      "Script './train_self_supervised.py' executed successfully.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "# For safely splitting command strings if needed\n",
    "\n",
    "# 1. Define the path to your Python interpreter (optional, often just 'python' works)\n",
    "python_executable = \"python\" # Or specify the full path if needed, e.g., sys.executable\n",
    "\n",
    "# 2. Define the path to your script\n",
    "script_path = \"./train_self_supervised.py\" # Assuming it's in the current directory\n",
    "\n",
    "# 3. Define the arguments for the script\n",
    "# These are the same arguments you would use on the command line.\n",
    "# Replace 'your_dataset_name' and other values with your actual settings.\n",
    "dataset_name = \"online_retail\" # This should match the prefix of your ml_... files\n",
    "use_memory_flag = \"--use_memory\" # Add this flag if you want to use memory\n",
    "num_epochs = \"20\" # Example: train for 10 epochs\n",
    "batch_size = \"300\" # Example batch size, matching script's default\n",
    "node_dim = \"100\" # Example node feature dimension, adjust as needed\n",
    "memory_dim = \"128\" # Example memory dimension, adjust as needed\n",
    "time_dim = \"100\" # Example time feature dimension, adjust as needed\n",
    "message_dim = \"100\" # Example message dimension, adjust as needed\n",
    "learning_rate = \"0.0001\" # Example learning rate, adjust as needed\n",
    "patience = \"10\" # Example patience for early stopping, adjust as needed\n",
    "drop_out = \"0.05\" # Example dropout rate, adjust as needed\n",
    "aggregator = \"last\" # Example aggregation method, adjust as needed\n",
    "m_updater = \"gru\"\n",
    "\n",
    "\n",
    "# Add other arguments as needed, e.g.:\n",
    "n_heads = \"2\" # Example number of attention heads, adjust as needed\n",
    "num_neighbors = \"10\"\n",
    "embedding_module = \"time\" # \n",
    "message_function = \"identity\" # Example message function, adjust as needed\n",
    "\n",
    "# Construct the command as a list of strings\n",
    "command = [\n",
    "    python_executable,\n",
    "    script_path,\n",
    "    \"--data\", dataset_name,\n",
    "    \"--n_epoch\", num_epochs,\n",
    "    \"--bs\", batch_size,\n",
    "    \"--node_dim\", node_dim,\n",
    "    \"--memory_dim\", memory_dim,\n",
    "    \"--time_dim\", time_dim,\n",
    "    \"--message_dim\", message_dim,\n",
    "    # Add flags like --use_memory directly if they are action='store_true'\n",
    "    # For example, if args.use_memory is True, the flag should be present.\n",
    "    # The train_self_supervised.py script uses 'store_true', so just including it enables it.\n",
    "    \"--use_memory\", # Include if you want to activate this option\n",
    "     \"--lr\", learning_rate, # Uncomment and set if you want to override defaults\n",
    "    \"--n_degree\", num_neighbors,\n",
    "    \"--embedding_module\", embedding_module,\n",
    "    \"--message_function\", message_function,\n",
    "    \"--n_head\", n_heads,\n",
    "    \"--patience\", patience,\n",
    "    \"--drop_out\", drop_out,\n",
    "    \"--aggregator\", aggregator,\n",
    "    \"--memory_updater\", m_updater,\n",
    "    # ... add any other arguments defined in train_self_supervised.py's parser\n",
    "]\n",
    "\n",
    "# If you want to use a flag that is 'store_false' by default and you want to set it to false,\n",
    "# you usually don't include it. If it's 'store_true' (like --use_memory), including it sets it to True.\n",
    "# For args.memory_update_at_end (which makes memory_update_at_start=False in TGN constructor):\n",
    "# if you want memory_update_at_end = True:\n",
    "#   command.append(\"--memory_update_at_end\")\n",
    "\n",
    "\n",
    "print(f\"Executing command: {' '.join(command)}\")\n",
    "\n",
    "# 4. Run the command\n",
    "try:\n",
    "    # Using subprocess.run for more control\n",
    "    result = subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "    print(\"Script STDOUT:\")\n",
    "    print(result.stdout)\n",
    "    if result.stderr:\n",
    "        print(\"Script STDERR:\")\n",
    "        print(result.stderr)\n",
    "    print(f\"Script '{script_path}' executed successfully.\")\n",
    "\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error executing script '{script_path}':\")\n",
    "    print(\"Return code:\", e.returncode)\n",
    "    print(\"STDOUT:\", e.stdout)\n",
    "    print(\"STDERR:\", e.stderr)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The script '{script_path}' or Python executable '{python_executable}' was not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "53b31ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2009-12-01 07:45:00')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed['InvoiceDate'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dbf0397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "Using device: cpu\n",
      "The dataset has 554594 interactions, involving 9547 different nodes\n",
      "The training dataset has 309859 interactions, involving 7656 different nodes\n",
      "The validation dataset has 71408 interactions, involving 4717 different nodes\n",
      "The test dataset has 73585 interactions, involving 4718 different nodes\n",
      "The new node validation dataset has 32646 interactions, involving 4230 different nodes\n",
      "The new node test dataset has 40825 interactions, involving 4358 different nodes\n",
      "954 nodes were used for the inductive testing, i.e. are never seen during training\n",
      "Data loaded for 'online_retail'.\n",
      "  Node features shape: (9548, 100)\n",
      "  Edge features shape: (554595, 15)\n",
      "  Training interactions: 309859\n",
      "Neighbor finder initialized.\n",
      "Time statistics computed.\n",
      "Initializing TGN model...\n",
      "TGN model instance created successfully!\n",
      "  TGN n_node_features (from loaded file): 100\n",
      "  TGN time_encoder dimension: 100\n",
      "  TGN memory dimension: 100\n",
      "  TGN message dimension: 315\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load model from saved_models folder\n",
    "import torch\n",
    "import argparse # To simulate args if needed, or just define them\n",
    "import sys\n",
    "from utils.data_processing import get_data, compute_time_statistics\n",
    "from utils.utils import get_neighbor_finder # Make sure NeighborFinder class is also importable\n",
    "from model.tgn import TGN\n",
    "\n",
    "\n",
    "model_path = 'saved_models/best_continuous_monthly.pth'\n",
    "model_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "# Check if the model is loaded correctly\n",
    "if model_dict is not None:\n",
    "    print(\"Model loaded successfully.\")\n",
    "else:\n",
    "    print(\"Failed to load the model.\")\n",
    "\n",
    "\n",
    "# --- 1. Define Arguments (Simulating args from your training command) ---\n",
    "# These should match what you used or intend to use for training/inference\n",
    "class ArgsSimulator:\n",
    "    def __init__(self):\n",
    "        self.data = \"online_retail\"\n",
    "        self.bs = 300 # Not directly used for model instantiation, but for context\n",
    "        self.prefix = ''\n",
    "        self.n_degree = 10  # NUM_NEIGHBORS for TGN\n",
    "        self.n_head = 2\n",
    "        self.n_epoch = 10 # Not for instantiation\n",
    "        self.n_layer = 1\n",
    "        self.lr = 0.0001 # Not for instantiation\n",
    "        self.patience = 7 # Not for instantiation\n",
    "        self.n_runs = 1 # Not for instantiation\n",
    "        self.drop_out = 0.05\n",
    "        self.gpu = 0 # Or your desired GPU index\n",
    "        \n",
    "        # These are from your command, but TGN derives node/time_enc dim from node_features file\n",
    "        self.node_dim_arg = 100 # What you passed to script\n",
    "        self.time_dim_arg = 100 # What you passed to script\n",
    "        \n",
    "        self.backprop_every = 1 # Not for instantiation\n",
    "        self.use_memory = True # From your command\n",
    "        self.embedding_module = \"time\" # Default in script\n",
    "        self.message_function = \"identity\"       # Default in script\n",
    "        self.memory_updater = \"gru\"              # Default in script\n",
    "        self.aggregator = \"last\"                 # Default in script\n",
    "        self.memory_update_at_end = False        # Default in script (so memory_update_at_start is True)\n",
    "        self.message_dim = 100                    # From your command\n",
    "        self.memory_dim = 100                    # From your command\n",
    "        self.node_dim = 100\n",
    "        self.time_dim = 100                       # From your command\n",
    "        self.different_new_nodes = False         # Default\n",
    "        self.uniform = False                     # Default (for neighbor sampling strategy)\n",
    "        self.randomize_features = False          # Default\n",
    "        self.use_destination_embedding_in_message = False # Default\n",
    "        self.use_source_embedding_in_message = False    # Default\n",
    "        self.dyrep = False                       # Default\n",
    "\n",
    "args = ArgsSimulator() # Simulate the parsed arguments\n",
    "\n",
    "# --- 2. Set Device ---\n",
    "if torch.cuda.is_available() and args.gpu >= 0:\n",
    "    device_string = f'cuda:{args.gpu}'\n",
    "else:\n",
    "    device_string = 'cpu'\n",
    "device = torch.device(device_string)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 3. Load Data (as done in train_self_supervised.py) ---\n",
    "# This loads node_features_np, edge_features_np, and Data objects\n",
    "# Ensure your ml_online_retail_node.npy has the feature dimension you expect (e.g., 64)\n",
    "# if you want n_node_features in TGN to be 64.\n",
    "try:\n",
    "    node_features_np, edge_features_np, full_data, train_data, val_data, test_data, \\\n",
    "    new_node_val_data, new_node_test_data = get_data(\n",
    "        dataset_name=args.data,\n",
    "        different_new_nodes_between_val_and_test=args.different_new_nodes,\n",
    "        randomize_features=args.randomize_features\n",
    "    )\n",
    "    print(f\"Data loaded for '{args.data}'.\")\n",
    "    print(f\"  Node features shape: {node_features_np.shape}\")\n",
    "    print(f\"  Edge features shape: {edge_features_np.shape}\")\n",
    "    print(f\"  Training interactions: {train_data.n_interactions}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    print(\"Please ensure that preprocess_data.py has been run correctly and the files\")\n",
    "    print(f\"ml_{args.data}.csv, ml_{args.data}.npy, ml_{args.data}_node.npy exist in ./data/\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# --- 4. Initialize Neighbor Finder ---\n",
    "# The training script initializes one for train_data and one for full_data.\n",
    "# For general use or inference, a finder built on more complete data is typical.\n",
    "# Let's use full_data for a general purpose neighbor_finder.\n",
    "# If you are about to train, you'd use train_data for train_ngh_finder.\n",
    "# For loading a pre-trained model for inference, use a finder that reflects the data seen by the model.\n",
    "ngh_finder = get_neighbor_finder(full_data, args.uniform)\n",
    "print(\"Neighbor finder initialized.\")\n",
    "\n",
    "# --- 5. Compute Time Statistics ---\n",
    "mean_time_shift_src, std_time_shift_src, mean_time_shift_dst, std_time_shift_dst = \\\n",
    "  compute_time_statistics(full_data.sources, full_data.destinations, full_data.timestamps)\n",
    "print(\"Time statistics computed.\")\n",
    "\n",
    "# --- 6. Instantiate TGN Model ---\n",
    "# Important Note on Dimensions:\n",
    "# As per tgn.py, self.n_node_features (and thus self.embedding_dimension and\n",
    "# self.time_encoder.dimension) is derived from node_features_np.shape[1].\n",
    "# The args.node_dim and args.time_dim from your command line might not directly\n",
    "# override this if your node_features_np file has a different dimension.\n",
    "# Ensure node_features_np has args.node_dim_arg columns if that's your intent.\n",
    "\n",
    "print(f\"Initializing TGN model...\")\n",
    "try:\n",
    "    tgn_model_instance = TGN(\n",
    "        neighbor_finder=ngh_finder,\n",
    "        node_features=node_features_np,    # NumPy array\n",
    "        edge_features=edge_features_np,    # NumPy array\n",
    "        device=device,\n",
    "        n_layers=args.n_layer,\n",
    "        n_heads=args.n_head,\n",
    "        dropout=args.drop_out,\n",
    "        use_memory=args.use_memory,\n",
    "        message_dimension=args.message_dim,\n",
    "        memory_dimension=args.memory_dim,\n",
    "        memory_update_at_start=not args.memory_update_at_end,\n",
    "        embedding_module_type=args.embedding_module,\n",
    "        message_function=args.message_function,\n",
    "        aggregator_type=args.aggregator,\n",
    "        memory_updater_type=args.memory_updater,\n",
    "        n_neighbors=args.n_degree,\n",
    "        mean_time_shift_src=mean_time_shift_src,\n",
    "        std_time_shift_src=std_time_shift_src,\n",
    "        mean_time_shift_dst=mean_time_shift_dst,\n",
    "        std_time_shift_dst=std_time_shift_dst,\n",
    "        use_destination_embedding_in_message=args.use_destination_embedding_in_message,\n",
    "        use_source_embedding_in_message=args.use_source_embedding_in_message,\n",
    "        dyrep=args.dyrep\n",
    "    )\n",
    "    tgn_model_instance = tgn_model_instance.to(device)\n",
    "    print(\"TGN model instance created successfully!\")\n",
    "    print(f\"  TGN n_node_features (from loaded file): {tgn_model_instance.n_node_features}\")\n",
    "    print(f\"  TGN time_encoder dimension: {tgn_model_instance.time_encoder.dimension}\")\n",
    "    print(f\"  TGN memory dimension: {tgn_model_instance.memory_dimension if tgn_model_instance.use_memory else 'N/A'}\")\n",
    "    print(f\"  TGN message dimension: {tgn_model_instance.message_function.mlp[2].out_features if args.message_function == 'mlp' and tgn_model_instance.use_memory else (tgn_model_instance.memory.message_dimension if tgn_model_instance.use_memory else 'N/A') }\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during TGN model instantiation: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "\n",
    "tgn_model_instance.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ad86ea39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TGN(\n",
       "  (time_encoder): TimeEncode(\n",
       "    (w): Linear(in_features=1, out_features=100, bias=True)\n",
       "  )\n",
       "  (memory): Memory()\n",
       "  (message_aggregator): LastMessageAggregator()\n",
       "  (message_function): IdentityMessageFunction()\n",
       "  (memory_updater): GRUMemoryUpdater(\n",
       "    (memory): Memory()\n",
       "    (layer_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "    (memory_updater): GRUCell(315, 100)\n",
       "  )\n",
       "  (embedding_module): TimeEmbedding(\n",
       "    (time_encoder): TimeEncode(\n",
       "      (w): Linear(in_features=1, out_features=100, bias=True)\n",
       "    )\n",
       "    (embedding_layer): NormalLinear(in_features=1, out_features=100, bias=True)\n",
       "  )\n",
       "  (affinity_score): MergeLayer(\n",
       "    (fc1): Linear(in_features=200, out_features=100, bias=True)\n",
       "    (fc2): Linear(in_features=100, out_features=1, bias=True)\n",
       "    (act): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgn_model_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ec96e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import math # For math.ceil\n",
    "\n",
    "# Assume TGN class and its components are defined as per the files you provided.\n",
    "# Assume Data class is also available if you choose to wrap parts of event_data_dict.\n",
    "\n",
    "def generate_clv_prediction_dataset(\n",
    "    tgn_model,  # Your trained TGN model instance\n",
    "    event_data_dict: dict,  # Contains all event streams (sources, dest, ts, edge_idx, event_datetimes)\n",
    "                            # and reference min_dataset_timestamp_ref\n",
    "    raw_transactions_df: pd.DataFrame, # Original df to calculate actual CLV\n",
    "    customer_global_id_map: dict, # Maps original customer ID to 1-based global TGN ID\n",
    "    \n",
    "    # Configuration for CLV dataset generation\n",
    "    period_start_dt: pd.Timestamp, # Actual datetime to start generating weekly samples\n",
    "    period_end_dt: pd.Timestamp,   # Actual datetime to end generating weekly samples\n",
    "    clv_horizon_months: int = 3,\n",
    "    \n",
    "    # Column names in raw_transactions_df\n",
    "    raw_df_customer_id_col: str = 'CustomerID',\n",
    "    raw_df_datetime_col: str = 'InvoiceDate',\n",
    "    raw_df_total_price_col: str = 'TotalPrice',\n",
    "    \n",
    "    batch_size_mem_update: int = 200 # Batch size for processing historical events\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a dataset for CLV prediction using embeddings from a trained TGN model.\n",
    "\n",
    "    Iterates weekly through the specified period, updates TGN memory incrementally,\n",
    "    extracts customer embeddings, and calculates the future CLV.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Generating CLV dataset from {period_start_dt} to {period_end_dt}...\")\n",
    "    tgn_model.eval()\n",
    "    # Ensure neighbor_finder is set, e.g., tgn_model.set_neighbor_finder(your_full_history_finder)\n",
    "    if tgn_model.neighbor_finder is None:\n",
    "        raise ValueError(\"TGN model must have a neighbor_finder set.\")\n",
    "\n",
    "    # Retrieve necessary arrays from event_data_dict (these are NumPy arrays)\n",
    "    #add 1 to all ids in customer_global_id_map_1_based to convert to 1-based indexing\n",
    "    customer_global_id_map_1_based = {k: v  for k, v in customer_global_id_map.items()}\n",
    "    all_sources = event_data_dict['sources']\n",
    "    all_destinations = event_data_dict['destinations']\n",
    "    all_numeric_timestamps = event_data_dict['timestamps'] # Numerical (e.g., t_seconds)\n",
    "    all_event_datetimes = event_data_dict['event_datetimes'] # pd.Timestamp objects\n",
    "    all_edge_idxs = event_data_dict['edge_idxs']\n",
    "    min_dataset_dt_ref = event_data_dict.get('min_dataset_timestamp_ref') # Should be the pd.Timestamp of time zero for numeric_timestamps\n",
    "\n",
    "    if min_dataset_dt_ref is None:\n",
    "        # Attempt to infer if not provided, assuming numeric_timestamps[0] corresponds to event_datetimes[0]\n",
    "        if len(all_event_datetimes) > 0 and len(all_numeric_timestamps) > 0 and all_numeric_timestamps[0] == 0:\n",
    "            min_dataset_dt_ref = pd.to_datetime(all_event_datetimes[0]) - pd.to_timedelta(all_numeric_timestamps[0], unit='s')\n",
    "            print(f\"Inferred min_dataset_timestamp_ref: {min_dataset_dt_ref}\")\n",
    "        else:\n",
    "            raise ValueError(\"'min_dataset_timestamp_ref' must be in event_data_dict or inferable.\")\n",
    "\n",
    "\n",
    "    clv_samples = []\n",
    "    \n",
    "    # Initialize TGN memory at the start of the overall period processing\n",
    "    if tgn_model.use_memory:\n",
    "        print(\"Initializing TGN memory...\")\n",
    "        tgn_model.memory.__init_memory__()\n",
    "\n",
    "    # Iterate weekly\n",
    "    current_week_start_dt = pd.to_datetime(period_start_dt)\n",
    "    last_processed_event_original_idx = -1 # Tracks index in the full event_data_dict\n",
    "\n",
    "    while current_week_start_dt <= pd.to_datetime(period_end_dt):\n",
    "        # Define the end of the current week for processing events & getting embeddings\n",
    "        # Embeddings will represent state at the end of this week's events\n",
    "        current_week_processing_end_dt = current_week_start_dt + pd.DateOffset(weeks=8) - pd.Timedelta(seconds=1)\n",
    "        \n",
    "        # Convert this week's end to the TGN's numerical timestamp scale\n",
    "        current_week_processing_end_numeric_ts = (current_week_processing_end_dt - min_dataset_dt_ref).total_seconds()\n",
    "\n",
    "        print(f\"\\nProcessing week ending: {current_week_processing_end_dt.strftime('%Y-%m-%d')}, \"\n",
    "              f\"Numerical TGN ts: {current_week_processing_end_numeric_ts:.2f}\")\n",
    "\n",
    "        # 1. Find events for memory update:\n",
    "        #    Events that occurred up to current_week_processing_end_dt AND haven't been processed yet.\n",
    "        #    Indices are based on the full event_data_dict arrays.\n",
    "        \n",
    "        # Find the index of the last event to process for this week's memory update\n",
    "        # np.searchsorted returns the insertion point, so events up to index-1 are <= target\n",
    "        idx_after_last_event_this_week = np.searchsorted(all_event_datetimes, \n",
    "                                                         np.datetime64(current_week_processing_end_dt), side='right')\n",
    "        \n",
    "        start_event_idx_for_update = last_processed_event_original_idx + 1\n",
    "        end_event_idx_for_update = idx_after_last_event_this_week\n",
    "\n",
    "        if start_event_idx_for_update < end_event_idx_for_update:\n",
    "            print(f\"  Updating memory with {end_event_idx_for_update - start_event_idx_for_update} new events \"\n",
    "                  f\"(indices {start_event_idx_for_update} to {end_event_idx_for_update -1})...\")\n",
    "            \n",
    "            sources_to_update = all_sources[start_event_idx_for_update:end_event_idx_for_update]\n",
    "            destinations_to_update = all_destinations[start_event_idx_for_update:end_event_idx_for_update]\n",
    "            timestamps_to_update = all_numeric_timestamps[start_event_idx_for_update:end_event_idx_for_update]\n",
    "            edge_idxs_to_update = all_edge_idxs[start_event_idx_for_update:end_event_idx_for_update]\n",
    "\n",
    "            # Process these new historical events in batches to update TGN memory\n",
    "            if tgn_model.use_memory:\n",
    "                num_batches = math.ceil(len(sources_to_update) / batch_size_mem_update)\n",
    "                for i in range(num_batches):\n",
    "                    batch_start = i * batch_size_mem_update\n",
    "                    batch_end = min(len(sources_to_update), batch_start + batch_size_mem_update)\n",
    "\n",
    "                    s_batch = sources_to_update[batch_start:batch_end]\n",
    "                    d_batch = destinations_to_update[batch_start:batch_end]\n",
    "                    ts_batch = timestamps_to_update[batch_start:batch_end]\n",
    "                    e_idx_batch = edge_idxs_to_update[batch_start:batch_end]\n",
    "                    \n",
    "                    # Get raw messages using current memory state\n",
    "                    # Assuming use_source/destination_embedding_in_message flags are False for pure memory update\n",
    "                    source_memory_batch = tgn_model.memory.get_memory(s_batch)\n",
    "                    destination_memory_batch = tgn_model.memory.get_memory(d_batch)\n",
    "\n",
    "                    unique_src, src_messages_dict = tgn_model.get_raw_messages(\n",
    "                        s_batch, source_memory_batch, d_batch, destination_memory_batch,\n",
    "                        ts_batch, e_idx_batch\n",
    "                    )\n",
    "                    unique_dst, dst_messages_dict = tgn_model.get_raw_messages(\n",
    "                        d_batch, destination_memory_batch, s_batch, source_memory_batch,\n",
    "                        ts_batch, e_idx_batch\n",
    "                    )\n",
    "                    \n",
    "                    if unique_src.size > 0:\n",
    "                         tgn_model.update_memory(unique_src, src_messages_dict)\n",
    "                    if unique_dst.size > 0:\n",
    "                         tgn_model.update_memory(unique_dst, dst_messages_dict)\n",
    "        else:\n",
    "            print(f\"  No new events to update memory for this week.\")\n",
    "\n",
    "        last_processed_event_original_idx = end_event_idx_for_update - 1\n",
    "\n",
    "        # 2. Obtain Customer Embeddings at current_week_processing_end_numeric_ts\n",
    "        # Using 1-based global IDs from your map (ensure this map is 1-based if tgn_model expects it)\n",
    "        # customer_global_id_map_1_based: map from original customer id to 1-based global_id\n",
    "        \n",
    "        # Create a mask for all events that have occurred up to the end of the current processing week\n",
    "        relevant_events_mask = (all_event_datetimes <= current_week_processing_end_dt)\n",
    "        customer_ids_from_relevant_events = all_sources[relevant_events_mask]\n",
    "        valid_customer_global_ids = set(customer_global_id_map_1_based.values())\n",
    "\n",
    "        active_customer_global_ids_set = set(\n",
    "            cid for cid in np.unique(customer_ids_from_relevant_events) if cid in valid_customer_global_ids\n",
    "        )\n",
    "\n",
    "        if not active_customer_global_ids_set:\n",
    "            print(f\"  No active customers with history found up to {current_week_processing_end_dt.strftime('%Y-%m-%d')}. Skipping embedding generation for this week.\")\n",
    "            current_week_start_dt += pd.DateOffset(weeks=4) # Move to the next week\n",
    "            continue\n",
    "\n",
    "        target_customer_global_ids_np = np.array(sorted(list(active_customer_global_ids_set)), dtype=np.int64)\n",
    "\n",
    "        query_timestamps_for_embedding_np = np.full(len(target_customer_global_ids_np),\n",
    "                                                    current_week_processing_end_numeric_ts, \n",
    "                                                    dtype=np.float32)\n",
    "\n",
    "        # Calculate time differences for these customers\n",
    "        # Ensure last_update is on CPU if doing numpy operations\n",
    "        last_update_times_np = tgn_model.memory.last_update[target_customer_global_ids_np].cpu().numpy()\n",
    "        time_diffs_np = query_timestamps_for_embedding_np - last_update_times_np\n",
    "        \n",
    "        # Normalize time_diffs (use mean_time_shift_src, assuming customers are \"sources\" here)\n",
    "        normalized_time_diffs_np = (time_diffs_np - tgn_model.mean_time_shift_src) / (tgn_model.std_time_shift_src + 1e-8) # Add epsilon for stability\n",
    "        time_diffs_tensor = torch.from_numpy(normalized_time_diffs_np).float().to(tgn_model.device)\n",
    "\n",
    "        print(f\"  Generating embeddings for {len(target_customer_global_ids_np)} customers at TGN ts {current_week_processing_end_numeric_ts:.2f}...\")\n",
    "        with torch.no_grad(): # Important for inference\n",
    "            customer_embeddings_tensor = tgn_model.embedding_module.compute_embedding(\n",
    "                memory=tgn_model.memory.memory.data, # Full current memory bank\n",
    "                source_nodes=target_customer_global_ids_np, # Customers to get embeddings for (NumPy)\n",
    "                timestamps=query_timestamps_for_embedding_np, # Query time (NumPy)\n",
    "                n_layers=tgn_model.n_layers,\n",
    "                n_neighbors=tgn_model.n_neighbors,\n",
    "                time_diffs=time_diffs_tensor # Normalized time diffs (Tensor)\n",
    "            )\n",
    "        customer_embeddings_np = customer_embeddings_tensor.cpu().numpy()\n",
    "        \n",
    "        reverse_dict_customer_global_id_map = {v: k for k, v in customer_global_id_map_1_based.items()}\n",
    "        # 3. Calculate 3-Month CLV Target for each customer\n",
    "        print(f\"  Calculating CLV targets...\")\n",
    "        print(target_customer_global_ids_np)\n",
    "        for i, global_1_based_id in enumerate(target_customer_global_ids_np):\n",
    "            original_customer_id = reverse_dict_customer_global_id_map.get(global_1_based_id)\n",
    "            if original_customer_id is None: continue\n",
    "\n",
    "            embedding_vector = customer_embeddings_np[i]\n",
    "            \n",
    "            # CLV calculation period\n",
    "            clv_period_start_dt = current_week_processing_end_dt  # Start CLV period right after embedding time\n",
    "            clv_period_end_dt = clv_period_start_dt + pd.DateOffset(months=clv_horizon_months)\n",
    "            \n",
    "            # Filter raw transactions for this customer in the future CLV window\n",
    "            customer_future_transactions = raw_transactions_df[\n",
    "                (raw_transactions_df[raw_df_customer_id_col] == original_customer_id) &\n",
    "                (pd.to_datetime(raw_transactions_df[raw_df_datetime_col]) > clv_period_start_dt) &\n",
    "                (pd.to_datetime(raw_transactions_df[raw_df_datetime_col]) < clv_period_end_dt) # Use < for end if DateOffset gives start of next month\n",
    "            ]\n",
    "            \n",
    "            clv_target = customer_future_transactions[raw_df_total_price_col].sum()\n",
    "            if pd.isna(clv_target): clv_target = 0.0\n",
    "\n",
    "            clv_samples.append({\n",
    "                'original_customer_id': original_customer_id,\n",
    "                'global_tgn_id': global_1_based_id,\n",
    "                'prediction_date': current_week_processing_end_dt, # Actual datetime\n",
    "                'embedding': embedding_vector,\n",
    "                'clv_3_month_target': clv_target\n",
    "            })\n",
    "        \n",
    "        print(f\"  Generated {len(target_customer_global_ids_np)} embedding/CLV pairs for this week.\")\n",
    "        # Move to the next week\n",
    "        current_week_start_dt += pd.DateOffset(weeks=8)\n",
    "        \n",
    "    return pd.DataFrame(clv_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e539278b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating CLV dataset from 2010-02-17 00:00:00 to 2011-08-31 00:00:00...\n",
      "Inferred min_dataset_timestamp_ref: 2009-12-31 00:00:00\n",
      "Initializing TGN memory...\n",
      "\n",
      "Processing week ending: 2010-04-13, Numerical TGN ts: 8985599.00\n",
      "  Updating memory with 95306 new events (indices 0 to 95305)...\n",
      "  Generating embeddings for 2102 customers at TGN ts 8985599.00...\n",
      "  Calculating CLV targets...\n",
      "[   1    2    3 ... 2100 2101 2102]\n",
      "  Generated 2102 embedding/CLV pairs for this week.\n",
      "\n",
      "Processing week ending: 2010-06-08, Numerical TGN ts: 13823999.00\n",
      "  Updating memory with 50306 new events (indices 95306 to 145611)...\n",
      "  Generating embeddings for 2640 customers at TGN ts 13823999.00...\n",
      "  Calculating CLV targets...\n",
      "[   1    2    3 ... 2638 2639 2640]\n",
      "  Generated 2640 embedding/CLV pairs for this week.\n",
      "\n",
      "Processing week ending: 2010-08-03, Numerical TGN ts: 18662399.00\n",
      "  Updating memory with 52134 new events (indices 145612 to 197745)...\n",
      "  Generating embeddings for 3093 customers at TGN ts 18662399.00...\n",
      "  Calculating CLV targets...\n",
      "[   1    2    3 ... 3091 3092 3093]\n",
      "  Generated 3093 embedding/CLV pairs for this week.\n",
      "\n",
      "Processing week ending: 2010-09-28, Numerical TGN ts: 23500799.00\n",
      "  Updating memory with 23818 new events (indices 197746 to 221563)...\n",
      "  Generating embeddings for 3255 customers at TGN ts 23500799.00...\n",
      "  Calculating CLV targets...\n",
      "[   1    2    3 ... 3253 3254 3255]\n",
      "  Generated 3255 embedding/CLV pairs for this week.\n",
      "\n",
      "Processing week ending: 2010-11-23, Numerical TGN ts: 28339199.00\n",
      "  Updating memory with 75719 new events (indices 221564 to 297282)...\n",
      "  Generating embeddings for 3860 customers at TGN ts 28339199.00...\n",
      "  Calculating CLV targets...\n",
      "[   1    2    3 ... 3858 3859 3860]\n",
      "  Generated 3860 embedding/CLV pairs for this week.\n",
      "\n",
      "Processing week ending: 2011-01-18, Numerical TGN ts: 33177599.00\n",
      "  Updating memory with 74615 new events (indices 297283 to 371897)...\n",
      "  Generating embeddings for 4258 customers at TGN ts 33177599.00...\n",
      "  Calculating CLV targets...\n",
      "[   1    2    3 ... 4256 4257 4258]\n",
      "  Generated 4258 embedding/CLV pairs for this week.\n",
      "\n",
      "Processing week ending: 2011-03-15, Numerical TGN ts: 38015999.00\n",
      "  Updating memory with 37703 new events (indices 371898 to 409600)...\n",
      "  Generating embeddings for 4455 customers at TGN ts 38015999.00...\n",
      "  Calculating CLV targets...\n",
      "[   1    2    3 ... 4453 4454 4455]\n",
      "  Generated 4455 embedding/CLV pairs for this week.\n",
      "\n",
      "Processing week ending: 2011-05-10, Numerical TGN ts: 42854399.00\n",
      "  Updating memory with 45661 new events (indices 409601 to 455261)...\n",
      "  Generating embeddings for 4738 customers at TGN ts 42854399.00...\n",
      "  Calculating CLV targets...\n",
      "[   1    2    3 ... 4736 4737 4738]\n",
      "  Generated 4738 embedding/CLV pairs for this week.\n",
      "\n",
      "Processing week ending: 2011-07-05, Numerical TGN ts: 47692799.00\n",
      "  Updating memory with 50512 new events (indices 455262 to 505773)...\n",
      "  Generating embeddings for 4956 customers at TGN ts 47692799.00...\n",
      "  Calculating CLV targets...\n",
      "[   1    2    3 ... 4954 4955 4956]\n",
      "  Generated 4956 embedding/CLV pairs for this week.\n",
      "\n",
      "Processing week ending: 2011-08-30, Numerical TGN ts: 52531199.00\n",
      "  Updating memory with 24202 new events (indices 505774 to 529975)...\n",
      "  Generating embeddings for 5056 customers at TGN ts 52531199.00...\n",
      "  Calculating CLV targets...\n",
      "[   1    2    3 ... 5054 5055 5056]\n",
      "  Generated 5056 embedding/CLV pairs for this week.\n",
      "\n",
      "Processing week ending: 2011-10-25, Numerical TGN ts: 57369599.00\n",
      "  Updating memory with 24618 new events (indices 529976 to 554593)...\n",
      "  Generating embeddings for 5166 customers at TGN ts 57369599.00...\n",
      "  Calculating CLV targets...\n",
      "[   1    2    3 ... 5164 5165 5166]\n",
      "  Generated 5166 embedding/CLV pairs for this week.\n"
     ]
    }
   ],
   "source": [
    "prediction_dataset = generate_clv_prediction_dataset(tgn_model=tgn_model_instance,\n",
    "                                                     event_data_dict=input_data,\n",
    "                                                   raw_transactions_df=df_processed,\n",
    "                                                  customer_global_id_map= customer_internal_map,\n",
    "                                                   period_start_dt=pd.Timestamp('2010-02-17'),\n",
    "                                                  period_end_dt=pd.Timestamp('2011-08-31'),)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0f07cac0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Data' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_data\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Data' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "05797d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_dataset.to_csv('data/clv_prediction_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "74ab4508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_customer_id</th>\n",
       "      <th>global_tgn_id</th>\n",
       "      <th>prediction_date</th>\n",
       "      <th>embedding</th>\n",
       "      <th>clv_3_month_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41201</th>\n",
       "      <td>15866.0</td>\n",
       "      <td>5189</td>\n",
       "      <td>2011-08-31 23:59:59</td>\n",
       "      <td>[0.047117744, -0.07445397, -0.174664, -0.12116...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41202</th>\n",
       "      <td>14660.0</td>\n",
       "      <td>5190</td>\n",
       "      <td>2011-08-31 23:59:59</td>\n",
       "      <td>[-0.18408115, -0.09849412, 0.10411159, 0.06672...</td>\n",
       "      <td>156.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41203</th>\n",
       "      <td>13726.0</td>\n",
       "      <td>5191</td>\n",
       "      <td>2011-08-31 23:59:59</td>\n",
       "      <td>[-0.037142083, -0.028932337, -0.29624906, -0.1...</td>\n",
       "      <td>786.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41204</th>\n",
       "      <td>15690.0</td>\n",
       "      <td>5192</td>\n",
       "      <td>2011-08-31 23:59:59</td>\n",
       "      <td>[0.008513287, -0.09047374, -0.23279461, -0.120...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41205</th>\n",
       "      <td>17777.0</td>\n",
       "      <td>5193</td>\n",
       "      <td>2011-08-31 23:59:59</td>\n",
       "      <td>[0.044509944, -0.054147597, -0.15317921, -0.03...</td>\n",
       "      <td>471.16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       original_customer_id  global_tgn_id     prediction_date  \\\n",
       "41201               15866.0           5189 2011-08-31 23:59:59   \n",
       "41202               14660.0           5190 2011-08-31 23:59:59   \n",
       "41203               13726.0           5191 2011-08-31 23:59:59   \n",
       "41204               15690.0           5192 2011-08-31 23:59:59   \n",
       "41205               17777.0           5193 2011-08-31 23:59:59   \n",
       "\n",
       "                                               embedding  clv_3_month_target  \n",
       "41201  [0.047117744, -0.07445397, -0.174664, -0.12116...                0.00  \n",
       "41202  [-0.18408115, -0.09849412, 0.10411159, 0.06672...              156.79  \n",
       "41203  [-0.037142083, -0.028932337, -0.29624906, -0.1...              786.98  \n",
       "41204  [0.008513287, -0.09047374, -0.23279461, -0.120...                0.00  \n",
       "41205  [0.044509944, -0.054147597, -0.15317921, -0.03...              471.16  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dataset with the customer embeddings and CLV targets\n",
    "prediction_dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a5f8d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of predictors (X): (28401, 100)\n",
      "Shape of target (y): (28401,)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train_data = prediction_dataset[(prediction_dataset['prediction_date'] >= pd.to_datetime('2009-12-01'))&(prediction_dataset['prediction_date'] <= pd.to_datetime('2011-06-01'))]\n",
    "    X = np.vstack(train_data['embedding'].values)\n",
    "except ValueError as e:\n",
    "    # This might happen if embeddings are not all the same length, or if there are NaNs/None\n",
    "    print(f\"Error stacking embeddings: {e}\")\n",
    "    print(\"Ensure all embeddings have the same dimension and there are no missing values.\")\n",
    "    # Handle error appropriately, e.g., by filtering problematic rows or investigating\n",
    "    # For now, let's assume embeddings are clean for this example\n",
    "    # You might need to filter clv_prediction_df first if there are issues:\n",
    "    # clv_prediction_df = clv_prediction_df.dropna(subset=['embedding'])\n",
    "    # clv_prediction_df = clv_prediction_df[clv_prediction_df['embedding'].apply(lambda x: isinstance(x, np.ndarray) and x.shape == (EXPECTED_DIM,))]\n",
    "    # X = np.vstack(clv_prediction_df['embedding'].values)\n",
    "    raise\n",
    "\n",
    "# The 'clv_3_month_target' column is your target variable.\n",
    "y = train_data['clv_3_month_target'].values\n",
    "\n",
    "# --- Verify shapes ---\n",
    "print(f\"Shape of predictors (X): {X.shape}\") # Should be (num_samples, embedding_dimension)\n",
    "print(f\"Shape of target (y): {y.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98146f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 36\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Randomized search\u001b[39;00m\n\u001b[1;32m     25\u001b[0m search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[1;32m     26\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mxgb_model,\n\u001b[1;32m     27\u001b[0m     param_distributions\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     34\u001b[0m )\n\u001b[0;32m---> 36\u001b[0m search\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[1;32m     38\u001b[0m best_model \u001b[38;5;241m=\u001b[39m search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1018\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1013\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1014\u001b[0m     )\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1018\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1959\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1957\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1958\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1959\u001b[0m     evaluate_candidates(\n\u001b[1;32m   1960\u001b[0m         ParameterSampler(\n\u001b[1;32m   1961\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_distributions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state\n\u001b[1;32m   1962\u001b[0m         )\n\u001b[1;32m   1963\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:964\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    958\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    960\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    961\u001b[0m         )\n\u001b[1;32m    962\u001b[0m     )\n\u001b[0;32m--> 964\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    965\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    966\u001b[0m         clone(base_estimator),\n\u001b[1;32m    967\u001b[0m         X,\n\u001b[1;32m    968\u001b[0m         y,\n\u001b[1;32m    969\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[1;32m    970\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[1;32m    971\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m    972\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[1;32m    973\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[1;32m    974\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[1;32m    975\u001b[0m     )\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[1;32m    978\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[1;32m    979\u001b[0m     )\n\u001b[1;32m    980\u001b[0m )\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    983\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    984\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    987\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [ 300, 400, 500, 600],\n",
    "    'max_depth': [6, 7, 8],\n",
    "    'learning_rate': [0.01, 0.02],\n",
    "    'subsample': [0.6, 0.8],\n",
    "    'colsample_bytree': [ 0.8, 0.9],\n",
    "    'gamma': [  0.2, 0.3],\n",
    "    'reg_alpha': [ 0.1, 0.5],\n",
    "    'reg_lambda': [ 2, 2.3]}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    eval_metric='rmse',\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Randomized search\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=50,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "search.fit(X, y)\n",
    "\n",
    "best_model = search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc607d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:05:05,550] A new study created in memory with name: lgbm_clv_optimization_cv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Optuna study with Cross-Validation: lgbm_clv_optimization_cv\n",
      "  Trial 0: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 2500, 'learning_rate': 0.004104546920696389, 'num_leaves': 268, 'max_depth': 10, 'min_child_samples': 66, 'subsample': 0.5172670677829115, 'colsample_bytree': 0.9619870952166075, 'reg_alpha': 1.0844553342258848e-08, 'reg_lambda': 0.0010718534171174248}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:05:35,910] Trial 0 finished with value: 840.8908532904713 and parameters: {'n_estimators': 2500, 'learning_rate': 0.004104546920696389, 'num_leaves': 268, 'max_depth': 10, 'min_child_samples': 66, 'subsample': 0.5172670677829115, 'colsample_bytree': 0.9619870952166075, 'reg_alpha': 1.0844553342258848e-08, 'reg_lambda': 0.0010718534171174248}. Best is trial 0 with value: 840.8908532904713.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 0: Average RMSE across 5 folds: 840.8909\n",
      "  Trial 1: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 1650, 'learning_rate': 0.09473929867173386, 'num_leaves': 41, 'max_depth': 12, 'min_child_samples': 6, 'subsample': 0.6183057084118296, 'colsample_bytree': 0.8549924860978079, 'reg_alpha': 1.2497227036618799, 'reg_lambda': 0.020201225347136642}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:05:38,707] Trial 1 finished with value: 827.1979538772964 and parameters: {'n_estimators': 1650, 'learning_rate': 0.09473929867173386, 'num_leaves': 41, 'max_depth': 12, 'min_child_samples': 6, 'subsample': 0.6183057084118296, 'colsample_bytree': 0.8549924860978079, 'reg_alpha': 1.2497227036618799, 'reg_lambda': 0.020201225347136642}. Best is trial 1 with value: 827.1979538772964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 1: Average RMSE across 5 folds: 827.1980\n",
      "  Trial 2: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 1300, 'learning_rate': 0.012155998257398933, 'num_leaves': 222, 'max_depth': 14, 'min_child_samples': 27, 'subsample': 0.9630328760565581, 'colsample_bytree': 0.6247313332074458, 'reg_alpha': 0.10034694919994465, 'reg_lambda': 0.00010698732738163738}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:06:01,387] Trial 2 finished with value: 807.2865304543182 and parameters: {'n_estimators': 1300, 'learning_rate': 0.012155998257398933, 'num_leaves': 222, 'max_depth': 14, 'min_child_samples': 27, 'subsample': 0.9630328760565581, 'colsample_bytree': 0.6247313332074458, 'reg_alpha': 0.10034694919994465, 'reg_lambda': 0.00010698732738163738}. Best is trial 2 with value: 807.2865304543182.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 2: Average RMSE across 5 folds: 807.2865\n",
      "  Trial 3: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 1900, 'learning_rate': 0.001521876242564328, 'num_leaves': 272, 'max_depth': 3, 'min_child_samples': 51, 'subsample': 0.6826534110631213, 'colsample_bytree': 0.5999410053727405, 'reg_alpha': 1.1729447280970825e-05, 'reg_lambda': 2.2106560580905104}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:06:08,822] Trial 3 finished with value: 870.174260659662 and parameters: {'n_estimators': 1900, 'learning_rate': 0.001521876242564328, 'num_leaves': 272, 'max_depth': 3, 'min_child_samples': 51, 'subsample': 0.6826534110631213, 'colsample_bytree': 0.5999410053727405, 'reg_alpha': 1.1729447280970825e-05, 'reg_lambda': 2.2106560580905104}. Best is trial 2 with value: 807.2865304543182.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 3: Average RMSE across 5 folds: 870.1743\n",
      "  Trial 4: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 200, 'learning_rate': 0.13086512218896598, 'num_leaves': 210, 'max_depth': 10, 'min_child_samples': 6, 'subsample': 0.5316473365709898, 'colsample_bytree': 0.5878271601298676, 'reg_alpha': 0.2046029122704128, 'reg_lambda': 0.046468610047953565}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:06:12,622] Trial 4 finished with value: 825.7562440938011 and parameters: {'n_estimators': 200, 'learning_rate': 0.13086512218896598, 'num_leaves': 210, 'max_depth': 10, 'min_child_samples': 6, 'subsample': 0.5316473365709898, 'colsample_bytree': 0.5878271601298676, 'reg_alpha': 0.2046029122704128, 'reg_lambda': 0.046468610047953565}. Best is trial 2 with value: 807.2865304543182.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 4: Average RMSE across 5 folds: 825.7562\n",
      "  Trial 5: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 2300, 'learning_rate': 0.003462976513543267, 'num_leaves': 234, 'max_depth': 7, 'min_child_samples': 57, 'subsample': 0.6324662023000134, 'colsample_bytree': 0.6419218314856161, 'reg_alpha': 0.026807403301016733, 'reg_lambda': 0.8448476098867589}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:06:27,011] Trial 5 finished with value: 853.7621961544295 and parameters: {'n_estimators': 2300, 'learning_rate': 0.003462976513543267, 'num_leaves': 234, 'max_depth': 7, 'min_child_samples': 57, 'subsample': 0.6324662023000134, 'colsample_bytree': 0.6419218314856161, 'reg_alpha': 0.026807403301016733, 'reg_lambda': 0.8448476098867589}. Best is trial 2 with value: 807.2865304543182.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 5: Average RMSE across 5 folds: 853.7622\n",
      "  Trial 6: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 2250, 'learning_rate': 0.00416910496668156, 'num_leaves': 278, 'max_depth': 15, 'min_child_samples': 95, 'subsample': 0.7203948083940418, 'colsample_bytree': 0.9719303011517244, 'reg_alpha': 1.0492899584100662e-05, 'reg_lambda': 9.537157778034615}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:07:11,371] Trial 6 finished with value: 855.7339568328471 and parameters: {'n_estimators': 2250, 'learning_rate': 0.00416910496668156, 'num_leaves': 278, 'max_depth': 15, 'min_child_samples': 95, 'subsample': 0.7203948083940418, 'colsample_bytree': 0.9719303011517244, 'reg_alpha': 1.0492899584100662e-05, 'reg_lambda': 9.537157778034615}. Best is trial 2 with value: 807.2865304543182.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 6: Average RMSE across 5 folds: 855.7340\n",
      "  Trial 7: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 1600, 'learning_rate': 0.030958661469354685, 'num_leaves': 267, 'max_depth': 12, 'min_child_samples': 55, 'subsample': 0.6767703322653962, 'colsample_bytree': 0.7083891509042508, 'reg_alpha': 0.000466250829232094, 'reg_lambda': 3.5229240648492137}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:07:22,770] Trial 7 finished with value: 841.8258809707155 and parameters: {'n_estimators': 1600, 'learning_rate': 0.030958661469354685, 'num_leaves': 267, 'max_depth': 12, 'min_child_samples': 55, 'subsample': 0.6767703322653962, 'colsample_bytree': 0.7083891509042508, 'reg_alpha': 0.000466250829232094, 'reg_lambda': 3.5229240648492137}. Best is trial 2 with value: 807.2865304543182.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 7: Average RMSE across 5 folds: 841.8259\n",
      "  Trial 8: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 750, 'learning_rate': 0.006257887940885135, 'num_leaves': 116, 'max_depth': 15, 'min_child_samples': 91, 'subsample': 0.9317302832773698, 'colsample_bytree': 0.5868584755966078, 'reg_alpha': 8.77883292052841e-06, 'reg_lambda': 1.0338263236587242}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:07:41,597] Trial 8 finished with value: 867.0146464400884 and parameters: {'n_estimators': 750, 'learning_rate': 0.006257887940885135, 'num_leaves': 116, 'max_depth': 15, 'min_child_samples': 91, 'subsample': 0.9317302832773698, 'colsample_bytree': 0.5868584755966078, 'reg_alpha': 8.77883292052841e-06, 'reg_lambda': 1.0338263236587242}. Best is trial 2 with value: 807.2865304543182.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 8: Average RMSE across 5 folds: 867.0146\n",
      "  Trial 9: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 850, 'learning_rate': 0.007709908233866313, 'num_leaves': 92, 'max_depth': 6, 'min_child_samples': 10, 'subsample': 0.8482140154992671, 'colsample_bytree': 0.51066056912237, 'reg_alpha': 4.086378750788909e-05, 'reg_lambda': 1.1954611661949429e-05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:07:47,449] Trial 9 finished with value: 801.9846447192695 and parameters: {'n_estimators': 850, 'learning_rate': 0.007709908233866313, 'num_leaves': 92, 'max_depth': 6, 'min_child_samples': 10, 'subsample': 0.8482140154992671, 'colsample_bytree': 0.51066056912237, 'reg_alpha': 4.086378750788909e-05, 'reg_lambda': 1.1954611661949429e-05}. Best is trial 9 with value: 801.9846447192695.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 9: Average RMSE across 5 folds: 801.9846\n",
      "  Trial 10: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 950, 'learning_rate': 0.03161029921468205, 'num_leaves': 109, 'max_depth': 5, 'min_child_samples': 30, 'subsample': 0.8312951342909024, 'colsample_bytree': 0.5290781382007783, 'reg_alpha': 7.380340044696241e-08, 'reg_lambda': 2.0678330375399114e-07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:07:50,917] Trial 10 finished with value: 798.4674562694391 and parameters: {'n_estimators': 950, 'learning_rate': 0.03161029921468205, 'num_leaves': 109, 'max_depth': 5, 'min_child_samples': 30, 'subsample': 0.8312951342909024, 'colsample_bytree': 0.5290781382007783, 'reg_alpha': 7.380340044696241e-08, 'reg_lambda': 2.0678330375399114e-07}. Best is trial 10 with value: 798.4674562694391.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 10: Average RMSE across 5 folds: 798.4675\n",
      "  Trial 11: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 1000, 'learning_rate': 0.03398603337774371, 'num_leaves': 101, 'max_depth': 5, 'min_child_samples': 29, 'subsample': 0.8512605022622334, 'colsample_bytree': 0.5008913204766638, 'reg_alpha': 1.50252039847086e-08, 'reg_lambda': 1.2637011421921809e-07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:07:54,560] Trial 11 finished with value: 793.8963282978019 and parameters: {'n_estimators': 1000, 'learning_rate': 0.03398603337774371, 'num_leaves': 101, 'max_depth': 5, 'min_child_samples': 29, 'subsample': 0.8512605022622334, 'colsample_bytree': 0.5008913204766638, 'reg_alpha': 1.50252039847086e-08, 'reg_lambda': 1.2637011421921809e-07}. Best is trial 11 with value: 793.8963282978019.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 11: Average RMSE across 5 folds: 793.8963\n",
      "  Trial 12: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 950, 'learning_rate': 0.03061800553534774, 'num_leaves': 140, 'max_depth': 3, 'min_child_samples': 32, 'subsample': 0.8374192819592658, 'colsample_bytree': 0.5006001150847229, 'reg_alpha': 1.3881540163666269e-08, 'reg_lambda': 1.0842335797755123e-08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:07:57,715] Trial 12 finished with value: 791.8630722748783 and parameters: {'n_estimators': 950, 'learning_rate': 0.03061800553534774, 'num_leaves': 140, 'max_depth': 3, 'min_child_samples': 32, 'subsample': 0.8374192819592658, 'colsample_bytree': 0.5006001150847229, 'reg_alpha': 1.3881540163666269e-08, 'reg_lambda': 1.0842335797755123e-08}. Best is trial 12 with value: 791.8630722748783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 12: Average RMSE across 5 folds: 791.8631\n",
      "  Trial 13: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 400, 'learning_rate': 0.049833001193978474, 'num_leaves': 162, 'max_depth': 3, 'min_child_samples': 33, 'subsample': 0.816664463164372, 'colsample_bytree': 0.767244517667308, 'reg_alpha': 1.9320357719251822e-07, 'reg_lambda': 1.0250174042974724e-08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:07:59,808] Trial 13 finished with value: 800.9043191858126 and parameters: {'n_estimators': 400, 'learning_rate': 0.049833001193978474, 'num_leaves': 162, 'max_depth': 3, 'min_child_samples': 33, 'subsample': 0.816664463164372, 'colsample_bytree': 0.767244517667308, 'reg_alpha': 1.9320357719251822e-07, 'reg_lambda': 1.0250174042974724e-08}. Best is trial 12 with value: 791.8630722748783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 13: Average RMSE across 5 folds: 800.9043\n",
      "  Trial 14: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 1200, 'learning_rate': 0.2363926008628011, 'num_leaves': 158, 'max_depth': 5, 'min_child_samples': 40, 'subsample': 0.9026202162246895, 'colsample_bytree': 0.5078731094499146, 'reg_alpha': 4.4035138123444737e-07, 'reg_lambda': 1.8499474271995158e-07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:08:00,891] Trial 14 finished with value: 829.383691587274 and parameters: {'n_estimators': 1200, 'learning_rate': 0.2363926008628011, 'num_leaves': 158, 'max_depth': 5, 'min_child_samples': 40, 'subsample': 0.9026202162246895, 'colsample_bytree': 0.5078731094499146, 'reg_alpha': 4.4035138123444737e-07, 'reg_lambda': 1.8499474271995158e-07}. Best is trial 12 with value: 791.8630722748783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 14: Average RMSE across 5 folds: 829.3837\n",
      "  Trial 15: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 550, 'learning_rate': 0.018631235817176628, 'num_leaves': 38, 'max_depth': 8, 'min_child_samples': 20, 'subsample': 0.7622984193332675, 'colsample_bytree': 0.7047081777088151, 'reg_alpha': 2.1015058838830428e-08, 'reg_lambda': 2.4528468587956853e-08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:08:05,567] Trial 15 finished with value: 806.8480599956774 and parameters: {'n_estimators': 550, 'learning_rate': 0.018631235817176628, 'num_leaves': 38, 'max_depth': 8, 'min_child_samples': 20, 'subsample': 0.7622984193332675, 'colsample_bytree': 0.7047081777088151, 'reg_alpha': 2.1015058838830428e-08, 'reg_lambda': 2.4528468587956853e-08}. Best is trial 12 with value: 791.8630722748783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 15: Average RMSE across 5 folds: 806.8481\n",
      "  Trial 16: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 1100, 'learning_rate': 0.05997077008478248, 'num_leaves': 156, 'max_depth': 4, 'min_child_samples': 42, 'subsample': 0.8803774064810147, 'colsample_bytree': 0.5565608200653163, 'reg_alpha': 1.0092035233576611e-06, 'reg_lambda': 2.2014956024028073e-06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:08:07,626] Trial 16 finished with value: 810.864947763085 and parameters: {'n_estimators': 1100, 'learning_rate': 0.05997077008478248, 'num_leaves': 156, 'max_depth': 4, 'min_child_samples': 42, 'subsample': 0.8803774064810147, 'colsample_bytree': 0.5565608200653163, 'reg_alpha': 1.0092035233576611e-06, 'reg_lambda': 2.2014956024028073e-06}. Best is trial 12 with value: 791.8630722748783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 16: Average RMSE across 5 folds: 810.8649\n",
      "  Trial 17: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 1500, 'learning_rate': 0.020391080145637747, 'num_leaves': 75, 'max_depth': 6, 'min_child_samples': 76, 'subsample': 0.9960013525821114, 'colsample_bytree': 0.8017329871870238, 'reg_alpha': 0.004087662552005817, 'reg_lambda': 8.919290355021397e-07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:08:14,696] Trial 17 finished with value: 822.6929687409851 and parameters: {'n_estimators': 1500, 'learning_rate': 0.020391080145637747, 'num_leaves': 75, 'max_depth': 6, 'min_child_samples': 76, 'subsample': 0.9960013525821114, 'colsample_bytree': 0.8017329871870238, 'reg_alpha': 0.004087662552005817, 'reg_lambda': 8.919290355021397e-07}. Best is trial 12 with value: 791.8630722748783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 17: Average RMSE across 5 folds: 822.6930\n",
      "  Trial 18: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 1000, 'learning_rate': 0.28435674922308957, 'num_leaves': 130, 'max_depth': 3, 'min_child_samples': 24, 'subsample': 0.7729471401678311, 'colsample_bytree': 0.6493294690368759, 'reg_alpha': 1.1990839497212994e-06, 'reg_lambda': 4.130433627257825e-05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:08:15,424] Trial 18 finished with value: 815.8800307759759 and parameters: {'n_estimators': 1000, 'learning_rate': 0.28435674922308957, 'num_leaves': 130, 'max_depth': 3, 'min_child_samples': 24, 'subsample': 0.7729471401678311, 'colsample_bytree': 0.6493294690368759, 'reg_alpha': 1.1990839497212994e-06, 'reg_lambda': 4.130433627257825e-05}. Best is trial 12 with value: 791.8630722748783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 18: Average RMSE across 5 folds: 815.8800\n",
      "  Trial 19: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 600, 'learning_rate': 0.09990211324865919, 'num_leaves': 67, 'max_depth': 8, 'min_child_samples': 18, 'subsample': 0.7958237447395756, 'colsample_bytree': 0.8759422146450632, 'reg_alpha': 0.00031938737264917017, 'reg_lambda': 7.041717752624816e-08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:08:17,535] Trial 19 finished with value: 819.3213133232064 and parameters: {'n_estimators': 600, 'learning_rate': 0.09990211324865919, 'num_leaves': 67, 'max_depth': 8, 'min_child_samples': 18, 'subsample': 0.7958237447395756, 'colsample_bytree': 0.8759422146450632, 'reg_alpha': 0.00031938737264917017, 'reg_lambda': 7.041717752624816e-08}. Best is trial 12 with value: 791.8630722748783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 19: Average RMSE across 5 folds: 819.3213\n",
      "  Trial 20: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 1900, 'learning_rate': 0.043114553898796426, 'num_leaves': 185, 'max_depth': 5, 'min_child_samples': 43, 'subsample': 0.8752927593306525, 'colsample_bytree': 0.6862624028138999, 'reg_alpha': 6.501150915196457e-08, 'reg_lambda': 2.016093422667151e-06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:08:20,840] Trial 20 finished with value: 821.2076512416703 and parameters: {'n_estimators': 1900, 'learning_rate': 0.043114553898796426, 'num_leaves': 185, 'max_depth': 5, 'min_child_samples': 43, 'subsample': 0.8752927593306525, 'colsample_bytree': 0.6862624028138999, 'reg_alpha': 6.501150915196457e-08, 'reg_lambda': 2.016093422667151e-06}. Best is trial 12 with value: 791.8630722748783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 20: Average RMSE across 5 folds: 821.2077\n",
      "  Trial 21: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 950, 'learning_rate': 0.028706651864811077, 'num_leaves': 114, 'max_depth': 5, 'min_child_samples': 32, 'subsample': 0.8353802335120506, 'colsample_bytree': 0.5472988708282386, 'reg_alpha': 7.600164868661735e-08, 'reg_lambda': 2.5291347519435956e-07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:08:24,701] Trial 21 finished with value: 807.5067106038725 and parameters: {'n_estimators': 950, 'learning_rate': 0.028706651864811077, 'num_leaves': 114, 'max_depth': 5, 'min_child_samples': 32, 'subsample': 0.8353802335120506, 'colsample_bytree': 0.5472988708282386, 'reg_alpha': 7.600164868661735e-08, 'reg_lambda': 2.5291347519435956e-07}. Best is trial 12 with value: 791.8630722748783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 21: Average RMSE across 5 folds: 807.5067\n",
      "  Trial 22: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 800, 'learning_rate': 0.012454670410856955, 'num_leaves': 136, 'max_depth': 4, 'min_child_samples': 34, 'subsample': 0.9300787344944368, 'colsample_bytree': 0.5086343703124705, 'reg_alpha': 1.7045983982918274e-08, 'reg_lambda': 5.723609803315634e-08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:08:28,591] Trial 22 finished with value: 805.0304446160114 and parameters: {'n_estimators': 800, 'learning_rate': 0.012454670410856955, 'num_leaves': 136, 'max_depth': 4, 'min_child_samples': 34, 'subsample': 0.9300787344944368, 'colsample_bytree': 0.5086343703124705, 'reg_alpha': 1.7045983982918274e-08, 'reg_lambda': 5.723609803315634e-08}. Best is trial 12 with value: 791.8630722748783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 22: Average RMSE across 5 folds: 805.0304\n",
      "  Trial 23: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 1200, 'learning_rate': 0.029327116776010697, 'num_leaves': 96, 'max_depth': 7, 'min_child_samples': 15, 'subsample': 0.7344306273622935, 'colsample_bytree': 0.5504127527957405, 'reg_alpha': 1.8259137180882807e-06, 'reg_lambda': 5.39651591687397e-07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:08:31,747] Trial 23 finished with value: 801.3250024434499 and parameters: {'n_estimators': 1200, 'learning_rate': 0.029327116776010697, 'num_leaves': 96, 'max_depth': 7, 'min_child_samples': 15, 'subsample': 0.7344306273622935, 'colsample_bytree': 0.5504127527957405, 'reg_alpha': 1.8259137180882807e-06, 'reg_lambda': 5.39651591687397e-07}. Best is trial 12 with value: 791.8630722748783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 23: Average RMSE across 5 folds: 801.3250\n",
      "  Trial 24: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 650, 'learning_rate': 0.07428184532036726, 'num_leaves': 73, 'max_depth': 4, 'min_child_samples': 49, 'subsample': 0.8438996408612331, 'colsample_bytree': 0.5005355195443624, 'reg_alpha': 1.1248947120104615e-07, 'reg_lambda': 1.1552579634380891e-08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:08:33,596] Trial 24 finished with value: 809.0603501839987 and parameters: {'n_estimators': 650, 'learning_rate': 0.07428184532036726, 'num_leaves': 73, 'max_depth': 4, 'min_child_samples': 49, 'subsample': 0.8438996408612331, 'colsample_bytree': 0.5005355195443624, 'reg_alpha': 1.1248947120104615e-07, 'reg_lambda': 1.1552579634380891e-08}. Best is trial 12 with value: 791.8630722748783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 24: Average RMSE across 5 folds: 809.0604\n",
      "  Trial 25: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 1400, 'learning_rate': 0.15518054446976756, 'num_leaves': 176, 'max_depth': 6, 'min_child_samples': 28, 'subsample': 0.8041677802891849, 'colsample_bytree': 0.5520087191114168, 'reg_alpha': 1.0393530080753283e-08, 'reg_lambda': 7.354043756086853e-06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:08:34,744] Trial 25 finished with value: 817.1751832343782 and parameters: {'n_estimators': 1400, 'learning_rate': 0.15518054446976756, 'num_leaves': 176, 'max_depth': 6, 'min_child_samples': 28, 'subsample': 0.8041677802891849, 'colsample_bytree': 0.5520087191114168, 'reg_alpha': 1.0393530080753283e-08, 'reg_lambda': 7.354043756086853e-06}. Best is trial 12 with value: 791.8630722748783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 25: Average RMSE across 5 folds: 817.1752\n",
      "  Trial 26: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 400, 'learning_rate': 0.012008635269949506, 'num_leaves': 22, 'max_depth': 4, 'min_child_samples': 39, 'subsample': 0.8978977531975133, 'colsample_bytree': 0.5424184774518102, 'reg_alpha': 2.2530609262432086e-07, 'reg_lambda': 1.2902636557605727e-07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:08:37,034] Trial 26 finished with value: 831.587243859907 and parameters: {'n_estimators': 400, 'learning_rate': 0.012008635269949506, 'num_leaves': 22, 'max_depth': 4, 'min_child_samples': 39, 'subsample': 0.8978977531975133, 'colsample_bytree': 0.5424184774518102, 'reg_alpha': 2.2530609262432086e-07, 'reg_lambda': 1.2902636557605727e-07}. Best is trial 12 with value: 791.8630722748783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 26: Average RMSE across 5 folds: 831.5872\n",
      "  Trial 27: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 1050, 'learning_rate': 0.039772068469465606, 'num_leaves': 136, 'max_depth': 7, 'min_child_samples': 62, 'subsample': 0.8590196218875084, 'colsample_bytree': 0.6699348747251256, 'reg_alpha': 5.8858244212605255e-05, 'reg_lambda': 0.0010384986920524888}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:08:40,856] Trial 27 finished with value: 821.1373624313441 and parameters: {'n_estimators': 1050, 'learning_rate': 0.039772068469465606, 'num_leaves': 136, 'max_depth': 7, 'min_child_samples': 62, 'subsample': 0.8590196218875084, 'colsample_bytree': 0.6699348747251256, 'reg_alpha': 5.8858244212605255e-05, 'reg_lambda': 0.0010384986920524888}. Best is trial 12 with value: 791.8630722748783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 27: Average RMSE across 5 folds: 821.1374\n",
      "  Trial 28: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 950, 'learning_rate': 0.022462238303834196, 'num_leaves': 99, 'max_depth': 9, 'min_child_samples': 14, 'subsample': 0.7833994457293036, 'colsample_bytree': 0.5978712435913086, 'reg_alpha': 4.077037808880301e-08, 'reg_lambda': 6.277389949231544e-06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:08:46,050] Trial 28 finished with value: 813.0199786560872 and parameters: {'n_estimators': 950, 'learning_rate': 0.022462238303834196, 'num_leaves': 99, 'max_depth': 9, 'min_child_samples': 14, 'subsample': 0.7833994457293036, 'colsample_bytree': 0.5978712435913086, 'reg_alpha': 4.077037808880301e-08, 'reg_lambda': 6.277389949231544e-06}. Best is trial 12 with value: 791.8630722748783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 28: Average RMSE across 5 folds: 813.0200\n",
      "  Trial 29: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 1350, 'learning_rate': 0.07100458270258986, 'num_leaves': 59, 'max_depth': 5, 'min_child_samples': 78, 'subsample': 0.9483660529159825, 'colsample_bytree': 0.9374895779806732, 'reg_alpha': 2.6253807227079332e-06, 'reg_lambda': 4.222200251078136e-08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:08:48,861] Trial 29 finished with value: 817.8129321762243 and parameters: {'n_estimators': 1350, 'learning_rate': 0.07100458270258986, 'num_leaves': 59, 'max_depth': 5, 'min_child_samples': 78, 'subsample': 0.9483660529159825, 'colsample_bytree': 0.9374895779806732, 'reg_alpha': 2.6253807227079332e-06, 'reg_lambda': 4.222200251078136e-08}. Best is trial 12 with value: 791.8630722748783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 29: Average RMSE across 5 folds: 817.8129\n",
      "  Trial 30: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 1750, 'learning_rate': 0.015543014547789528, 'num_leaves': 190, 'max_depth': 3, 'min_child_samples': 22, 'subsample': 0.7077748083745639, 'colsample_bytree': 0.6227077026312686, 'reg_alpha': 1.2225595353783457e-08, 'reg_lambda': 5.528534293540225e-07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:08:52,555] Trial 30 finished with value: 795.4008204578631 and parameters: {'n_estimators': 1750, 'learning_rate': 0.015543014547789528, 'num_leaves': 190, 'max_depth': 3, 'min_child_samples': 22, 'subsample': 0.7077748083745639, 'colsample_bytree': 0.6227077026312686, 'reg_alpha': 1.2225595353783457e-08, 'reg_lambda': 5.528534293540225e-07}. Best is trial 12 with value: 791.8630722748783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 30: Average RMSE across 5 folds: 795.4008\n",
      "  Trial 31: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 1800, 'learning_rate': 0.013178004191489406, 'num_leaves': 184, 'max_depth': 3, 'min_child_samples': 24, 'subsample': 0.6851639210145026, 'colsample_bytree': 0.5356687107312613, 'reg_alpha': 1.0944455474409225e-08, 'reg_lambda': 5.28616414727913e-07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:08:56,366] Trial 31 finished with value: 798.2207320469287 and parameters: {'n_estimators': 1800, 'learning_rate': 0.013178004191489406, 'num_leaves': 184, 'max_depth': 3, 'min_child_samples': 24, 'subsample': 0.6851639210145026, 'colsample_bytree': 0.5356687107312613, 'reg_alpha': 1.0944455474409225e-08, 'reg_lambda': 5.28616414727913e-07}. Best is trial 12 with value: 791.8630722748783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 31: Average RMSE across 5 folds: 798.2207\n",
      "  Trial 32: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 1850, 'learning_rate': 0.009550054618762396, 'num_leaves': 201, 'max_depth': 3, 'min_child_samples': 21, 'subsample': 0.5922118613744994, 'colsample_bytree': 0.5672948918697331, 'reg_alpha': 1.1372898270617517e-08, 'reg_lambda': 8.469295637376796e-07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:09:01,030] Trial 32 finished with value: 796.5166609522219 and parameters: {'n_estimators': 1850, 'learning_rate': 0.009550054618762396, 'num_leaves': 201, 'max_depth': 3, 'min_child_samples': 21, 'subsample': 0.5922118613744994, 'colsample_bytree': 0.5672948918697331, 'reg_alpha': 1.1372898270617517e-08, 'reg_lambda': 8.469295637376796e-07}. Best is trial 12 with value: 791.8630722748783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 32: Average RMSE across 5 folds: 796.5167\n",
      "  Trial 33: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 2100, 'learning_rate': 0.006939099286525081, 'num_leaves': 201, 'max_depth': 3, 'min_child_samples': 22, 'subsample': 0.5362227708604779, 'colsample_bytree': 0.5763759148226626, 'reg_alpha': 7.299330012633659, 'reg_lambda': 4.423944187744019e-05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:09:07,292] Trial 33 finished with value: 796.9146506357541 and parameters: {'n_estimators': 2100, 'learning_rate': 0.006939099286525081, 'num_leaves': 201, 'max_depth': 3, 'min_child_samples': 22, 'subsample': 0.5362227708604779, 'colsample_bytree': 0.5763759148226626, 'reg_alpha': 7.299330012633659, 'reg_lambda': 4.423944187744019e-05}. Best is trial 12 with value: 791.8630722748783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 33: Average RMSE across 5 folds: 796.9147\n",
      "  Trial 34: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 1700, 'learning_rate': 0.002568590055310793, 'num_leaves': 217, 'max_depth': 4, 'min_child_samples': 12, 'subsample': 0.574814911600074, 'colsample_bytree': 0.6165723646026767, 'reg_alpha': 2.852412455665766e-07, 'reg_lambda': 1.7050037443553742e-06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:09:16,556] Trial 34 finished with value: 819.1982873646608 and parameters: {'n_estimators': 1700, 'learning_rate': 0.002568590055310793, 'num_leaves': 217, 'max_depth': 4, 'min_child_samples': 12, 'subsample': 0.574814911600074, 'colsample_bytree': 0.6165723646026767, 'reg_alpha': 2.852412455665766e-07, 'reg_lambda': 1.7050037443553742e-06}. Best is trial 12 with value: 791.8630722748783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 34: Average RMSE across 5 folds: 819.1983\n",
      "  Trial 35: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 1950, 'learning_rate': 0.009487339120786606, 'num_leaves': 231, 'max_depth': 3, 'min_child_samples': 46, 'subsample': 0.6104479519566144, 'colsample_bytree': 0.6237687105644909, 'reg_alpha': 3.9279479235015135e-08, 'reg_lambda': 0.00013079298912914956}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:09:21,464] Trial 35 finished with value: 819.9754897078244 and parameters: {'n_estimators': 1950, 'learning_rate': 0.009487339120786606, 'num_leaves': 231, 'max_depth': 3, 'min_child_samples': 46, 'subsample': 0.6104479519566144, 'colsample_bytree': 0.6237687105644909, 'reg_alpha': 3.9279479235015135e-08, 'reg_lambda': 0.00013079298912914956}. Best is trial 12 with value: 791.8630722748783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 35: Average RMSE across 5 folds: 819.9755\n",
      "  Trial 36: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 1550, 'learning_rate': 0.01590638789161194, 'num_leaves': 244, 'max_depth': 12, 'min_child_samples': 6, 'subsample': 0.6507494577909757, 'colsample_bytree': 0.5686379410257759, 'reg_alpha': 1.0439800967014685e-08, 'reg_lambda': 0.002683021394781616}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:09:38,076] Trial 36 finished with value: 807.3959532678467 and parameters: {'n_estimators': 1550, 'learning_rate': 0.01590638789161194, 'num_leaves': 244, 'max_depth': 12, 'min_child_samples': 6, 'subsample': 0.6507494577909757, 'colsample_bytree': 0.5686379410257759, 'reg_alpha': 1.0439800967014685e-08, 'reg_lambda': 0.002683021394781616}. Best is trial 12 with value: 791.8630722748783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 36: Average RMSE across 5 folds: 807.3960\n",
      "  Trial 37: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 1750, 'learning_rate': 0.009165678097889386, 'num_leaves': 205, 'max_depth': 4, 'min_child_samples': 26, 'subsample': 0.5090052780463235, 'colsample_bytree': 0.6477325462379617, 'reg_alpha': 3.4664820986996885e-08, 'reg_lambda': 2.4599416836615052e-08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:09:41,871] Trial 37 finished with value: 802.8308792566644 and parameters: {'n_estimators': 1750, 'learning_rate': 0.009165678097889386, 'num_leaves': 205, 'max_depth': 4, 'min_child_samples': 26, 'subsample': 0.5090052780463235, 'colsample_bytree': 0.6477325462379617, 'reg_alpha': 3.4664820986996885e-08, 'reg_lambda': 2.4599416836615052e-08}. Best is trial 12 with value: 791.8630722748783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 37: Average RMSE across 5 folds: 802.8309\n",
      "  Trial 38: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 2500, 'learning_rate': 0.0012515666535272195, 'num_leaves': 251, 'max_depth': 3, 'min_child_samples': 37, 'subsample': 0.5974893874311509, 'colsample_bytree': 0.605670338059609, 'reg_alpha': 4.5260850278203687e-07, 'reg_lambda': 0.032342008949296286}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:09:51,770] Trial 38 finished with value: 852.9586261049865 and parameters: {'n_estimators': 2500, 'learning_rate': 0.0012515666535272195, 'num_leaves': 251, 'max_depth': 3, 'min_child_samples': 37, 'subsample': 0.5974893874311509, 'colsample_bytree': 0.605670338059609, 'reg_alpha': 4.5260850278203687e-07, 'reg_lambda': 0.032342008949296286}. Best is trial 12 with value: 791.8630722748783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 38: Average RMSE across 5 folds: 852.9586\n",
      "  Trial 39: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 2100, 'learning_rate': 0.005922951613757512, 'num_leaves': 147, 'max_depth': 13, 'min_child_samples': 18, 'subsample': 0.7091719219735676, 'colsample_bytree': 0.7422114561242082, 'reg_alpha': 0.0037607705868242854, 'reg_lambda': 1.1280894949525597e-07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:10:20,023] Trial 39 finished with value: 811.5329109201627 and parameters: {'n_estimators': 2100, 'learning_rate': 0.005922951613757512, 'num_leaves': 147, 'max_depth': 13, 'min_child_samples': 18, 'subsample': 0.7091719219735676, 'colsample_bytree': 0.7422114561242082, 'reg_alpha': 0.0037607705868242854, 'reg_lambda': 1.1280894949525597e-07}. Best is trial 12 with value: 791.8630722748783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 39: Average RMSE across 5 folds: 811.5329\n",
      "  Trial 40: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 1250, 'learning_rate': 0.0023737880090673865, 'num_leaves': 173, 'max_depth': 11, 'min_child_samples': 5, 'subsample': 0.7478249203768187, 'colsample_bytree': 0.524224034522819, 'reg_alpha': 4.858126481385522e-06, 'reg_lambda': 1.7246712462069986e-05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:11:18,450] Trial 40 finished with value: 793.369296082565 and parameters: {'n_estimators': 1250, 'learning_rate': 0.0023737880090673865, 'num_leaves': 173, 'max_depth': 11, 'min_child_samples': 5, 'subsample': 0.7478249203768187, 'colsample_bytree': 0.524224034522819, 'reg_alpha': 4.858126481385522e-06, 'reg_lambda': 1.7246712462069986e-05}. Best is trial 12 with value: 791.8630722748783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 40: Average RMSE across 5 folds: 793.3693\n",
      "  Trial 41: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 1250, 'learning_rate': 0.0019536908888610495, 'num_leaves': 174, 'max_depth': 10, 'min_child_samples': 8, 'subsample': 0.6537394972827398, 'colsample_bytree': 0.524060150659421, 'reg_alpha': 8.349438851212175e-05, 'reg_lambda': 1.9470794814689147e-05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:12:08,060] Trial 41 finished with value: 801.6270708643567 and parameters: {'n_estimators': 1250, 'learning_rate': 0.0019536908888610495, 'num_leaves': 174, 'max_depth': 10, 'min_child_samples': 8, 'subsample': 0.6537394972827398, 'colsample_bytree': 0.524060150659421, 'reg_alpha': 8.349438851212175e-05, 'reg_lambda': 1.9470794814689147e-05}. Best is trial 12 with value: 791.8630722748783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 41: Average RMSE across 5 folds: 801.6271\n",
      "  Trial 42: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 1450, 'learning_rate': 0.0047732727918106555, 'num_leaves': 192, 'max_depth': 11, 'min_child_samples': 17, 'subsample': 0.7456016218527046, 'colsample_bytree': 0.5928930340174593, 'reg_alpha': 3.3588285213044033e-06, 'reg_lambda': 1.1958150792830926e-06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:12:45,136] Trial 42 finished with value: 801.7041044562 and parameters: {'n_estimators': 1450, 'learning_rate': 0.0047732727918106555, 'num_leaves': 192, 'max_depth': 11, 'min_child_samples': 17, 'subsample': 0.7456016218527046, 'colsample_bytree': 0.5928930340174593, 'reg_alpha': 3.3588285213044033e-06, 'reg_lambda': 1.1958150792830926e-06}. Best is trial 12 with value: 791.8630722748783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 42: Average RMSE across 5 folds: 801.7041\n",
      "  Trial 43: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 1150, 'learning_rate': 0.002768456901877077, 'num_leaves': 124, 'max_depth': 11, 'min_child_samples': 5, 'subsample': 0.7096906346298507, 'colsample_bytree': 0.5248035866507951, 'reg_alpha': 1.2549932272527843e-07, 'reg_lambda': 5.011635143266326e-07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:13:21,380] Trial 43 finished with value: 791.8489924189481 and parameters: {'n_estimators': 1150, 'learning_rate': 0.002768456901877077, 'num_leaves': 124, 'max_depth': 11, 'min_child_samples': 5, 'subsample': 0.7096906346298507, 'colsample_bytree': 0.5248035866507951, 'reg_alpha': 1.2549932272527843e-07, 'reg_lambda': 5.011635143266326e-07}. Best is trial 43 with value: 791.8489924189481.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 43: Average RMSE across 5 folds: 791.8490\n",
      "  Trial 44: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 1150, 'learning_rate': 0.003241737163056518, 'num_leaves': 299, 'max_depth': 11, 'min_child_samples': 6, 'subsample': 0.69870605678541, 'colsample_bytree': 0.5205211482114517, 'reg_alpha': 1.2811029868533523e-07, 'reg_lambda': 3.182143263527014e-06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:14:17,679] Trial 44 finished with value: 795.3707557750986 and parameters: {'n_estimators': 1150, 'learning_rate': 0.003241737163056518, 'num_leaves': 299, 'max_depth': 11, 'min_child_samples': 6, 'subsample': 0.69870605678541, 'colsample_bytree': 0.5205211482114517, 'reg_alpha': 1.2811029868533523e-07, 'reg_lambda': 3.182143263527014e-06}. Best is trial 43 with value: 791.8489924189481.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 44: Average RMSE across 5 folds: 795.3708\n",
      "  Trial 45: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 1100, 'learning_rate': 0.0030212712552355574, 'num_leaves': 123, 'max_depth': 11, 'min_child_samples': 5, 'subsample': 0.6865670750389865, 'colsample_bytree': 0.5180174177599846, 'reg_alpha': 1.380533405732595e-05, 'reg_lambda': 5.096166893587879e-06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:14:53,574] Trial 45 finished with value: 790.7054233737214 and parameters: {'n_estimators': 1100, 'learning_rate': 0.0030212712552355574, 'num_leaves': 123, 'max_depth': 11, 'min_child_samples': 5, 'subsample': 0.6865670750389865, 'colsample_bytree': 0.5180174177599846, 'reg_alpha': 1.380533405732595e-05, 'reg_lambda': 5.096166893587879e-06}. Best is trial 45 with value: 790.7054233737214.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 45: Average RMSE across 5 folds: 790.7054\n",
      "  Trial 46: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 1300, 'learning_rate': 0.0019824174340087298, 'num_leaves': 125, 'max_depth': 11, 'min_child_samples': 10, 'subsample': 0.7302982121659788, 'colsample_bytree': 0.530147686878438, 'reg_alpha': 1.30841501094824e-05, 'reg_lambda': 0.22027437337253336}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:15:36,224] Trial 46 finished with value: 806.3693451076235 and parameters: {'n_estimators': 1300, 'learning_rate': 0.0019824174340087298, 'num_leaves': 125, 'max_depth': 11, 'min_child_samples': 10, 'subsample': 0.7302982121659788, 'colsample_bytree': 0.530147686878438, 'reg_alpha': 1.30841501094824e-05, 'reg_lambda': 0.22027437337253336}. Best is trial 45 with value: 790.7054233737214.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 46: Average RMSE across 5 folds: 806.3693\n",
      "  Trial 47: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 800, 'learning_rate': 0.0015344392144230092, 'num_leaves': 110, 'max_depth': 13, 'min_child_samples': 12, 'subsample': 0.7542247541404635, 'colsample_bytree': 0.5772962652339634, 'reg_alpha': 7.076655128498394e-06, 'reg_lambda': 0.00015241443347147998}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:16:06,400] Trial 47 finished with value: 838.1764546211874 and parameters: {'n_estimators': 800, 'learning_rate': 0.0015344392144230092, 'num_leaves': 110, 'max_depth': 13, 'min_child_samples': 12, 'subsample': 0.7542247541404635, 'colsample_bytree': 0.5772962652339634, 'reg_alpha': 7.076655128498394e-06, 'reg_lambda': 0.00015241443347147998}. Best is trial 45 with value: 790.7054233737214.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 47: Average RMSE across 5 folds: 838.1765\n",
      "  Trial 48: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 1050, 'learning_rate': 0.00113031284742075, 'num_leaves': 151, 'max_depth': 9, 'min_child_samples': 5, 'subsample': 0.6622664392643677, 'colsample_bytree': 0.5057259011019922, 'reg_alpha': 2.537811101228904e-05, 'reg_lambda': 2.2064318799730366e-05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:16:49,596] Trial 48 finished with value: 815.1556359453346 and parameters: {'n_estimators': 1050, 'learning_rate': 0.00113031284742075, 'num_leaves': 151, 'max_depth': 9, 'min_child_samples': 5, 'subsample': 0.6622664392643677, 'colsample_bytree': 0.5057259011019922, 'reg_alpha': 2.537811101228904e-05, 'reg_lambda': 2.2064318799730366e-05}. Best is trial 45 with value: 790.7054233737214.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 48: Average RMSE across 5 folds: 815.1556\n",
      "  Trial 49: Starting 5-fold TimeSeriesSplit with params: {'n_estimators': 900, 'learning_rate': 0.0030763182260024275, 'num_leaves': 144, 'max_depth': 13, 'min_child_samples': 57, 'subsample': 0.8124909809231388, 'colsample_bytree': 0.50003593801713, 'reg_alpha': 5.65692763503795e-07, 'reg_lambda': 0.0007656969069685531}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 15:17:15,857] Trial 49 finished with value: 861.3403900875219 and parameters: {'n_estimators': 900, 'learning_rate': 0.0030763182260024275, 'num_leaves': 144, 'max_depth': 13, 'min_child_samples': 57, 'subsample': 0.8124909809231388, 'colsample_bytree': 0.50003593801713, 'reg_alpha': 5.65692763503795e-07, 'reg_lambda': 0.0007656969069685531}. Best is trial 45 with value: 790.7054233737214.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 49: Average RMSE across 5 folds: 861.3404\n",
      "\n",
      "CV Study statistics: \n",
      "  Number of finished trials:  50\n",
      "\n",
      "Best trial from CV study:\n",
      "  Value (Average RMSE): 790.7054\n",
      "  Best hyperparameters: \n",
      "    n_estimators: 1100\n",
      "    learning_rate: 0.0030212712552355574\n",
      "    num_leaves: 123\n",
      "    max_depth: 11\n",
      "    min_child_samples: 5\n",
      "    subsample: 0.6865670750389865\n",
      "    colsample_bytree: 0.5180174177599846\n",
      "    reg_alpha: 1.380533405732595e-05\n",
      "    reg_lambda: 5.096166893587879e-06\n",
      "\n",
      "Training final LightGBM model using best hyperparameters from CV study on full training data...\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit # For chronological cross-validation\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd # Assuming X_clv_full_train and y_clv_full_train are available\n",
    "\n",
    "\n",
    "def objective_cv(trial, X_data, y_data): # Pass X and y as arguments\n",
    "    \"\"\"\n",
    "    Objective function for Optuna with TimeSeries Cross-Validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    lgbm_params = {\n",
    "        'objective': 'regression_l2',\n",
    "        'metric': 'rmse',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'random_state': 42, # Seed for LightGBM's randomness\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 200, 2500, step=50),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "    }\n",
    "\n",
    "    # --- Cross-Validation Setup ---\n",
    "    # For CLV, if your samples (embeddings generated weekly) are in chronological order,\n",
    "    # TimeSeriesSplit is appropriate.\n",
    "    n_splits = 5 # Or 3, depending on your dataset size and computational budget\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    \n",
    "    fold_rmses = []\n",
    "\n",
    "    # Convert to NumPy if X_data/y_data are Pandas DataFrames/Series for tscv compatibility if needed\n",
    "    # Though tscv.split usually works with pandas indices too. For safety with lgb.fit:\n",
    "    if isinstance(X_data, pd.DataFrame): X_data_np = X_data.values\n",
    "    else: X_data_np = X_data\n",
    "    if isinstance(y_data, pd.Series): y_data_np = y_data.values\n",
    "    else: y_data_np = y_data\n",
    "\n",
    "    print(f\"  Trial {trial.number}: Starting {n_splits}-fold TimeSeriesSplit with params: {trial.params}\")\n",
    "    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_data_np)):\n",
    "        # print(f\"    Fold {fold+1}/{n_splits}\")\n",
    "        X_train_fold, X_val_fold = X_data_np[train_idx], X_data_np[val_idx]\n",
    "        y_train_fold, y_val_fold = y_data_np[train_idx], y_data_np[val_idx]\n",
    "\n",
    "        model = lgb.LGBMRegressor(**lgbm_params)\n",
    "        \n",
    "        model.fit(\n",
    "            X_train_fold, \n",
    "            y_train_fold,\n",
    "            eval_set=[(X_val_fold, y_val_fold)],\n",
    "            eval_metric='rmse',\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)] # Early stopping per fold\n",
    "        )\n",
    "        \n",
    "        preds = model.predict(X_val_fold)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val_fold, preds))\n",
    "        fold_rmses.append(rmse)\n",
    "        # print(f\"      Fold {fold+1} RMSE: {rmse:.4f}\")\n",
    "\n",
    "    average_rmse = np.mean(fold_rmses)\n",
    "    print(f\"  Trial {trial.number}: Average RMSE across {n_splits} folds: {average_rmse:.4f}\")\n",
    "    \n",
    "    return average_rmse\n",
    "\n",
    "# --- Optuna Study Setup ---\n",
    "study_name_cv = \"lgbm_clv_optimization_cv\"\n",
    "study_cv = optuna.create_study(\n",
    "    study_name=study_name_cv,\n",
    "    direction='minimize',\n",
    "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=2, n_min_trials=3) # Prune after 2 trials if not promising, ensure at least 3 trials run this far.\n",
    ")\n",
    "\n",
    "print(f\"\\nStarting Optuna study with Cross-Validation: {study_name_cv}\")\n",
    "# Use a lambda function to pass X_clv_full_train and y_clv_full_train to the objective\n",
    "study_cv.optimize(lambda trial: objective_cv(trial, X, y), \n",
    "                  n_trials=50, # Adjust number of trials as needed\n",
    "                  timeout=1800) # Example: 30 minutes timeout\n",
    "\n",
    "# --- Get and Print Best Results ---\n",
    "print(\"\\nCV Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study_cv.trials))\n",
    "\n",
    "best_trial_cv = study_cv.best_trial\n",
    "\n",
    "print(\"\\nBest trial from CV study:\")\n",
    "print(f\"  Value (Average RMSE): {best_trial_cv.value:.4f}\")\n",
    "print(\"  Best hyperparameters: \")\n",
    "for key, value in best_trial_cv.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# --- Step 5: Train Final Model with Best Hyperparameters on Full Training Data ---\n",
    "print(\"\\nTraining final LightGBM model using best hyperparameters from CV study on full training data...\")\n",
    "best_params_cv = best_trial_cv.params\n",
    "\n",
    "\n",
    "\n",
    "# You would then evaluate this 'final_lgbm_model' on your actual hold-out CLV test set\n",
    "# (which consists of embeddings and CLV targets from a period after X_clv_full_train).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f185dfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final LightGBM model trained.\n"
     ]
    }
   ],
   "source": [
    "final_lgbm_model = lgb.LGBMRegressor(objective='regression_l2', metric='rmse', verbosity=-1, boosting_type='gbdt', random_state=42, **best_params_cv)\n",
    "final_lgbm_model.fit(X, y) \n",
    "print(\"Final LightGBM model trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ceaa1efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = prediction_dataset[\n",
    "    (prediction_dataset['prediction_date'] >= pd.to_datetime('2011-08-30 23:59:59')) &\n",
    "    (prediction_dataset['prediction_date'] < pd.to_datetime('2011-08-31 23:59:59'))\n",
    "]\n",
    "X_test = np.vstack(test_data['embedding'].values)\n",
    "y_test = test_data['clv_3_month_target'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d991a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57695.90776190476"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad1077f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['y_hat'] = final_lgbm_model.predict(X_test)\n",
    "test_data['y_hat'] = np.maximum(test_data['y_hat'], 0)  # Ensure no negative CLV predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c72e7f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 1123.6914\n",
      "Test MAE: 372.1760\n",
      "Test R^2: 0.4810\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "mse = mean_squared_error(test_data['clv_3_month_target'], test_data['y_hat'])\n",
    "r2 = r2_score(test_data['clv_3_month_target'], test_data['y_hat'])\n",
    "#mse = mean_squared_error(y_val, y_val_hat)\n",
    "#r2 = r2_score(y_val, y_val_hat)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(test_data['clv_3_month_target'], test_data['y_hat'])\n",
    "print(f\"Test RMSE: {rmse:.4f}\")\n",
    "print(f\"Test MAE: {mae:.4f}\")\n",
    "print(f\"Test R^2: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ff54de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plot_data = test_data.sort_values(by='clv_3_month_target').reset_index(drop=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(plot_data['y_hat'], label='Predicted CLV', color='orange', )\n",
    "#plt.plot(plot_data['clv_3_month_target'], label='Actual CLV', color='blue',)\n",
    "\n",
    "plt.xlabel('Customer Index (sorted by actual CLV)')\n",
    "plt.ylabel('CLV (3-month target)')\n",
    "plt.title('Actual vs Predicted CLV (3-month target)')\n",
    "plt.legend()\n",
    "#plt.ylim(0,20000)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "fb5190d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CTGNN_monthly = test_data[['clv_3_month_target', 'y_hat']].copy()\n",
    "CTGNN_monthly.to_csv('CTGNN_results_monthly.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31db70c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
